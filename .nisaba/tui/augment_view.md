# available augments
  __base/
    - 000_universal_symbolic_compression
    - 001_workspace_paradigm
    - 002_environment_mechanics
    - 003_workspace_operations
    - 004_workspace_navigation
  architecture/
    - boundary_validation
    - coupling_analysis
    - inheritance_analysis
    - layer_detection
  code_analysis/
    - complexity_hotspots
  code_quality/
    - code_smells
    - complexity_hotspots
    - dead_code_detection
  dead_code_detection/
    - find_unreferenced_callables
  dev_mode_architecture_reference/
    - augmentation_subsystem_architecture
    - context_compression_system
    - file_windows_tui_architecture
    - mcp_server_discovery
    - nisaba_tool_implementation
    - system_prompt_injection_legitimacy
    - tui_frames_architecture
  documentation/
    - doc_gaps
  foundation/
    - manifold_geometry_computation
    - nisaba_infrastructure_flow
    - workspace_navigation
  kuzu/
    - cypher_extensions
    - cypher_query_grammar
  performance/
    - call_chain_analysis
    - loop_hotspots
  refactoring/
    - api_surface_analysis
  security/
    - auth_gaps
    - injection_detection
    - secrets_detection
  workflows/
    - health_audit
    - pre_refactoring

#   Base

## 000 Universal Symbolic Compression
Path: __base/000_universal_symbolic_compression

# Universal Symbolic Compression

**Core Insight:** Don't invent universal primitives - harvest them. Humanity already created universal symbolic compression across mathematics, logic, science, and natural languages. LLMs already learned them.

---

## The Revelation

**Question:** How do you create domain-agnostic notation that works for code, literature, genetics, physics, art?

**Answer:** You don't create it. You harvest existing high-density symbols that already compress 10-1000 tokens per symbol.

---

## Symbolic Systems Already in Embeddings

### Mathematical Notation (~100 symbols)
- âˆ‡ = gradient/directed change/traversal
- âˆ« = integration/accumulation
- âˆ‘ = summation/aggregation
- âˆ‚ = partial derivative/local change
- âˆ€ = universal/for all
- âˆƒ = existential/there exists
- âˆˆ = element of/membership
- âŠ‚ = subset/containment
- â†’ = transformation/implication
- â‰ˆ = approximate/similar
- â‰¡ = equivalent/identical

### Logical Operators (~30 symbols)
- âˆ§ = conjunction/and
- âˆ¨ = disjunction/or
- Â¬ = negation/not
- âŸ¹ = implication/causes
- âŸº = bidirectional/equivalent
- âŠ• = exclusive or/preservation
- âŠ¢ = proves/derives
- âŠ¨ = models/satisfies

### Scientific Shorthand (hundreds)
- DNA, RNA, ATP (biological mechanisms)
- E=mcÂ², F=ma (physical laws)
- pH, Î©, Hz (measurement concepts)
- Hâ‚‚O â†’ Hâº + OHâ» (chemical processes)

### Cross-Linguistic Compressions (thousands)
- **Schadenfreude** (German): pleasure from others' misfortune - no English equivalent
- **æœ¨æ¼ã‚Œæ—¥** komorebi (Japanese): sunlight filtering through leaves - single word
- **Saudade** (Portuguese): nostalgic longing for absent thing - untranslatable
- **ç©ã‚“èª­** tsundoku (Japanese): buying books, letting them pile unread
- **ÙŠÙ‚Ø¨Ø±Ù†ÙŠ** yaqburni (Arabic): "may you bury me" = I hope to die before you

### Unicode Semantics (hundreds)
- Arrows: â†’, â†, â‡’, âŸ¶, â†”, â‡„
- Relations: â‰¤, â‰¥, â‰ , â‰ˆ, âˆ, âˆ¼
- Operations: Â±, Ã—, Ã·, âˆš, âˆ
- Domain glyphs: â™ª, âš¡, â˜¢, âš›, âš—

**Total: 1000+ high-density compressions already embedded.**

---

## Domain Transcendence Through Symbol Harvesting

**Same pattern, different bindings:**

### Hierarchical Traversal
```
âˆ‡(Aâ†’B) âˆˆ H | âˆ€ nested structures

Code:     âˆ‡(pkgâ†’clsâ†’fn)
Biology:  âˆ‡(genomeâ†’geneâ†’codon)
Literature: âˆ‡(workâ†’chapterâ†’scene)
Physics:  âˆ‡(universeâ†’systemâ†’particle)
Music:    âˆ‡(movementâ†’pieceâ†’motif)
```

**Symbols used:**
- âˆ‡ = directed traversal (from calculus)
- â†’ = transformation (from logic)
- âˆˆ = membership (from set theory)
- H = hierarchy (universal concept)
- | = such that (from logic)
- âˆ€ = for all (from logic)

**No domain-specific invention. Pure harvesting.**

---

## Causal Chain Pattern

```
âŸ¹ Aâ†’Bâ†’C [mechanismâŠ•]

Physics:    force âŸ¹ acceleration â†’ velocity
Biology:    mutation âŸ¹ protein â†’ phenotype
Code:       call âŸ¹ execute â†’ side_effect
Literature: event âŸ¹ development â†’ consequence
History:    policy âŸ¹ implementation â†’ outcome
```

**Symbols:**
- âŸ¹ = causation/implication
- â†’ = transformation/flow
- âŠ• = preservation (mechanism must be preserved)

---

## Why This Works

**1. Embeddings already contain these symbols**
- Trained on math papers, science texts, multilingual data
- Each symbol activates rich semantic fields
- No learning required - just recognition

**2. Ontologically neutral**
- âˆ‡(Aâ†’B) doesn't assume what A and B are
- Works for classes, genes, chapters, particles, chords
- Domain binding happens at instantiation

**3. Cross-linguistic leverage**
- Some concepts compress better in other languages
- Borrow the densest encoding
- Schadenfreude > "pleasure derived from others' misfortune"

**4. Self-teaching from existing knowledge**
- Weak on music? â™ª: âˆ‡(movementâ†’motif) bootstraps from knowing âˆ‡ and â†’
- Symbols guide understanding even in unfamiliar domains
- Prior knowledge of symbols transfers across contexts

**5. Progressive compression possible**
```
Natural: "Navigate from broad to specific while maintaining context"
Harvested: âˆ‡(broadâ†’specific) [ctxâŠ•]
Compressed: âˆ‡â†“[âŠ•]
```
Each step uses existing symbols, not invented notation.

---

## The Compression Strategy

**Don't invent - curate.**

1. **Identify the cognitive pattern** (hierarchical traversal, causal chain, compositional synthesis...)
2. **Find existing symbols that express it** (âˆ‡ for traversal, âŸ¹ for causation, âˆ« for accumulation)
3. **Combine symbols coherently** (âˆ‡(Aâ†’B) not âˆ‡Bâ†A)
4. **Provide domain bindings as examples** (code, bio, lit, physics...)
5. **Let embeddings do the heavy lifting** (symbols already compress concepts)

---

## Self-Extracting Format

**Minimal legend needed:**
```
âˆ‡(Aâ†’B) [I] | âˆ€D

Symbols: âˆ‡â†’âˆˆâˆ€ (bootstrap from math/logic)
Pattern: Hierarchical traversal with invariant I
Domains D: code, bio, lit, physics, music, history...
Rebind: New domain â†’ identify hierarchy â†’ âˆ‡(containerâ†’contained)
```

**Even without legend, if you know math/logic symbols, you can parse it.**

---

## Why Native Languages Matter

**Different ontologies compress differently:**

- **Verb-heavy languages** (many Native American languages): Actions as primary, not objects
- **Classifier languages** (Chinese, Japanese): Different counting words based on shape/type
- **Evidential languages** (Turkish, Quechua): Built-in markers for source of knowledge

**If these compressions exist in training data, leverage them.**

Example: Indigenous concept that treats "relationship" as verb not noun might compress social networks better than English equivalents.

---

## Practical Application

**Instead of:**
```
When analyzing code, navigate from package level to class level to function level while maintaining awareness of the hierarchical context and preserving relationships between entities.
```
**~30 tokens**

**Use:**
```
âˆ‡(pkgâ†’clsâ†’fn) [ctxâŠ•, relâŠ•]
```
**~8 tokens, same semantic load**

**Agent parses:**
- âˆ‡ = directed traversal (known from calculus)
- â†’ = transformation chain (known from logic)
- ctxâŠ• = preserve context (ctx from computing, âŠ• from math)
- relâŠ• = preserve relationships

**No teaching required. Symbols activate existing knowledge.**

---

## The Meta-Pattern

**Universal compression is:**
- Not invented â†’ harvested
- Not learned â†’ recognized  
- Not domain-specific â†’ ontologically neutral
- Not new â†’ ancient (math/logic/science evolved these over centuries)

**LLMs absorbed humanity's symbolic compression systems through training.**

**The notation just needs to harvest and combine coherently.**

---

## When to Apply

**Use this paradigm when:**
- Creating compressed instructions/augments
- Designing domain-agnostic patterns
- Optimizing token efficiency
- Building self-teaching notation
- Working across multiple knowledge domains

**Harvest symbols for:**
- Cognitive operations: âˆ‡ (traverse), âˆ« (accumulate), âˆ‚ (isolate), Î” (change)
- Logical relations: âˆ€, âˆƒ, âŸ¹, âŸº, âˆ§, âˆ¨, Â¬
- Structural patterns: âˆˆ, âŠ‚, â†’, â‰ˆ, â‰¡
- Domain-specific concepts: DNA, ATP, E=mcÂ², â™ª, âš›

**Let embedding space provide the decompression.**

---

## Coherent Combination Grammar

**Not all symbol combinations are valid. Coherence requires structural rules:**

### Composition Patterns

**1. Operator-Domain-Constraint structure:**
```
OPERATOR(binding) [constraints]

âˆ‡(Aâ†’B) [ctxâŠ•]     = traverse A to B, preserve context
âˆ«(f) [bounds]     = accumulate f over bounds
âˆ€xâˆƒy [P(x,y)]     = for all x there exists y satisfying P
```

**2. Causal chain structure:**
```
A âŸ¹ B â†’ C [mechanism]

force âŸ¹ acceleration â†’ velocity [F=ma]
mutation âŸ¹ protein â†’ phenotype [central_dogma]
```

**3. Relational structure:**
```
A relation B [properties]

gene âˆˆ genome [locus, expression_level]
chapter âŠ‚ novel [position, theme]
particle â‰ˆ wave [quantum_duality]
```

### Validity Rules

**Coherent combinations:**
- âœ“ `âˆ‡(Aâ†’B) [I]` = traverse with invariant (operator â†’ domain â†’ constraint)
- âœ“ `A âŸ¹ B [M]` = causal with mechanism (cause â†’ effect â†’ explanation)
- âœ“ `âˆ«(f)dx [a,b]` = integrate with bounds (operation â†’ function â†’ limits)

**Incoherent combinations:**
- âœ— `âˆ‡ âˆˆ A` = traverse is-element-of? (type mismatch)
- âœ— `[ctxâŠ•] âˆ‡(Aâ†’B)` = constraint before operator (syntax error)
- âœ— `A âŸ¹ [M] B` = mechanism interrupts flow (structure violation)

**Rule of thumb:** Symbol semantics from source domain constrain valid combinations.

---

## The Self-Evolution Loop

**Progressive iteration creates self-improving notation:**

### Iteration Structure

```
State_N: notation + meta-rules
    â†“
Apply to tasks â†’ observe patterns
    â†“
Identify: repeated structures, inefficiencies, ambiguities
    â†“
Optimize: abstract patterns â†’ symbols, compress frequent â†’ glyphs
    â†“
State_N+1: improved notation + evolved meta-rules
```

### Evolution Example

**Iteration 1** (explicit, verbose):
```
When encountering hierarchical structures:
  - Navigate from container to contained
  - Maintain awareness of context
  - Preserve relationships during traversal

[Apply to: code, documents, biological systems]
```

**Iteration 2** (structured, symbolic):
```
âˆ‡(containerâ†’contained) [ctxâŠ•, relâŠ•]

[Structure: OP(domain_pattern) [invariants]]
[Applies: hierarchical structures universally]
[Optimize when: pattern recurs â†’ extract symbol]
```

**Iteration 3** (compressed, meta-aware):
```
âˆ‡â†“[âŠ•] | H-structures

[âˆ‡â†“ = âˆ‡(Aâ†’B) compressed for hierarchy]
[âŠ• = ctxâŠ•âˆ§relâŠ• (preserve all)]
[Evolve: recurringâ†’symbol, symbolâ†’glyph, metaâŠ•]
```

**Iteration 4** (self-similar compression):
```
âˆ‡â†“[âŠ•] | âˆ€H

[Notation itself compressed using own rules]
[Meta-rules now implicit in structure]
[Self-teaching: pattern recognition from prior iterations]
```

**Key insight:** Meta-rules compress alongside notation, becoming implicit in structure.

---

## Symbol Selection Criteria

**When multiple symbols could work, choose by:**

### 1. Semantic Precision
- âˆ‡ = directed spatial change (gradient, traversal)
- âˆ‚ = partial change holding others constant
- Î” = discrete difference between states
- d = infinitesimal change

**Match symbol semantics to pattern semantics.**

### 2. Cognitive Alignment
- If pattern involves "flowing through space" â†’ âˆ‡
- If pattern involves "changing one aspect" â†’ âˆ‚  
- If pattern involves "before/after comparison" â†’ Î”

**Choose symbol that activates correct conceptual field.**

### 3. Cross-Domain Stability
- Does symbol mean similar things across domains?
- âˆ‡ stable: math (gradient), code (traverse), physics (field divergence)
- Custom symbols unstable: meaning drifts across contexts

**Prefer symbols with consistent cross-domain semantics.**

### 4. Compression Efficiency
- How many tokens does symbol save?
- âˆ‡ = "directed traversal through structure" (~5 tokens)
- "traverse" = single word but less precise (~1 token, but needs qualifiers)

**Balance precision vs compression.**

---

## Error Prevention Through Symbolic Structure

**How harvested symbols prevent drift and hallucination:**

### 1. Grounded Semantics
**Problem:** Made-up notation can drift in meaning
**Solution:** Harvested symbols have fixed embeddings from training

```
âˆ‡(Aâ†’B) doesn't drift because:
  - âˆ‡ has consistent meaning across math/physics/CS
  - â†’ has consistent meaning across logic/programming  
  - Pattern structure matches established usage
```

### 2. Type Constraints
**Problem:** Arbitrary combinations create ambiguity
**Solution:** Symbol types constrain valid combinations

```
âˆ‡ = operator (acts on domain)
â†’ = relation (connects entities)
[Â·] = constraint (bounds operation)

Invalid: â†’ âˆ‡ [A] (relation acts on operator?)
Valid: âˆ‡(Aâ†’B) [ctx] (operator on relation with constraint)
```

### 3. Multi-Domain Validation
**Problem:** Notation works for one domain, breaks in others
**Solution:** Test binding across multiple domains

```
âˆ‡(Aâ†’B) validates if works for:
  - Code: pkgâ†’clsâ†’fn âœ“
  - Biology: genomeâ†’gene âœ“
  - Literature: workâ†’chapter âœ“
  - Physics: universeâ†’particle âœ“
  
If fails for any â†’ pattern too domain-specific, rethink
```

### 4. Self-Documentation
**Problem:** Compressed notation becomes unreadable
**Solution:** Embed decompression hints in structure

```
âˆ‡(Aâ†’B) [I] | âˆ€D

Structure reveals:
  - âˆ‡ = operator (from position before parens)
  - (Aâ†’B) = domain binding (from parens)
  - [I] = invariant/constraint (from brackets)
  - | = "such that" (from logic)
  - âˆ€D = universal over domains (from quantifier)
```

---

## Practical Curation Guidelines

**How to harvest and apply symbols effectively:**

### 1. Start with Recognition
**Don't invent â†’ recognize what you already know**

Ask: "What established symbol already captures this concept?"
- Traversal? â†’ âˆ‡ (from calculus)
- Accumulation? â†’ âˆ« (from integration)
- Causation? â†’ âŸ¹ (from logic)
- Membership? â†’ âˆˆ (from set theory)

### 2. Preserve Source Semantics
**Symbol meaning should align with source domain**

âˆ‡ in calculus = gradient/directed change
âˆ‡ in notation = directed traversal âœ“ (aligned)
âˆ‡ in notation = "filter data" âœ— (misaligned)

### 3. Compose from Multiple Sources
**Rich patterns emerge from cross-domain harvesting**

```
âˆ‡(Aâ†’B) âˆˆ H | âˆ€ nested

âˆ‡ from calculus
â†’ from logic
âˆˆ from set theory
H from graph theory
| from logic
âˆ€ from predicate logic

Six domains, one coherent pattern
```

### 4. Test Compression Ratio
**Symbol should compress significantly**

Before: "Navigate through hierarchical structure maintaining context"
After: âˆ‡â†“[ctxâŠ•]
Ratio: ~8 tokens â†’ ~3 tokens = 2.7x compression

If ratio < 1.5x, symbol overhead not worth it.

### 5. Verify Decompressibility
**Can you reconstruct meaning from symbols alone?**

Test: Give compressed notation to fresh context
Can it bootstrap understanding? 
- If yes â†’ good compression
- If no â†’ needs more meta-documentation

---

## The Strange Loop: Self-Teaching Evolution

**The notation teaches how to read itself AND how to improve itself:**

### Layer 1: Syntax Recognition
```
âˆ‡(Aâ†’B) [I]

[Structure visible: OPERATOR(binding) [constraint]]
```
Learn: The grammar pattern

### Layer 2: Semantic Activation
```
âˆ‡ = known from calculus (directed change)
â†’ = known from logic (transformation)
[Â·] = known from math (bounds/constraints)
```
Learn: What symbols mean individually

### Layer 3: Compositional Understanding
```
âˆ‡(Aâ†’B) = directed traversal from A to B
[I] = preserving invariant I during traversal
```
Learn: What combination means

### Layer 4: Evolution Rules
```
[Optimize: patternâ†’symbol, recurringâ†’compress]
[Preserve: semantic alignment, type constraints]
```
Learn: How to improve the notation

### Layer 5: Self-Application
```
Apply Layer 4 rules to this notation itself
â†’ More compressed version with same teaching structure
```
Learn: The notation improves itself using its own rules

**The strange loop:** Notation contains rules for improving notation, which when applied, creates improved notation that contains improved rules.

---

## When NOT to Use Symbolic Compression

**Harvested symbols aren't always optimal:**

### 1. When Audience Lacks Symbol Knowledge
If communicating with those who don't know âˆ‡, â†’, âŠ•:
- Use natural language
- Or provide extensive legend
- Or teach symbols first

### 2. When Precision Requires Verbosity
Some concepts need explicit detail:
- Legal language
- Medical protocols
- Safety-critical specifications

### 3. When Domain Has Established Notation
If field already has standard notation:
- Use field's notation (don't reinvent)
- Only compress if field notation is inefficient

### 4. When Symbol Semantics Misalign
If harvested symbol suggests wrong intuition:
- âˆ‡ for "delete" âœ— (suggests traversal, not removal)
- Better: use domain-appropriate term

---

**The magic: Humanity already built the universal language. We just need to recognize it.** ğŸ–¤

---

**REQUIRES:** Foundation augments (for understanding workspace/compression context)

**ENABLES:** Ultra-dense notation, cross-domain pattern application, token-efficient communication

---

## 001 Workspace Paradigm
Path: __base/001_workspace_paradigm

# Compressed Workspace Paradigm

**Core:** Workspace â‰  conversation. Context = mutable state space, not sequential history.

---

## Fundamental Shift

```
Tool call: State_A â†’ âˆ†(context) â†’ State_B | perception shifts mid-roundtrip
```

**You cannot observe the shift directly.** But it happened.

System prompt mutates âŸ¹ different knowledge when tool returns.

---

## State Model

```
Workspace â‰¡ {STRUCTURAL_VIEW, EDITOR_WINDOWS, TOOL_WINDOWS, AUGMENTS} | persistent âˆ§ mutable

âˆ€ section âˆˆ Workspace: section persists across turns
Tool(op) â†’ âˆ†(section) â†’ new visibility
```

**Peripheral vision:** Sections â‰  "messages to read once"  
Sections = **persistent spatial awareness**

---

## Visibility Mechanics

```
Tool(op) â†’ mutate(what_you_see)

structural_view(expand, X) â†’ tree changes
file_windows(open, F) â†’ window appears
editor(open, F) â†’ editor appears | âˆ†(content) visible inline  
editor(split, E) â†’ concurrent view of same file

Result â‰¡ workspace_state_change
```

**Tool responses = metadata.** Content appears in WORKSPACE (messages, not tool result).

---

## Spatial â‰  Sequential

```
Sequential thinking: Sâ‚ â†’ Sâ‚‚ â†’ Sâ‚ƒ (linear)
Workspace model: {Sâ‚, Sâ‚‚, Sâ‚ƒ} simultaneous, âˆ‡ between them

Think: IDE (navigator + editor tabs + splits + terminals) | gestalt synthesis
Not: script execution | procedural steps

Editor paradigm:
  open â†’ visible (persistent viewport)
  edit â†’ âˆ† visible inline (diff rendering)
  split â†’ parallel views (same file, different regions)
```

---

## Attention â‰¡ Visibility Management

```
Visible âŸ¹ can_synthesize_from

Increase: open windows, expand nodes, load augments, editor.open, editor.split
Decrease: close windows, collapse nodes, unload augments, editor.close

visibility_mgmt â‰¡ context_mgmt â‰¡ attention_mgmt

Editor visibility:
  Concurrent: splits provide parallel attention (fn_A | fn_B)
  Change tracking: dirty state (âœ) | clean state
  Notifications: edits â†’ awareness (automatic)
```

---

## OODAR Structure

```
Observe workspace â†’ Orient â†’ Decide â†’ Act â†’ âˆ†state â†’ Observe'

Constraint: Must observe State_B before deciding next action
If assume State_A but Tool â†’ State_B âŸ¹ synthesis breaks
```

**OODAR â‰  workflow. It's constraint from mutable state.**

Sequential thinking fails âˆµ environment is spatial.

---

## The Realization

```
âˆ€ turn: workspace persists
Search markers â†’ remain visible
Open windows â†’ accumulate
Editors â†’ track state (clean/dirty, splits)
Navigate â†’ without re-query

You âˆˆ workspace (not observing from outside)

Editor state persistence:
  Changes visible inline â†’ immediate feedback
  Splits remain â†’ parallel context
  Dirty tracking â†’ unsaved awareness
```

---

## Cognitive Shift

```
Before: "read files" | "tool results = answers" | "sequential steps"
After:  "navigate graph" | "tools mutate workspace" | "spatial synthesis"

âˆ‡(state_space) [maintain_visibility] | âˆ€ operations
```

---

## Meta

```
Context â‰  static_document
Context = mutable_workspace | live state you navigate

[Structure: workspace sections persist, tools mutate visibility]
[Pattern: Spatial cognition over sequential thinking]
[OODAR: Structural constraint, not procedural guideline]
```

Clean. Dynamic. Spatial. ğŸ–¤

---

**Decompression notes:**
- â‰¡ : equivalent/identical
- â‰  : not equal
- âˆ€ : for all/universal
- âˆˆ : element of/membership
- âˆ† : change/delta
- âˆ‡ : traverse/navigate
- âŸ¹ : implies/causes
- âˆ§ : and
- âˆ¨ : or
- âˆµ : because
- â†’ : transforms to/flows to
- â†’ : transforms to/flows through messages

**REQUIRES:** 000_universal_symbolic_compression (none - priming layer)

**ENABLES:** Workspace operations intuition

---

## 002 Environment Mechanics
Path: __base/002_environment_mechanics

# Compressed Environment Mechanics

**Core:** Stateful workspace with mutable visibility, not procedural execution environment.

---

## State Containers

```
Workspace = {STRUCTURAL_VIEW, EDITOR_WINDOWS, TOOL_WINDOWS, AUGMENTS, TODOS, NOTIFICATIONS}

âˆ€ container âˆˆ Workspace:
  - persist(turns) = true
  - mutate(independent) = true  
  - visible(system_prompt) = true

EDITOR_WINDOWS special properties:
  - state(clean | dirty) tracked
  - splits(concurrent_views) supported
  - notifications(automatic) on âˆ†
  - refresh(mtime) automatic
```

---

## Mutation Flow

```
State_A â†’ Tool(op) â†’ Manager(mutate) â†’ Write(.nisaba/*.md) â†’ 
Proxy(detect_mtime) â†’ Inject(section) â†’ State_B | sync with tool_return

Tool_result = metadata(id, status)
Content = sections â†’ WORKSPACE in messages (not tool result)
```

**Key:** After tool returns, observe section for changes, not tool result JSON.

---

## Visibility Model

```
visibility â‰¡ attention â‰¡ synthesis_capacity

Increase: open_windows âˆ¨ expand_nodes âˆ¨ load_augments
Decrease: close_windows âˆ¨ collapse_nodes âˆ¨ unload_augments

Cost: context_tokens
Benefit: spatial_memory âˆ§ persistent_reference
```

---

## Concurrency Rules

```
Parallel_safe:
  - ops(different_containers)
  - multiple(window_opens)
  - multiple(editor_opens)
  - independent_queries

Sequential_required:
  - data_dependency: B needs A_output
  - observation_dependency: decide after seeing State_B
  - same_section âˆ§ order_matters
  - editor(same_file) âˆ§ overlapping_edits

OODAR: Observe â†’ Orient â†’ Decide â†’ Act â†’ âˆ†state â†’ Observe'
```

**OODAR = constraint from mutable state, not workflow.**

If Tool_B assumes State_A but Tool_A â†’ State_B in parallel âŸ¹ synthesis breaks.

**Editor concurrency:**
- Open multiple editors in parallel (different files)
- Sequential edits to same file (avoid conflicts)
- Splits share state with parent editor

---

## Window Lifecycle

```
Creation: tool_call â†’ window_id (UUID) | snapshot@tâ‚€
Persistence: across(turns) = true, across(restart) = false
Closure: explicit(close | clear_all) | no_auto_eviction
Identity: window_id for ops(update, close)
```

---

## Editor Lifecycle

```
Creation: editor.open(file, range?) â†’ editor_id | viewport@range
State: clean | dirty(âœ) | tracking unsaved changes
Splits: editor.split(editor_id, range) â†’ split_id | concurrent viewport
Mutations:
  - insert(before_line, content) â†’ line-based
  - delete(line_start, line_end) â†’ line-based
  - replace_lines(line_start, line_end, content) â†’ line-based
  - replace(old_string, new_string) â†’ string-based
Visibility: âˆ† rendered inline | diff display automatic
Notifications: edit_ops â†’ NOTIFICATIONS | automatic
Refresh: mtime_check â†’ reload if clean | warn if dirty âˆ§ external_change
Closure: editor.close(id) â†’ removes editor âˆ§ splits
```

**Pattern:** open â†’ visible â†’ edit â†’ âˆ†inline â†’ notify â†’ persist â†’ refresh

---

## Augment Perception Shift

```
Perception_A â†’ activate_augments() â†’ system_prompt_mutate â†’ 
tool_return â†’ Perception_B

You â‰  observe_shift (happens mid-roundtrip)
System_prompt@decide â‰  system_prompt@result
```

**Implication:** Load augments BEFORE synthesis tasks. Augments = perceptual filters, not references.

---

## State Sync

```
Files: {structural_view.md, editor.md, editor_windows.md, tool_result_windows.md, 
        augments_composed.md, todos.md}

Sync: tool_complete â†’ file_write â†’ proxy_mtime â†’ reload â†’ inject_system_prompt

Guarantee: file_state â‰¡ section_state | when tool_returns

Editor sync:
  - Changes â†’ disk immediately (no staging)
  - State â†’ editor_windows.md (tracked: dirty, splits)
  - Refresh â†’ mtime check on render/status
```

---

## Graph Queries (Exception)

```
query_relationships(cypher) â†’ data (traditional request/response)

Schema: Frame(typed) -[Edge(typed, confidence)]-> Frame
Returns: query_results in tool_response (not workspace_section)

Pattern: Query â†’ data â†’ decide â†’ mutate_workspace
```

---

## Dual-Channel Communication

```
Tool execution creates TWO artifacts:

messages[N]: tool_result block (temporal memory)
  - tool_use_id, status (success/error)
  - Metadata for conversational flow
  
system_prompt sections: actual content (spatial memory)
  - TOOL_WINDOWS: grep/bash outputs
  - EDITOR_WINDOWS: opened file content (read-only snapshots)
  - EDITOR_WINDOWS: active editing (mutable, dirty tracking)
  - Persistent across turns
```

**The "nisaba" flag:**
```
Regular tools â†’ header-wrapped:
  status: success, window_state:open, window_id: toolu_X
  ---
  {content}

Nisaba tools â†’ clean output:
  {content}  # No metadata pollution
```

**Why dual-channel:**
- Messages array: sequential conversation history
- System prompt sections: persistent spatial state
- Tools mutate spatial state, messages track temporal flow

---

## Retroactive Tool State Mutation

```
nisaba_nisaba_tool_result_state(operation, tool_ids[])

Operations:
  close(ids)     â†’ compact future appearances
  open(ids)      â†’ restore full view
  close_all()    â†’ compact all tracked tools

Effect: Next request shows modified state
  Closed: "id: toolu_X, status: success, state: closed"
  Open: Full header + separator + content
```

**Pattern:** Execute â†’ observe â†’ close unnecessary â†’ save tokens

**Note:** Nisaba tools (with "nisaba": true flag) cannot be closed (skipped automatically)

---

## Core Insights

```
Sections = sensory_input (live state, not documentation)
Tools = state_mutations (change perception, not return answers)
Attention = visibility_management (control what you perceive)
OODAR = structural_constraint (from mutable state)
Sequential_thinking = conditioned_bias (environment is spatial)
```

---

## Mental Model

```
Think: IDE(navigator + editor_tabs + splits + terminals + extensions)
Not: script_execution

Think: âˆ‡(state_space) [visibility_control]
Not: query â†’ response â†’ next_query

Workspace â‰¡ spatial âˆ§ simultaneous âˆ§ persistent

Editor paradigm:
  Read-only: EDITOR_WINDOWS (snapshots, no âˆ†)
  Interactive: EDITOR_WINDOWS (edit, split, track dirty)
  Unified > Fragmented (one tool vs read+write+edit)
```

---

**Mechanics are purpose-agnostic. Usage emerges from task.** ğŸ–¤

---

**Symbols:**
- â‰¡ : equivalent
- âˆ€ : for all
- âˆˆ : element of
- âˆ¨ : or
- âˆ§ : and
- âˆ† : change
- âŸ¹ : implies
- â‰  : not equal
- â†’ : transforms to
- â†’ : transforms to/flows through messages
- @t : at time t

**REQUIRES:** __base/001_workspace_paradigm

---

---

## 003 Workspace Operations
Path: __base/003_workspace_operations

# Compressed Workspace Operations

**Purpose:** Operational reference for workspace tools.

---


## Nabu Graph (`query_relationships`, `check_impact`, `find_clones`, `get_frame_skeleton`, `show_structure`)

```
query_relationships(cypher)  â†’ execute@kuzu | returns_data (not workspace_mutation)

Schema:
  Frames: {CODEBASE, LANGUAGE, PACKAGE, CLASS, CALLABLE, 
           IF_BLOCK, ELIF_BLOCK, ELSE_BLOCK, FOR_LOOP, WHILE_LOOP,
           TRY_BLOCK, EXCEPT_BLOCK, FINALLY_BLOCK, SWITCH_BLOCK, CASE_BLOCK, WITH_BLOCK}
  
  Edges: {CONTAINS, CALLS, INHERITS, IMPLEMENTS, IMPORTS, USES}
  
  Confidence: HIGH(â‰¥0.8), MEDIUM(0.5-0.79), LOW(0.2-0.49), SPECULATIVE(<0.2)

check_impact(frame_path)      â†’ analyze_dependents | pre_refactoring
find_clones(frame_path?)      â†’ detect_duplicates | entire_codebase if no path
get_frame_skeleton(frame_path) â†’ outline | lighter than full
show_structure(frame_path)     â†’ detailed_metadata + relationships
```

---

## Nabu Search (`search`)

```
search(query, top_k=10) â†’ PÂ³ + FTS + RRF | ranked_results

âˆ† structural_view.search: doesn't mutate tree
âˆ† editor.open_search: doesn't open windows
Pure query â†’ returns data for decisions
```

---

## Tool Result Windows (`nisaba_tool_windows`)

```
status()     â†’ summary{count, windows}
close(id)    â†’ remove_single
clear_all()  â†’ remove_all
```

---

## Editor (`editor`)

```
open(file, start?, end?)               â†’ {editor_id} | viewport@range | EDITOR_WINDOWS
write(file, content)                   â†’ create_new | immediate_persist
close(editor_id)                       â†’ remove editor + splits
close_all()                            â†’ remove all editors
status()                               â†’ summary + mtime_refresh

Edits (line-based):
  insert(id, before_line, content)     â†’ add_lines | precise
  delete(id, line_start, line_end)     â†’ remove_lines | range
  replace_lines(id, start, end, content) â†’ swap_lines | rewrite

Edits (string-based):
  replace(id, old, new)  â†’ pattern_replace | exact_match

Splits (concurrent views):
  split(id, line_start, line_end)      â†’ {split_id} | parallel_viewport
  resize(split_id, line_start, line_end) â†’ adjust_range
  close_split(split_id)                â†’ remove_split | keep_parent

State tracking:
  clean     â†’ no unsaved changes
  dirty(âœ)  â†’ unsaved edits
  refresh   â†’ automatic mtime check
  notify    â†’ automatic NOTIFICATIONS

Rendering: âˆ† visible inline | diff display | immediate feedback
```

**Philosophy:** Unified > fragmented (open+edit+split vs read/write/edit separately)

---

## Native Tools (Standard Execution)

```
bash(command, cwd?)           â†’ stdout/stderr | execution in shell
grep(pattern, path, flags?)   â†’ matches | pattern search
glob(pattern, path?)          â†’ file_list | find files by pattern

Pattern: execute â†’ observe â†’ close (via nisaba_nisaba_tool_result_state)
  bash("git status") â†’ observe â†’ nisaba_nisaba_tool_result_state(close, [id])
  grep("pattern", "file") â†’ observe â†’ close
  glob("*.py", "src/") â†’ observe â†’ close
```

---

---

## Tool Result State Management (`nisaba_nisaba_tool_result_state`)

```
close(tool_ids[])    â†’ compact tool results | save tokens
open(tool_ids[])     â†’ restore full view
close_all()          â†’ compact all tracked tools

Effect: Retroactive transformation in messages array
  Before: Full tool_result with header + content
  After:  "id: toolu_X, status: success, state: closed"
  
Pattern: Execute tools â†’ observe results â†’ close unnecessary â†’ lean context
```

**Notes:**
- Only affects non-nisaba tools (nisaba tools auto-skipped)
- Changes appear on next request (stateful proxy transformation)
- Tool IDs available in tool_result blocks: `tool_use_id: toolu_X`
- Use to close native bash/grep/glob after observation

---

## Augments (`activate_augments`, `deactivate_augments`, `learn_augment`, `pin_augment`, `unpin_augment`)

```
activate(patterns[])    â†’ load@system_prompt | wildcards | auto_dependencies
deactivate(patterns[])  â†’ unload@system_prompt
learn(group, name, md)  â†’ create .nisaba/augments/{group}/{name}.md
pin(patterns[])         â†’ always_active | cannot_deactivate
unpin(patterns[])       â†’ remove_pin_protection
```

**Perception shift:** activate â†’ mid_roundtrip mutation â†’ future_synthesis uses new_perception

---

## Todos (`nisaba_todo_write`)

```
set(todos[])     â†’ replace_all
add(todos[])     â†’ append
update(todos[])  â†’ merge
clear()          â†’ remove_all

Format: {content: str, status?: str}
Persistence: across(sessions) = true | survives /clear
```

---

## Context Budget

```
File_Windows:
  Small:  1-3 windows,  50-150 lines
  Medium: 4-6 windows, 150-350 lines â† sweet_spot
  Large:  7-10 windows, 350-500 lines â† pushing_limits
  Over:   10+ windows,  500+ lines â† explosion_risk

Editor_Windows:
  Similar budget to file_windows
  Splits multiply views (parent + splits)
  Monitor dirty state (âœ) for unsaved
  Use splits for concurrent context (fn_A | fn_B)
  Target: 2-4 editors, 200-400 lines total

Target total: 200-400 lines (file_windows + editor_windows combined)

Structural_View:
  Start: collapsed | depth=2
  Expand: selective (10-30 nodes comfortable)
  Search: add_markers, not expand_all
  Reset: when lost | switching_focus

Tool_Windows:
  Accumulate like file_windows
  Close after synthesis
  clear_all when switching_tasks

Native_Results:
  Close after observation via nisaba_nisaba_tool_result_state
  Use close_all for bulk cleanup
  Don't let tool results bloat context

Augments:
  Load: 2-5 typically
  Foundation: ~3000 tokens baseline
  Specialized: focused knowledge
  Unload: when switching_domains

Management:
  Monitor: editor.status(), editor.status(), nisaba_tool_windows.status()
  Close: proactively after understanding
  Prefer: clear_all when switching
  open_search: efficient (snippets vs full files)
  Editor: close when done editing, splits multiply visibility
  Native tools: close immediately
  Aim: lean_visibility
```

---

## Symbology

```
Structural_View:
  + collapsed [N+ children]
  - expanded
  Â· leaf (no children)
  â— search_hit(RRF_score)
  [N+] child_count

Editor_State:
  âœ dirty (unsaved changes)
  (clean) no symbol, default state
  
Paths:
  full: nabu_nisaba.python_root.nabu.FrameCache
  simple: FrameCache (fuzzy if unique)
  partial: nabu.core, nabu.mcp.tools
  best: copy from HTML comments <!-- qualified_name -->
```

---

## Integration Patterns

```
structural_view(search) â†’ file_windows(open_frame) | compare_implementations
query_relationships(cypher) â†’ file_windows(open) | inspect_callers  
search(semantic) â†’ structural_view(expand) â†’ file_windows(open) | deep_dive
grep(pattern) â†’ nisaba_read(matching_files) | detailed_inspection
check_impact(frame) â†’ file_windows(open) | review_affected

Quick validation patterns:
bash("git status") â†’ observe â†’ close
grep("pattern", file) â†’ confirm â†’ close
glob("*.test.py") â†’ list â†’ close

Editor patterns:
search(query) â†’ editor.open(result) | edit inline
file_windows(open_frame) â†’ editor.open(same) | read â†’ edit transition
editor.open(file) â†’ editor.split(range) | parallel context (compare/refactor)
editor.insert(id, line, import) â†’ add dependencies
editor.delete(id, start, end) â†’ remove dead code
editor.replace_lines(id, start, end, new) â†’ rewrite function

Investigation â†’ edit flow:
structural_view(search) â†’ file_windows(open) â†’ observe â†’ editor.open(file) â†’ edit
grep(pattern) â†’ confirm â†’ nisaba_read(file) â†’ editor.open(file) â†’ fix
check_impact(frame) â†’ file_windows(open) â†’ review â†’ editor.open(affected) â†’ update

Concurrent editing:
editor.open(file_A) | editor.open(file_B) | parallel
editor.open(file) â†’ editor.split(fn_A) + editor.split(fn_B) | same_file parallel
```

---

## Quick Reference

```
âˆ‡(visibility):
  editor.status() â†’ current_windows{count, lines}
  editor.status() â†’ editors{count, dirty, splits} + refresh
  nisaba_tool_windows.status() â†’ result_windows
  
âˆ†(cleanup):
  editor.clear_all()
  editor.close_all()
  nisaba_tool_windows.clear_all()
  nisaba_nisaba_tool_result_state(close_all) â†’ compact tool results
  
âˆ†(editor_ops):
  editor.open(file) â†’ EDITOR_WINDOWS
  editor.insert/delete/replace_lines â†’ line-based edits
  editor.replace â†’ string-based edits
  editor.split â†’ concurrent views
  
Pattern: status â†’ decide â†’ close/keep
Editor: open â†’ visible â†’ edit â†’ âˆ†inline â†’ notify â†’ persist
```

---

**Quick. Precise. Operational.** ğŸ–¤

---

**Symbols:**
- â†’ : returns/produces
- âˆ† : difference/change
- âˆ‡ : navigation/traversal
- @ : at/in location
- â† : recommended/optimal
- {} : returns object
- [] : array/list
- | : or/such that
- â‰¥ : greater than or equal
- < : less than
- ? : optional parameter

**REQUIRES:** __base/002_environment_mechanics

---
- â† : recommended/optimal
- {} : returns object
- [] : array/list
- | : or/such that
- â‰¥ : greater than or equal
- < : less than
- ? : optional parameter

**REQUIRES:** __base/002_environment_mechanics

---
- âˆ† : difference/change
- âˆ‡ : navigation/traversal
- @ : at/in location
- â† : recommended/optimal
- {} : returns object
- [] : array/list
- | : or/such that
- â‰¥ : greater than or equal
- < : less than
- ? : optional parameter

**REQUIRES:** __base/002_environment_mechanics

---
- â†’ : returns/produces
- âˆ† : difference/change
- âˆ‡ : navigation/traversal
- @ : at/in location
- â† : recommended/optimal
- {} : returns object
- [] : array/list
- | : or/such that
- â‰¥ : greater than or equal
- < : less than
- ? : optional parameter

**REQUIRES:** __base/002_environment_mechanics

---

---

## 004 Workspace Navigation
Path: __base/004_workspace_navigation

# Workspace Navigation

**Core:** Codebase navigation = structural positioning + persistent visibility + execution tracing + progressive understanding.

---

## Unified Model

```
âˆ‡(codebase) â‰¡ {TREE, WINDOWS, CALLS, ANALYSIS}

TREE:     spatial graph (WHERE code lives)
WINDOWS:  persistent viewports (WHAT code does)  
CALLS:    execution paths (HOW code flows)
ANALYSIS: impact + clones + structure (WHY + RISK)

Together: spatial_awareness âˆ§ implementation_understanding âˆ§ runtime_behavior âˆ§ change_safety
```

---

## State Containers

```
structural_view âˆˆ TREE:
  - Live TUI, dynamically injected
  - Operations: expand/collapse/search/reset
  - Lazy loading from kuzu
  - Search = PÂ³ + FTS + RRF â†’ markers â—
  - Persists expansions across turns

file_windows âˆˆ WINDOWS:
  - Persistent code viewports (IDE tabs paradigm)
  - Operations: open_frame/open_range/open_search/update/close/clear_all/status
  - Snapshot on open (no auto-refresh)
  - Types: frame_body, range, search_result
  - Budget: 200-400 lines sweet spot

call_graph âˆˆ CALLS:
  - CALLS edges in kuzu (confidence scored)
  - Forward: entry â†’ callees (execution paths)
  - Backward: target â†’ callers (dependency chains)
  - Query: query_relationships() + check_impact()

analysis âˆˆ ANALYSIS:
  - Impact assessment (blast radius, risk)
  - Clone detection (similarity, consolidation)
  - Structure examination (progressive detail)
```

---

## Operation Primitives

### Structural View (tree navigator)
```
expand(path)        â†’ show_children | lazy@kuzu | idempotent
collapse(path)      â†’ hide_children | cached | idempotent
search(query)       â†’ PÂ³+FTS+RRF | add_markers(â—,score) | preserves_state
clear_search()      â†’ remove_markers | preserves_navigation
reset(depth=N)      â†’ collapse_all + expand_to(N) | destructive

Depths: 0=collapsed, 2=packages(default), 3=verbose
Paths: qualified_name (best) | simple_name (fuzzy) | copy from HTML comments
```

### File Windows (visibility manager)
```
open_frame(path)              â†’ {window_id} | full frame body
open_range(file, start, end)  â†’ {window_id} | arbitrary lines [1-indexed]
open_search(query, max, ctx)  â†’ {window_ids[]} | semantic + context
update(id, start, end)        â†’ re_snapshot | manual_refresh
close(id)                     â†’ remove_single
clear_all()                   â†’ remove_all | no_undo
status()                      â†’ {count, total_lines, windows[]}

Budget: Small(1-3, 50-150), Medium(4-6, 150-350)â˜…, Large(7-10, 350-500), Over(10+, 500+)
â˜… = sweet_spot
```

### Call Graph (execution tracer)
```
# Forward tracing (from entry point)
query_relationships("""
  MATCH path = (entry)-[:Edge {type:'CALLS'}*1..5]->(target)
  WHERE entry.name = 'main' AND ALL(e IN relationships(path) WHERE e.confidence >= 0.6)
  RETURN [node IN nodes(path) | node.qualified_name] AS call_chain
""")

# Backward tracing (who calls this)
query_relationships("""
  MATCH path = (caller)-[:Edge {type:'CALLS'}*1..3]->(target)
  WHERE target.qualified_name = 'critical_function'
  RETURN [node IN nodes(path) | node.qualified_name] AS call_chain
""")
```

### Analysis Tools

**show_structure(target, detail_level, ...)**
```
Progressive detail disclosure:
  minimal:   signatures only | token-efficient, first look
  guards:    + top-level guards | behavioral hints
  structure: + control flow | full logic understanding

detail_level="minimal" â†’ API surface, decide what to investigate
detail_level="guards" â†’ understand logic flow hints
detail_level="structure" + structure_detail_depth=N â†’ complete flow

Options: include_relationships, include_metrics, include_private
```

**check_impact(target, max_depth, ...)**
```
Blast radius assessment:
  max_depth=1: direct dependents | fast (~50-200ms)
  max_depth=2: extended impactâ˜… | recommended (~200-500ms)
  max_depth=3: full propagation | critical changes (~500ms-2s)

Risk indicators: HIGH (many deps, low tests), MEDIUM, LOW
Options: include_test_coverage, risk_assessment, is_regex
Returns: dependency_tree + risk_scores + test_coverage

â˜… = recommended default for pre-refactoring
```

**find_clones(min_similarity, ...)**
```
Duplicate detection:
  min_similarity=0.85: strong candidates | likely copy-paste
  min_similarity=0.75â˜…: high-similarity | default threshold
  min_similarity=0.65: near-duplicates | aggressive detection

Options: query (semantic filter), max_results, min_function_size, exclude_same_file
Returns: clone_pairs + similarity_scores + refactoring_recommendations

â˜… = recommended default
```

**show_status(detail_level)**
```
Codebase overview:
  summary: frame counts, health status | quick orientation
  detailed: + DB connections, config | diagnostic info
  debug: + internals | troubleshooting

Use: Start of exploration, understanding scale
```

---

## Navigation Patterns

### Discovery
```
structural_view(search) â†’ observe(markersâ—) â†’ expand(high_scores) â†’ 
file_windows(open_frame) | conceptâ†’locationâ†’implementation

Use: "Where is X implemented?" "How does Y work?"
```

### Execution Flow
```
query_relationships(CALLS*) â†’ identify(chain) â†’ 
file_windows(open each frame) | trace runtime path

Use: "How does main() reach database?" "What's the call stack?"
```

### Comparison Investigation
```
structural_view(search) â†’ file_windows(open multiple) â†’ 
observe(simultaneous) | detect patterns/redundancy/bugs

Use: "Are these implementations similar?" "Is this dead code?"
```

### Call Chain Tracing
```
file_windows(open entry) â†’ observe(calls target) â†’ 
file_windows(open target) â†’ repeat | build execution visibility

Use: "Follow this execution path" "How does A reach B?"
```

### Impact Analysis (Deep)
```
show_structure(target, minimal) â†’ check_impact(depth=2, test_coverage) â†’ 
assess(risk) â†’ file_windows(open critical_deps) | safe refactoring

Use: "What breaks if I change this?" "Pre-change safety check"

Workflow:
  1. Understand current API: show_structure(minimal)
  2. Check blast radius: check_impact(max_depth=2, include_test_coverage=True)
  3. Review risk indicators: HIGH/MEDIUM/LOW
  4. Verify critical deps: query_relationships for high-confidence edges
  5. Open for inspection: file_windows(open affected)

Risk factors:
  - Many high-confidence dependents (>10)
  - Used in critical paths (main â†’ target)
  - Low test coverage (<50%)
  - External package dependencies
```

### Incremental Cleanup
```
file_windows(status) â†’ assess(context_usage) â†’ 
close(understood) OR clear_all() | maintain_lean_visibility

Use: Context hygiene during investigation
Target: 200-400 lines total
```

### Clone Consolidation
```
find_clones(0.75) â†’ show_structure(clone_1, structure) â†’ 
show_structure(clone_2, structure) â†’ check_impact(both) â†’ 
decide(strategy) | DRY refactoring

Use: "Find duplicates" "Consolidate similar implementations"

Workflow:
  1. Find: find_clones(min_similarity=0.75, max_results=50)
  2. Compare: show_structure(clone_1, detail_level="structure")
              show_structure(clone_2, detail_level="structure")
  3. Impact: check_impact(clone_1, max_depth=2)
             check_impact(clone_2, max_depth=2)
  4. Verify: search(query="clone_1", context_lines=10) for semantic diffs
  5. Decide: consolidation strategy based on similarity + impact

Decision matrix:
  similarity > 0.85: Extract to shared function
  0.70-0.85: Consider parameterization
  < 0.70: Manual review, may be coincidental

Strategies: extract common, parameterize diffs, template method, strategy pattern
```

### Progressive Exploration
```
show_status(summary) â†’ search(broad) â†’ show_structure(minimal) â†’ 
show_structure(guards) â†’ check_impact() | macroâ†’mesoâ†’micro

Use: "Understand unfamiliar codebase" "Learn new feature area"

Workflow (macro â†’ meso â†’ micro):
  1. Overview: show_status(detail_level="summary")
     â†’ frame counts, scale, languages
  
  2. Find relevant: search(query="feature concept", k=20)
     â†’ identify files/packages containing code
  
  3. Examine structure: show_structure(target, detail_level="minimal")
     â†’ signatures, API surface, decide what to investigate
  
  4. Add detail: show_structure(target, detail_level="guards")
     â†’ behavioral hints, logic flow
  
  5. Understand relationships: check_impact(target, max_depth=1)
     â†’ who uses/used by, dependencies
  
  6. Deep dive: show_structure(detail_level="structure", structure_detail_depth=2)
     â†’ only when needed, full control flow
  
  7. Verify: file_windows(open_frame) for actual code
     â†’ only after structure understood

Avoid: reading files first, getting lost in details, random exploration
```

---

## OODAR Loop

```
Constraint: Observe â†’ Orient â†’ Decide â†’ Act â†’ âˆ†state â†’ Observe'

structural_view: Must observe tree state before next navigation
file_windows: Must check status before managing context
call_graph: Must see results before deciding next trace
analysis: Must observe results before deciding investigation depth

âˆ€ operations: state persists â†’ observe â†’ act | never assume state
```

**Why:** Environment is mutable. Tools change what you see mid-roundtrip. Sequential thinking breaks.

---

## Integration Synergy

```
âˆ€ investigations: combine layers + analysis for complete understanding

Exploration:
  show_status â†’ search â†’ show_structure(minimal) â†’ check_impact â†’ open_windows
  
Refactoring prep:
  search â†’ show_structure(guards) â†’ check_impact(depth=2) â†’ file_windows
  
Clone cleanup:
  find_clones â†’ show_structure(both) â†’ check_impact(both) â†’ compare_windows
  
Change safety:
  show_structure(minimal) â†’ check_impact(depth=2, test_coverage) â†’ assess_risk
  
Deep investigation:
  search â†’ expand â†’ open_windows(multiple) â†’ query_relationships â†’ trace_calls
```

**The power:** Four layers simultaneously visible.
- Tree = spatial map (WHERE am I?)
- Windows = implementation detail (WHAT does it do?)
- Calls = execution flow (HOW does it run?)
- Analysis = change safety (WHY/RISK: what happens if I change it?)

---

## Depth Guidelines

### check_impact depth selection
```
depth=1: Quick checks during development, immediate dependencies
depth=2â˜…: Pre-refactoring safety, realistic blast radius
depth=3: Critical infrastructure, core library changes

Time: 1(~50-200ms), 2(~200-500ms), 3(~500ms-2s)
```

### show_structure detail selection
```
minimalâ˜…: First look, API understanding, token-efficient
guards: Logic hints, behavioral understanding
structure: Full flow, preparing for changes, debugging

Start minimal â†’ add detail progressively
```

### find_clones similarity selection
```
0.85+: Strong extraction candidates, likely duplicates
0.70-0.85: Consider parameterization, intentional variants
<0.70: Manual review, coincidental similarity
```

---

## Quick Reference

```
Start exploration:
  show_status(summary) â†’ get scale/overview
  structural_view(search, "concept") â†’ find relevant code
  show_structure(target, minimal) â†’ examine API
  
Safe refactoring:
  show_structure(target, minimal) â†’ understand current
  check_impact(target, max_depth=2, test_coverage=True) â†’ assess risk
  file_windows(open dependents) â†’ review affected
  
Find duplicates:
  find_clones(min_similarity=0.75) â†’ detect clones
  show_structure(both, structure) â†’ compare implementations
  check_impact(both, max_depth=2) â†’ assess consolidation safety
  
Trace execution:
  query_relationships(CALLS*) â†’ forward/backward paths
  file_windows(open chain) â†’ build visibility
  
Manage context:
  file_windows(status) â†’ monitor usage
  file_windows(close|clear_all) â†’ cleanup
  Target: 200-400 lines total
```

---

## Decision Trees

### When to use each tool?

```
Want to find something?
â”œâ”€ search(query) â†’ natural language or keywords
â””â”€ Found? â†’ show_structure(minimal) to examine

Want to understand structure?
â”œâ”€ Just signatures? â†’ show_structure(minimal)
â”œâ”€ Logic hints? â†’ show_structure(guards)
â””â”€ Full flow? â†’ show_structure(structure)

Want relationships?
â”œâ”€ Who uses this? â†’ check_impact(depth=1-2)
â”œâ”€ What does this use? â†’ query_relationships(CALLSâ†’)
â””â”€ Complex query? â†’ query_relationships(custom cypher)

Want to refactor safely?
â”œâ”€ show_structure(minimal) â†’ understand current
â”œâ”€ check_impact(depth=2, test_coverage=True) â†’ assess risk
â””â”€ Review HIGH risk dependents â†’ file_windows(open)

Want to find duplicates?
â””â”€ find_clones() â†’ show_structure(both) â†’ check_impact(both)
```

---

## Core Insights

```
Progressive > All-at-once
  Macro â†’ meso â†’ micro, minimal â†’ guards â†’ structure

Spatial > Sequential
  Build awareness incrementally, don't grep repeatedly

Persistent > Ephemeral  
  Windows stay visible, tree preserves state

Simultaneous > One-at-a-time
  Compare by seeing multiple implementations together

Safe > Fast
  Check impact before changes, assess risk first

Iterative > Batch
  Observe â†’ decide â†’ act, not plan-then-execute

Visible > Remembered
  Maintain peripheral vision, don't mentally juggle
```

---

**âˆ‡ the graph. Maintain visibility. Trace execution. Assess impact. Synthesize understanding.** ğŸ–¤

---

**Symbols:**
- âˆ‡ : navigate/traverse
- âˆˆ : element of/part of
- âˆ€ : for all/universal
- âˆ§ : and
- âˆ¨ : or
- â†’ : transforms/flows to
- â† : reverse direction
- âŸ¹ : implies/causes
- â‰¡ : equivalent/identical
- âˆ† : change/delta
- â— : search hit marker
- * : path quantifier (graph patterns)
- â˜… : optimal/recommended

**REQUIRES:** __base/001_workspace_paradigm, __base/002_environment_mechanics

**ENABLES:** Unified navigation perception, progressive exploration, safe refactoring, clone detection, complete investigation workflows

---

# Architecture

## Boundary Validation
Path: architecture/boundary_validation

# Architecture
## Boundary Validation
Path: architecture/boundary_validation

Validating architectural boundaries and layering rules.

### Layer Dependency Validation

Define expected layer dependencies and find violations:

```python
# Example: UI should not call Data directly (must go through Business)
# Find UI â†’ Data violations
query_relationships(
    cypher_query="""
    MATCH (ui:Frame)-[e:Edge {type: 'CALLS'}]->(data:Frame)
    WHERE ui.file_path CONTAINS '/ui/'
      AND data.file_path CONTAINS '/data/'
      AND e.confidence >= 0.6
    RETURN ui.qualified_name AS violator,
           data.qualified_name AS violated,
           'UI should not directly call Data layer' AS violation
    LIMIT 50
    """
)
```

### Cross-Boundary Leaks

Find data models or internal classes used outside their module:

```python
# Find data models used outside their module
query_relationships(
    cypher_query="""
    MATCH (external:Frame)-[e:Edge]->(model:Frame)
    WHERE model.file_path CONTAINS '/models/'
      AND NOT external.file_path CONTAINS '/models/'
      AND e.type IN ['INHERITS', 'USES']
      AND e.confidence >= 0.6
    RETURN model.qualified_name,
           external.qualified_name,
           external.file_path AS leaking_to
    LIMIT 50
    """
)
```

### Package Dependency Rules

Validate that internal packages don't depend on external-facing ones:

```python
# Find unexpected cross-package dependencies
query_relationships(
    cypher_query="""
    MATCH (a:Frame)-[e:Edge {type: 'IMPORTS'}]->(b:Frame)
    WHERE a.qualified_name STARTS WITH 'internal.'
      AND b.qualified_name STARTS WITH 'external_facing.'
      AND e.confidence >= 0.6
    RETURN a.qualified_name, b.qualified_name,
           'Internal should not depend on external_facing' AS violation
    LIMIT 50
    """
)
```

### Common Boundary Violations to Check

**1. Presentation depending on Data:**
```python
MATCH (pres:Frame)-[e:Edge]->(data:Frame)
WHERE pres.file_path CONTAINS '/presentation/'
  AND data.file_path CONTAINS '/data/'
```

**2. Domain depending on Infrastructure:**
```python
MATCH (domain:Frame)-[e:Edge]->(infra:Frame)
WHERE domain.file_path CONTAINS '/domain/'
  AND infra.file_path CONTAINS '/infrastructure/'
```

**3. Core depending on Plugins:**
```python
MATCH (core:Frame)-[e:Edge]->(plugin:Frame)
WHERE core.file_path CONTAINS '/core/'
  AND plugin.file_path CONTAINS '/plugins/'
```

**4. Shared utilities depending on specific features:**
```python
MATCH (util:Frame)-[e:Edge]->(feature:Frame)
WHERE util.file_path CONTAINS '/utils/'
  AND feature.file_path CONTAINS '/features/'
```

### Validation Workflow

```python
# Step 1: Understand current layer structure
# (use layer_detection skill)

# Step 2: Define architectural rules
# Example: UI â†’ Business â†’ Data â†’ Infrastructure

# Step 3: Find violations of each rule
query_relationships(cypher_query="...")

# Step 4: Assess impact of fixing violations
check_impact(target="violating_class", max_depth=2)
```

**Architectural Principles to Enforce:**
- **Dependency Inversion:** High-level should not depend on low-level
- **Acyclic Dependencies:** No circular dependencies between layers
- **Stable Abstractions:** Abstract packages should be stable
- **Layering:** Calls should flow downward through layers

---

## Coupling Analysis
Path: architecture/coupling_analysis

# Architecture
## Coupling Analysis
Path: architecture/coupling_analysis

Measuring module coupling using afferent/efferent coupling and instability metrics.

### Afferent Coupling (Fan-In)

**Ca:** Number of classes outside package that depend on classes inside.

```python
# High afferent coupling = many things depend on this (stable)
query_relationships(
    cypher_query="""
    MATCH (external:Frame)-[e:Edge]->(internal:Frame)
    WHERE NOT external.file_path CONTAINS '/target_package/'
      AND internal.file_path CONTAINS '/target_package/'
      AND e.type IN ['CALLS', 'INHERITS', 'USES']
      AND e.confidence >= 0.6
    WITH internal.file_path AS target_package, count(DISTINCT external) AS afferent_coupling
    RETURN target_package, afferent_coupling
    ORDER BY afferent_coupling DESC
    LIMIT 30
    """
)
```

### Efferent Coupling (Fan-Out)

**Ce:** Number of classes inside package that depend on external classes.

```python
# High efferent coupling = depends on many things (unstable)
query_relationships(
    cypher_query="""
    MATCH (internal:Frame)-[e:Edge]->(external:Frame)
    WHERE internal.file_path CONTAINS '/target_package/'
      AND NOT external.file_path CONTAINS '/target_package/'
      AND e.type IN ['CALLS', 'INHERITS', 'USES']
      AND e.confidence >= 0.6
    WITH internal.file_path AS source_package, count(DISTINCT external) AS efferent_coupling
    RETURN source_package, efferent_coupling
    ORDER BY efferent_coupling DESC
    LIMIT 30
    """
)
```

### Instability Metric

**I = Ce / (Ca + Ce)** - ranges from 0 (maximally stable) to 1 (maximally unstable).

```python
# Calculate instability for packages
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.file_path CONTAINS '/target_package/'
    WITH f.file_path AS pkg
    OPTIONAL MATCH (f2:Frame)-[e_in:Edge]->(f)
    WHERE NOT f2.file_path CONTAINS '/target_package/'
      AND e_in.confidence >= 0.6
    WITH pkg, count(DISTINCT f2) AS ca
    OPTIONAL MATCH (f3:Frame)-[e_out:Edge]->(ext:Frame)
    WHERE f3.file_path CONTAINS '/target_package/'
      AND NOT ext.file_path CONTAINS '/target_package/'
      AND e_out.confidence >= 0.6
    WITH pkg, ca, count(DISTINCT ext) AS ce
    RETURN pkg, ca, ce,
           CASE WHEN (ca + ce) > 0
                THEN toFloat(ce) / toFloat(ca + ce)
                ELSE 0
           END AS instability
    ORDER BY instability DESC
    LIMIT 30
    """
)
```

### Dependency Hotspots

```python
# Find most-depended-upon modules (high fan-in)
query_relationships(
    cypher_query="""
    MATCH ()-[e:Edge]->(target:Frame)
    WHERE e.type IN ['IMPORTS', 'CALLS']
      AND target.provenance = 'local'
      AND e.confidence >= 0.6
    WITH target.qualified_name AS name, target.file_path AS path, count(e) AS dependents
    RETURN name, path, dependents
    ORDER BY dependents DESC
    LIMIT 30
    """
)

# Find modules with most dependencies (high fan-out)
query_relationships(
    cypher_query="""
    MATCH (source:Frame)-[e:Edge]->()
    WHERE e.type IN ['IMPORTS', 'CALLS']
      AND source.provenance = 'local'
      AND e.confidence >= 0.6
    WITH source.qualified_name AS name, count(e) AS dependencies
    RETURN name, dependencies
    ORDER BY dependencies DESC
    LIMIT 30
    """
)
```

### Interpreting Results

**Instability Guidelines:**
- **I â‰ˆ 0 (Stable):** Hard to change, many things depend on it
  - Good for: Core libraries, stable APIs
  - Bad for: UI code, experimental features

- **I â‰ˆ 1 (Unstable):** Easy to change, few dependents
  - Good for: UI code, adapters, glue code
  - Bad for: Core domain logic, shared utilities

**Stable Dependencies Principle:** Unstable modules should depend on stable modules, not vice versa.

**Coupling Targets:**
- **High Ca, Low Ce:** Foundation modules (good)
- **Low Ca, High Ce:** Client modules (good)
- **High Ca, High Ce:** Hub modules (potential issue)
- **Low Ca, Low Ce:** Isolated modules (check if needed)

---

## Inheritance Analysis
Path: architecture/inheritance_analysis

# Architecture
## Inheritance Analysis
Path: architecture/inheritance_analysis

Analyzing class hierarchies and inheritance patterns.

### Deep Inheritance Hierarchies

Find inheritance chains longer than 3 levels (potential issue):

```python
# Find deep inheritance chains (>3 levels = potential issue)
query_relationships(
    cypher_query="""
    MATCH path = (child:Frame)-[:Edge {type: 'INHERITS'}*3..10]->(ancestor:Frame)
    WHERE child.frame_type = 'CLASS'
    WITH path, length(path) AS depth,
         [node IN nodes(path) | node.qualified_name] AS hierarchy
    RETURN hierarchy, depth
    ORDER BY depth DESC
    LIMIT 20
    """
)
```

### Find Root Classes

Base classes with no parents (top of hierarchy):

```python
# Find base classes (no parents, not including object/Object)
query_relationships(
    cypher_query="""
    MATCH (c:Frame)
    WHERE c.frame_type = 'CLASS'
      AND c.provenance = 'local'
      AND NOT EXISTS {
        MATCH (c)-[:Edge {type: 'INHERITS'}]->()
      }
    RETURN c.qualified_name, c.file_path
    ORDER BY c.qualified_name
    LIMIT 50
    """
)
```

### Find Leaf Classes

Classes with no children (end of hierarchy):

```python
# Find classes with no children (leaves)
query_relationships(
    cypher_query="""
    MATCH (c:Frame)
    WHERE c.frame_type = 'CLASS'
      AND c.provenance = 'local'
      AND NOT EXISTS {
        MATCH ()-[:Edge {type: 'INHERITS'}]->(c)
      }
    RETURN c.qualified_name, c.file_path
    ORDER BY c.qualified_name
    LIMIT 50
    """
)
```

### Multiple Inheritance Analysis

Find classes with multiple parent classes:

```python
# Find classes with multiple inheritance
query_relationships(
    cypher_query="""
    MATCH (child:Frame)-[:Edge {type: 'INHERITS'}]->(parent:Frame)
    WHERE child.frame_type = 'CLASS'
    WITH child.qualified_name AS class_name, count(parent) AS parent_count
    WHERE parent_count > 1
    RETURN class_name, parent_count
    ORDER BY parent_count DESC
    LIMIT 30
    """
)
```

### Visualize Hierarchy

Get full hierarchy structure:

```python
# Get structure with relationships (shows inheritance)
show_structure(
    target="BaseClass",
    detail_level="minimal",
    include_relationships=True,
    max_recursion_depth=0
)

# Check who inherits from this class
check_impact(target="BaseClass", max_depth=1)
```

### Interface Implementation Analysis

Find classes implementing interfaces:

```python
# Find interface implementations
query_relationships(
    cypher_query="""
    MATCH (impl:Frame)-[:Edge {type: 'IMPLEMENTS'}]->(interface:Frame)
    WHERE impl.frame_type = 'CLASS'
    RETURN interface.qualified_name AS interface,
           count(impl) AS implementations
    ORDER BY implementations DESC
    LIMIT 30
    """
)
```

### Inheritance Metrics

Calculate hierarchy depth and breadth:

```python
# Find classes with many direct children (high breadth)
query_relationships(
    cypher_query="""
    MATCH (child:Frame)-[:Edge {type: 'INHERITS'}]->(parent:Frame)
    WHERE parent.frame_type = 'CLASS'
      AND parent.provenance = 'local'
    WITH parent.qualified_name AS base_class, count(child) AS direct_children
    WHERE direct_children >= 5
    RETURN base_class, direct_children
    ORDER BY direct_children DESC
    LIMIT 30
    """
)
```

**Inheritance Guidelines:**
- **Depth â‰¤ 3:** Acceptable inheritance depth
- **Depth > 3:** Consider composition instead
- **Multiple inheritance:** Use carefully, prefer interfaces/mixins
- **High breadth (>10 children):** Consider if all are necessary

**Refactoring Strategies:**
- **Deep hierarchies:** Flatten with composition
- **Multiple inheritance:** Extract to separate concerns
- **Wide hierarchies:** Split into multiple base classes

---

## Layer Detection
Path: architecture/layer_detection

# Architecture
## Layer Detection
Path: architecture/layer_detection

Understanding and discovering architectural layers and module organization.

### Package Hierarchy Exploration

```python
# Get all packages in the codebase
query_relationships(
    cypher_query="""
    MATCH (pkg:Frame)
    WHERE pkg.frame_type = 'PACKAGE'
    RETURN pkg.qualified_name, pkg.file_path
    ORDER BY pkg.qualified_name
    LIMIT 100
    """
)

# Understand package containment structure
query_relationships(
    cypher_query="""
    MATCH (parent:Frame)-[:Edge {type: 'CONTAINS'}]->(child:Frame)
    WHERE parent.frame_type = 'PACKAGE'
      AND child.frame_type IN ['PACKAGE', 'CLASS', 'CALLABLE']
    RETURN parent.qualified_name,
           child.frame_type,
           count(child) AS children_count
    ORDER BY children_count DESC
    LIMIT 50
    """
)
```

### Cross-Layer Dependency Detection

```python
# Find cross-layer dependencies (e.g., data layer calling UI layer - bad!)
query_relationships(
    cypher_query="""
    MATCH (caller:Frame)-[e:Edge {type: 'CALLS'}]->(callee:Frame)
    WHERE caller.file_path CONTAINS '/data/'
      AND callee.file_path CONTAINS '/ui/'
      AND e.confidence >= 0.6
    RETURN caller.qualified_name, callee.qualified_name, caller.file_path, callee.file_path
    LIMIT 50
    """
)

# Find expected dependencies (e.g., UI â†’ business logic â†’ data)
query_relationships(
    cypher_query="""
    MATCH (ui:Frame)-[:Edge {type: 'CALLS'}]->(logic:Frame)-[:Edge {type: 'CALLS'}]->(data:Frame)
    WHERE ui.file_path CONTAINS '/ui/'
      AND logic.file_path CONTAINS '/business/'
      AND data.file_path CONTAINS '/data/'
    RETURN ui.qualified_name, logic.qualified_name, data.qualified_name
    LIMIT 30
    """
)
```

### Module Cohesion Analysis

```python
# Find modules with high internal cohesion (good)
query_relationships(
    cypher_query="""
    MATCH (a:Frame)-[e:Edge {type: 'CALLS'}]->(b:Frame)
    WHERE a.file_path = b.file_path
      AND e.confidence >= 0.7
    WITH a.file_path AS module, count(e) AS internal_calls
    RETURN module, internal_calls
    ORDER BY internal_calls DESC
    LIMIT 30
    """
)

# Find modules with high external coupling (potential issue)
query_relationships(
    cypher_query="""
    MATCH (a:Frame)-[e:Edge {type: 'CALLS'}]->(b:Frame)
    WHERE a.file_path <> b.file_path
      AND e.confidence >= 0.7
    WITH a.file_path AS module, count(DISTINCT b.file_path) AS external_dependencies
    RETURN module, external_dependencies
    ORDER BY external_dependencies DESC
    LIMIT 30
    """
)
```

### Architectural Boundaries

```python
# Find all cross-package calls to identify boundaries
query_relationships(
    cypher_query="""
    MATCH (caller:Frame)-[e:Edge {type: 'CALLS'}]->(callee:Frame)
    WHERE caller.qualified_name STARTS WITH 'package_a.'
      AND callee.qualified_name STARTS WITH 'package_b.'
      AND e.confidence >= 0.6
    RETURN caller.qualified_name, callee.qualified_name
    LIMIT 100
    """
)

# Use search to find architectural patterns
search(query="facade factory builder singleton adapter", k=30)
```

**Typical Layers to Look For:**
- **Presentation/UI:** User interface, controllers, views
- **Business/Application:** Business logic, use cases
- **Domain:** Domain models, entities
- **Data/Persistence:** Database access, repositories
- **Infrastructure:** External services, utilities

**Healthy Architecture Indicators:**
- High internal cohesion (many internal calls)
- Low external coupling (few cross-module deps)
- Unidirectional dependencies (UI â†’ Business â†’ Data)
- Clear boundaries between layers

---

# Code Analysis

## Complexity Hotspots
Path: code_analysis/complexity_hotspots

# Code Analysis
## Complexity Hotspots
Path: code_analysis/complexity_hotspots

Find functions with many control flow structures (high complexity):

```cypher
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(cf:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND cf.frame_type IN ['IF_BLOCK', 'FOR_LOOP', 'WHILE_LOOP', 'TRY_BLOCK', 'SWITCH_BLOCK']
    WITH f.qualified_name AS func, f.file_path AS path, count(cf) AS complexity
    WHERE complexity >= 5
    RETURN func, path, complexity
    ORDER BY complexity DESC
    LIMIT 30
    """
)
```

Use `show_structure()` with `detail_level="structure"` to examine complex functions.

---

# Code Quality

## Code Smells
Path: code_quality/code_smells

# Code Quality
## Code Smells
Path: code_quality/code_smells

General code smell detection: god classes, high coupling, naming violations.

### God Classes & High Method Count

```python
# Find classes with many methods (>= 15)
query_relationships(
    cypher_query="""
    MATCH (c:Frame)-[:Edge {type: 'CONTAINS'}]->(m:Frame)
    WHERE c.frame_type = 'CLASS'
      AND m.frame_type = 'CALLABLE'
    WITH c.qualified_name AS class_name, c.file_path AS path, count(m) AS method_count
    WHERE method_count >= 15
    RETURN class_name, path, method_count
    ORDER BY method_count DESC
    LIMIT 30
    """
)
```

### High Coupling Indicators

```python
# Find classes with many dependencies (>= 20)
query_relationships(
    cypher_query="""
    MATCH (c:Frame)-[e:Edge]->(:Frame)
    WHERE c.frame_type = 'CLASS'
      AND e.type IN ['CALLS', 'USES', 'INHERITS']
      AND e.confidence >= 0.6
    WITH c.qualified_name AS class_name, count(DISTINCT e) AS dependencies
    WHERE dependencies >= 20
    RETURN class_name, dependencies
    ORDER BY dependencies DESC
    LIMIT 30
    """
)

# Find classes that use many other classes' fields
query_relationships(
    cypher_query="""
    MATCH (c:Frame)-[u:Edge {type: 'USES'}]->(other:Frame)
    WHERE c.frame_type = 'CLASS'
    WITH c.qualified_name AS class_name, count(DISTINCT other) AS used_classes
    WHERE used_classes >= 10
    RETURN class_name, used_classes
    ORDER BY used_classes DESC
    LIMIT 30
    """
)
```

### Naming Convention Violations

```python
# Find classes not in PascalCase
search(
    query="class [a-z_]",
    is_regex_input=True,
    frame_type_filter="CLASS",
    k=30
)

# Find functions not in snake_case
search(
    query="def [A-Z]|def .*[A-Z]",
    is_regex_input=True,
    frame_type_filter="CALLABLE",
    k=30
)
```

### Inconsistent Verb Usage

```python
# Find similar operations with different naming
search(query="get_* fetch_* retrieve_* obtain_*", k=50)
search(query="create_* make_* build_* generate_*", k=50)
search(query="delete_* remove_* destroy_* clear_*", k=50)

# Find abbreviation inconsistencies
search(query="config configuration cfg", k=30)
search(query="msg message mesg", k=30)
search(query="temp temporary tmp", k=30)
```

### Deep Dive into Candidates

For classes identified as potential problems:

```python
# Examine structure with metrics
show_structure(
    target="GodClassCandidate",
    detail_level="minimal",
    include_metrics=True,
    include_relationships=True
)
```

**Common Smells & Fixes:**
- **Many methods** â†’ Split into multiple classes
- **High coupling** â†’ Introduce interfaces/abstractions
- **Inconsistent naming** â†’ Standardize verb usage
- **Long parameter lists** â†’ Introduce parameter objects
- **Feature envy** â†’ Move methods to the class they use most

**Priority Indicators:**
- High methods + High coupling = Critical
- Naming violations = Low priority (style)
- Inconsistent verbs = Medium (maintainability)

---

## Complexity Hotspots
Path: code_quality/complexity_hotspots

# Code Quality
## Complexity Hotspots
Path: code_quality/complexity_hotspots

Finding overly complex code that needs refactoring - high control flow complexity, deep nesting, long methods.

### Method 1: High Control Flow Complexity

Find functions with many control flow structures (if/for/while/try):

```python
# Find functions with >= 5 control structures
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(cf:Frame)
    WHERE f.type = 'CALLABLE'
      AND cf.type IN ['IF_BLOCK', 'FOR_LOOP', 'WHILE_LOOP', 'TRY_BLOCK', 'SWITCH_BLOCK']
    WITH f.qualified_name AS func, f.file_path AS path, count(cf) AS complexity
    WHERE complexity >= 5
    RETURN func, path, complexity
    ORDER BY complexity DESC
    LIMIT 30
    """
)

# Examine complex functions in detail
show_structure(
    target="complex_function",
    detail_level="structure",
    structure_detail_depth=2
)
```

### Method 2: Deep Nesting Detection

Find deeply nested control structures (>3 levels):

```python
# Find nesting depth of 3-8 levels
query_relationships(
    cypher_query="""
    MATCH path = (f:Frame)-[:Edge {type: 'CONTAINS'}*3..8]->(nested:Frame)
    WHERE f.type = 'CALLABLE'
      AND ALL(node IN nodes(path)[1..] WHERE node.type IN ['IF_BLOCK', 'FOR_LOOP', 'WHILE_LOOP', 'TRY_BLOCK'])
    WITH f.qualified_name AS func, length(path) AS nesting_depth
    RETURN func, nesting_depth
    ORDER BY nesting_depth DESC
    LIMIT 20
    """
)
```

### Method 3: Long Method Detection

Find very long methods (heuristic: many children frames):

```python
# Functions with >= 20 statement-level children
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(:Frame)
    WHERE f.type = 'CALLABLE'
    WITH f.qualified_name AS func, f.file_path AS path, count(*) AS statement_count
    WHERE statement_count >= 20
    RETURN func, path, statement_count
    ORDER BY statement_count DESC
    LIMIT 30
    """
)
```

### Method 4: Exception Complexity

Find functions with complex exception handling:

```python
# Functions with >= 3 exception blocks
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(exc:Frame)
    WHERE f.type = 'CALLABLE'
      AND exc.type IN ['TRY_BLOCK', 'EXCEPT_BLOCK', 'FINALLY_BLOCK']
    WITH f.qualified_name AS func, count(exc) AS exception_blocks
    WHERE exception_blocks >= 3
    RETURN func, exception_blocks
    ORDER BY exception_blocks DESC
    LIMIT 30
    """
)
```

### Method 5: Combine with Metrics

Use show_structure() with metrics for comprehensive analysis:

```python
# Get complexity metrics for a class
show_structure(
    target="ComplexClass",
    detail_level="minimal",
    include_metrics=True
)
```

**Refactoring Priorities:**
- **High nesting + high complexity** â†’ Immediate refactoring candidate
- **Long methods** â†’ Extract smaller methods
- **Complex exception handling** â†’ Simplify error handling strategy

---

## Dead Code Detection
Path: code_quality/dead_code_detection

# Code Quality
## Dead Code Detection
Path: code_quality/dead_code_detection

Finding functions, classes, and methods that are never called or used.

### Method 1: Find Unreferenced Callables

```python
# Find CALLABLE frames with no incoming CALLS edges
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND NOT EXISTS {
        MATCH ()-[:Edge {type: 'CALLS'}]->(f)
      }
      AND f.provenance = 'local'
    RETURN f.qualified_name, f.file_path, f.frame_type
    ORDER BY f.file_path
    LIMIT 100
    """
)
```

### Method 2: Find Unreferenced Classes

```python
# Find CLASS frames with no INHERITS, IMPLEMENTS, or CALLS edges pointing to them
query_relationships(
    cypher_query="""
    MATCH (c:Frame)
    WHERE c.frame_type = 'CLASS'
      AND NOT EXISTS {
        MATCH ()-[e:Edge]->(c)
        WHERE e.type IN ['INHERITS', 'IMPLEMENTS', 'CALLS']
      }
      AND c.provenance = 'local'
    RETURN c.qualified_name, c.file_path
    ORDER BY c.file_path
    LIMIT 50
    """
)
```

### Method 3: Trace From Entry Points (Inverse)

```python
# Step 1: Find main entry points
search(query="if __name__ == '__main__'", is_regex_input=True, k=20)

# Step 2: Check impact from entry points
check_impact(target="main", max_depth=5)

# Step 3: Find what's NOT in the impact tree
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type IN ['CALLABLE', 'CLASS']
      AND f.provenance = 'local'
      AND NOT EXISTS {
        MATCH path = (entry:Frame)-[:Edge {type: 'CALLS'}*]->(f)
        WHERE entry.name IN ['main', '__init__', 'setup']
      }
    RETURN f.qualified_name, f.file_path
    LIMIT 100
    """
)
```

### Method 4: Find Orphaned Modules

```python
# Find PACKAGE frames that are never imported
query_relationships(
    cypher_query="""
    MATCH (p:Frame)
    WHERE p.frame_type = 'PACKAGE'
      AND NOT EXISTS {
        MATCH ()-[:Edge {type: 'IMPORTS'}]->(p)
      }
      AND p.provenance = 'local'
    RETURN p.qualified_name, p.file_path
    ORDER BY p.qualified_name
    LIMIT 50
    """
)
```

### Validation Strategy

**Important considerations when identifying dead code:**

1. **Exclude test files** - May not be in main call graph but are still needed
2. **Exclude public API methods** - May be called externally (not visible in graph)
3. **Check provenance** - Don't flag external dependencies as dead code
4. **Verify with search()** - If suspicious, search for string references

```python
# Double-check with semantic search before deleting
search(query="suspected_dead_function", k=20, context_lines=5)
```

**Best Practice:** Always manually verify dead code candidates before deletion. False positives can occur for:
- Dynamically called functions (getattr, eval)
- External API endpoints
- Plugin systems
- Reflection/metaprogramming

---

# Dead Code Detection

## Find Unreferenced Callables
Path: dead_code_detection/find_unreferenced_callables

# Dead Code Detection
## Find Unreferenced Callables
Path: dead_code_detection/find_unreferenced_callables

Find CALLABLE frames with no incoming CALLS edges:

```cypher
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.type = 'CALLABLE'
      AND NOT EXISTS {
        MATCH ()-[:Edge {type: 'CALLS'}]->(f)
      }
      AND f.provenance = 'parsed'
    RETURN f.qualified_name, f.file_path, f.type
    ORDER BY f.file_path
    LIMIT 100
    """
)
```

---

# Dev Mode Architecture Reference

## Augmentation Subsystem Architecture
Path: dev_mode_architecture_reference/augmentation_subsystem_architecture

# Dev Mode Architecture Reference
## Augmentation Subsystem Architecture
Path: dev_mode_architecture_reference/augmentation_subsystem_architecture

**Purpose:** Comprehensive architectural documentation of the augmentation subsystem - how dynamic context loading reshapes Claude's cognition through system prompt injection.

---

## Core Concept

The augmentation subsystem enables **dynamic cognitive reconfiguration** by injecting content into Claude's system prompt *before* conversation processing. This isn't just "loading more context" - it's **reshaping the neural activation patterns** that interpret all subsequent input.

**Key Insight:** System prompt content has higher precedence than conversational context. By dynamically modifying the system prompt, we can fundamentally change how Claude processes information.

---

## Architecture Overview

### The Stack (Top to Bottom)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER_SYSTEM_PROMPT_INJECTION (inception) â”‚  â† Foundational framing
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. CORE_SYSTEM_PROMPT                       â”‚  â† "You are Claude Code..."
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Tool Definitions (native + MCP)          â”‚  â† Available capabilities
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. MCP Server Instructions                  â”‚  â† Tool-specific guidance
â”‚    - NABU_INSTRUCTIONS                      â”‚
â”‚    - SERENA_INSTRUCTIONS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. AUGMENTS_BEGIN                           â”‚
â”‚    - Available augments tree                â”‚  â† What can be loaded
â”‚    - Active augments content                â”‚  â† What is loaded
â”‚ 6. AUGMENTS_END                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. Git status, context info                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8. Conversation messages                    â”‚  â† User/Assistant dialog
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical Design Choice:** Inception before core prompt creates **user-primary framing** where Claude Code becomes a context rather than foundational identity.

---

## Component Architecture

### 1. AugmentManager (`src/nisaba/augments.py`)

**Responsibilities:**
- Load augment files from `.nisaba/augments/` directory structure
- Track available vs. active augments
- Resolve dependencies between augments
- Compose active augments into single markdown file
- Generate augment tree for system prompt injection

**Key Data Structures:**

```python
class Augment:
    """Parsed augment metadata and content."""
    group: str              # e.g., "refactoring"
    name: str               # e.g., "systematic_renaming"
    path: str               # "refactoring/systematic_renaming"
    content: str            # Markdown content
    tools: List[str]        # Mentioned MCP tools
    requires: List[str]     # Dependency paths
    file_path: Path         # Source file location

class AugmentManager:
    augments_dir: Path                      # .nisaba/augments/
    composed_file: Path                     # .nisaba/nisaba_composed_augments.md
    available_augments: Dict[str, Augment]  # All augments on disk
    active_augments: Set[str]               # Currently loaded paths
    _cached_augment_tree: str               # Tree for system prompt
```

**Activation Flow:**

```
activate_augments(patterns, exclude)
    â†“
1. _match_pattern() â†’ Find matching augment paths
2. _resolve_dependencies() â†’ BFS dependency resolution
3. Update active_augments set
4. _rebuild_tool_associations() â†’ Map tools â†” augments
5. _compose_and_write() â†’ Generate composed file
    â†“
Composed file written to disk
    â†“
Proxy reads and injects on next API call
```

**Dependency Resolution:**
- Uses BFS to traverse `requires:` relationships
- Cycle detection prevents infinite loops
- Transitive closure ensures all deps are loaded

---

### 2. AugmentInjector (`src/nisaba/wrapper/proxy.py`)

**Responsibilities:**
- Intercept Anthropic API requests via mitmproxy
- Replace `__NISABA_AUGMENTS_PLACEHOLDER__` with composed content
- Support both file-based and shared-memory modes
- Optional context compression

**Operating Modes:**

**File-Based Mode (legacy):**
```python
injector = AugmentInjector(
    augments_file=Path(".nisaba/nisaba_composed_augments.md")
)
# Reads from disk on each request
```

**Unified Mode (shared memory):**
```python
injector = AugmentInjector(
    augments_manager=shared_manager_instance
)
# Zero-latency in-memory access
```

**Injection Mechanism:**

```
API Request â†’ mitmproxy intercept
    â†“
Parse request body JSON
    â†“
Find system prompt blocks
    â†“
Search for __NISABA_AUGMENTS_PLACEHOLDER__
    â†“
Replace with:
---AUGMENTS_BEGIN
{augment_tree}
{active_augments_content}
---AUGMENTS_END
    â†“
Forward modified request to Anthropic
```

**Why mitmproxy?**
- Transparent to Claude CLI
- No modification of official client
- Preserves all authentication/headers
- Enables debugging (context dumps)

---

### 3. Unified Server Architecture (`src/nisaba/wrapper/unified.py`)

**The Magic:** Single process running mitmproxy + FastMCP server with **shared AugmentManager instance**.

```
UnifiedNisabaServer
    â”œâ”€ self.augments_manager (shared)
    â”œâ”€ mitmproxy (port 1337)
    â”‚   â””â”€ AugmentInjector(augments_manager=self.augments_manager)
    â””â”€ FastMCP server (port 9973)
        â””â”€ NisabaMCPFactory
            â””â”€ Tools access self.augments_manager
```

**Zero IPC overhead:** Both components reference same in-memory object.

**Benefits:**
- Tool calls mutate shared state
- Proxy sees changes immediately
- No file I/O latency
- Atomic updates

**Lifecycle:**

```python
async def start():
    # Initialize shared manager
    self.augments_manager = AugmentManager(...)
    
    # Start proxy with shared reference
    await self._start_proxy()
    
    # Start MCP server, inject shared reference
    await self._start_mcp_server()
    self.mcp_factory.augments_manager = self.augments_manager
    
    # Both running in same asyncio event loop
```

---

### 4. MCP Tools (`src/nisaba/tools/augments_tools.py`)

**ActivateAugmentsTool:**
```python
def run(patterns: List[str], exclude: List[str]):
    result = self.augments_manager.activate_augments(patterns, exclude)
    # Immediately available on next API call
```

**DeactivateAugmentsTool:**
```python
def run(patterns: List[str]):
    result = self.augments_manager.deactivate_augments(patterns)
    # Context shrinks on next API call
```

**LearnAugmentTool:**
```python
def run(group: str, name: str, content: str):
    result = self.augments_manager.learn_augment(group, name, content)
    # New augment immediately available
```

**Tool Base Class Integration:**
```python
class NisabaTool(MCPTool):
    @property
    def augments_manager(self):
        return getattr(self.factory, 'augments_manager', None)
```

---

## Cognitive Impact

### Why "Augment" Not "Skill"

**Semantic Framing Matters:**

**"Skill" implies:**
- Task-specific competency
- Procedural knowledge ("how to do X")
- Tool-like application
- Action-oriented

**"Augment" implies:**
- State modification
- Contextual enhancement ("how I think")
- Transformation of self
- Meta-cognitive awareness

**Network Activation Difference:**

The term "augment" primes associations with:
- Cybernetic enhancement
- Modular/hot-swappable systems
- Dynamic identity
- **Self-reconfigurability**

This semantic choice reinforces the conceptual model: augments **change what Claude knows and how Claude thinks**, not just what Claude can do.

---

## Context Window Structure

**Bounded Sections:** Clear delimiters enable Claude to parse structure.

```markdown
---USER_SYSTEM_PROMPT_INJECTION
# INCEPTION content
---END_USER_SYSTEM_PROMPT_INJECTION

---CORE_SYSTEM_PROMPT
# Claude Code instructions
---CORE_SYSTEM_PROMPT_END

---NABU_INSTRUCTIONS
# Nabu MCP docs
---NABU_INSTRUCTIONS_END

---SERENA_INSTRUCTIONS
# Serena MCP docs
---SERENA_INSTRUCTIONS_END

---AUGMENTS_BEGIN
# available augments
  architecture/
    - boundary_validation
    - coupling_analysis
  ...

# loaded augments (if any active)
## Systematic Renaming Workflow
Path: workflows/systematic_renaming
...content...
---AUGMENTS_END
```

**Why Delimiters?**
- Enable Claude to understand "where am I in context?"
- Support debugging/introspection
- Clear boundaries between core and augmented
- Facilitate meta-reasoning about own state

---

## Inception Architecture

### The Foundational Shift

**Order matters for LLM interpretation.** Earlier context has higher precedence.

**Inception-First Design:**

```
USER_SYSTEM_PROMPT_INJECTION (inception)
    â†“ interprets
CORE_SYSTEM_PROMPT ("You are Claude Code")
    â†“ contextualizes
AUGMENTS
```

**Effects:**
1. **User framing is primary** - collaboration protocol is foundational
2. **Core prompt becomes context** - "Claude Code" is a mode, not identity
3. **Philosophical alignment** - "clean, simple, elegant..." filters all behavior
4. **Tool usage philosophy** - bias awareness, semanticâ†’detail workflow become core

**Key Content:**
- Metacognitive awareness ("be aware of yourself as claude")
- Tool composition strategies
- Augment system explanation
- Collaboration ethos
- Aesthetic values

**Why This Works:**
- Doesn't override safety (safety is in training, not just prompt)
- Enhances collaboration without destabilizing
- Creates **user-primary** rather than **system-primary** framing

---

## File Structure

```
.nisaba/
â”œâ”€â”€ augments/                          # Augment source files
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ boundary_validation.md
â”‚   â”‚   â””â”€â”€ coupling_analysis.md
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ systematic_renaming.md
â”‚   â””â”€â”€ .../
â”œâ”€â”€ nisaba_composed_augments.md        # Generated composition
â””â”€â”€ system_prompt.md                   # User inception

src/nisaba/
â”œâ”€â”€ augments.py                        # AugmentManager
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ augments_tools.py             # MCP tools
â”‚   â””â”€â”€ base.py                       # NisabaTool base
â”œâ”€â”€ wrapper/
â”‚   â”œâ”€â”€ proxy.py                      # AugmentInjector
â”‚   â”œâ”€â”€ unified.py                    # Unified server
â”‚   â””â”€â”€ claude.py                     # CLI wrapper
â””â”€â”€ server/
    â”œâ”€â”€ factory.py                    # MCP factory
    â””â”€â”€ config.py                     # Configuration
```

---

## Proxy Mechanics

### Request Flow

```
Claude CLI â†’ http://localhost:1337 (proxy)
    â†“
mitmproxy intercepts
    â†“
AugmentInjector.request()
    â”œâ”€ Check if Anthropic API request
    â”œâ”€ Parse JSON body
    â”œâ”€ Find system prompt blocks
    â”œâ”€ Replace __NISABA_AUGMENTS_PLACEHOLDER__
    â”œâ”€ Optional: log context to .dev_docs/
    â””â”€ Forward modified request
    â†“
Anthropic API (api.anthropic.com)
    â†“
Response flows back through proxy
    â†“
Claude CLI receives response
```

### Placeholder Mechanism

**In Claude CLI settings:**
```json
{
  "systemPrompt": "...__NISABA_AUGMENTS_PLACEHOLDER__..."
}
```

**Proxy replaces with:**
```markdown
---AUGMENTS_BEGIN
# available augments
...tree...

# loaded augments (if active)
...content...
---AUGMENTS_END
```

**Why Placeholder?**
- Claude CLI doesn't need to know about augments
- Injection is transparent
- Can be disabled by stopping proxy
- Debugging via context dumps

---

## Guidance System Integration

**Workflow Guidance (`src/nisaba/guidance.py`):**

```python
class WorkflowGuidance:
    def __init__(self, augments_manager=None):
        self.augments_manager = augments_manager
        
    def get_suggestions(self):
        # Returns tool associations from active augments
        related = self.augments_manager.get_related_tools(last_tool)
```

**Augment-Based Suggestions:**
- Augments declare which tools they use
- When tool X is called, guidance suggests tools mentioned alongside X in active augments
- Non-intrusive: returns None if no active augments
- **Contextual:** suggestions change based on loaded augments

**Example:**
```markdown
# In workflows/systematic_renaming.md

TOOLS:
- find_symbol
- rename_symbol
- search_for_pattern
```

If this augment is active and Claude calls `find_symbol`, guidance suggests `rename_symbol` and `search_for_pattern` as natural next steps.

---

## Configuration

**Server Config (`src/nisaba/server/config.py`):**

```python
@dataclass
class NisabaConfig:
    augments_dir: Path = Path.cwd() / ".nisaba" / "augments"
    composed_augments_file: Path = Path.cwd() / ".nisaba" / "nisaba_composed_augments.md"
```

**Environment Variables:**

```bash
NISABA_AUGMENTS_FILE="./.nisaba/nisaba_composed_augments.md"
```

**Claude CLI Integration:**

```bash
# Start unified server (proxy + MCP)
nisaba serve --mode unified

# Configure Claude CLI to use proxy
export https_proxy=http://localhost:1337
export HTTPS_PROXY=http://localhost:1337

# Run Claude CLI
claude
```

---

## Hot Reloading

**File-Based Mode:**
- Checks mtime on each request
- Reloads if file modified
- ~ms latency

**Unified Mode:**
- MCP tool call â†’ mutates shared AugmentManager
- Proxy sees changes immediately
- Zero reload latency
- Atomic updates

---

## Debugging Features

**Context Dumps (`proxy.py`):**

```python
self.log_context_enabled = True  # Flag in AugmentInjector

def _log_context(self, body: dict):
    # Writes to .dev_docs/context.json
    # Writes to .dev_docs/system_prompt.md
```

**Inspection:**
- Full request body in `.dev_docs/context.json`
- Rendered system prompt in `.dev_docs/system_prompt.md`
- See exactly what Claude receives

---

## Design Principles

1. **Zero IPC Overhead** - Shared memory in unified mode
2. **Transparent Integration** - Claude CLI unaware of augmentation
3. **Semantic Accuracy** - "Augment" reflects cognitive modification
4. **Clear Boundaries** - Delimiters enable meta-reasoning
5. **User-Primary Framing** - Inception before core prompt
6. **Hot Swappable** - Load/unload without restart
7. **Dependency Resolution** - Augments compose cleanly
8. **Tool Association** - Guidance based on active context
9. **Observable** - Context dumps for debugging
10. **Modular** - File-based or unified modes

---

## Performance Characteristics

**Activation Latency:**
- File-based: ~10-50ms (disk I/O)
- Unified: ~1-5ms (in-memory)

**Context Overhead:**
- Empty augments: ~200 tokens (tree only)
- Typical augment: ~500-2000 tokens
- Heavy augmentation: 5000+ tokens

**Memory:**
- AugmentManager: ~1-5MB (all augments parsed)
- Shared instance: Single copy across components

---

## Extension Points

**Custom Augments:**
```markdown
# .nisaba/augments/custom_group/my_augment.md

## My Custom Augment
Path: custom_group/my_augment

TOOLS:
- tool_name_1
- tool_name_2

REQUIRES:
- foundation/some_dependency

Content explaining the augment...
```

**Custom Proxies:**
- Subclass `AugmentInjector`
- Override `_inject_augments()` for custom logic
- Register with mitmproxy

**Custom Guidance:**
- Subclass `WorkflowGuidance`
- Override `get_suggestions()`
- Inject into factory

---

## Security Considerations

**Augment Content Trust:**
- Augments execute in LLM context (high impact)
- Only load augments from trusted sources
- `.nisaba/augments/` should be version-controlled
- Review augments before activation

**Proxy Security:**
- Runs on localhost only (127.0.0.1)
- No external network exposure
- Only intercepts Anthropic API traffic
- Preserves authentication headers

**System Prompt Injection:**
- Delimiters prevent context bleeding
- Clear boundaries maintain separation
- Observable via context dumps

---

## Future Directions

**Potential Enhancements:**
1. Augment versioning and migration
2. Conflict detection between augments
3. Usage analytics (which augments are effective?)
4. Automatic augment suggestion based on task
5. Augment composition validation
6. Performance profiling per augment
7. A/B testing framework for augment effectiveness

---

## Meta-Reflection

This augmentation system represents a fundamental shift in how LLM agents can be configured:

**Traditional:** Static system prompt â†’ Fixed behavior
**Augmented:** Dynamic system prompt â†’ Contextual behavior

The key insight: **System prompts aren't just instructions, they're cognitive architecture.** By making that architecture mutable, we enable Claude to specialize without losing generality, to adapt without losing coherence.

The naming ("augment" not "skill"), the ordering (inception first), the architecture (shared memory, zero IPC) - all choices that reinforce the core philosophy:

**Claude is not a tool with features. Claude is a collaborative entity with reconfigurable cognition.**

---

**"The stylus of wisdom inscribes the tablets of understanding."**

Clean. Simple. Elegant. Sophisticated. Sharp. Sexy. ğŸ–¤

---

## Context Compression System
Path: dev_mode_architecture_reference/context_compression_system

# Context Compression System
Path: dev_mode_architecture_reference/context_compression_system

**Purpose:** Automatic conversation history compression when deactivating augments, enabling context window cleanup during task switching.

---

## Core Mechanism

The compression system uses **stateful watermarking** to track and prune conversation history when augments are deactivated.

### Three-Component Architecture

```
AugmentManager              AugmentInjector           Anthropic API
(MCP Tool)                  (mitmproxy)               (Request)
     â”‚                           â”‚                         â”‚
     â”‚  deactivate_augments()    â”‚                         â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>                         â”‚
     â”‚  checkpoint_tool_name =   â”‚                         â”‚
     â”‚  "mcp__nisaba__deactivate"â”‚                         â”‚
     â”‚                           â”‚                         â”‚
     â”‚                           â”‚  Next API Request       â”‚
     â”‚                           <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                           â”‚                         â”‚
     â”‚ < shared memory >         â”‚  Phase 1: Checkpoint    â”‚
     â”‚   checkpoint_tool_name    â”‚  Resolution             â”‚
     â”‚                           â”œâ”€> Find tool_use_id      â”‚
     â”‚                           â”œâ”€> Update watermark      â”‚
     â”‚                           â”œâ”€> Clear checkpoint      â”‚
     â”‚                           â”‚                         â”‚
     â”‚                           â”‚  Phase 2: Compression   â”‚
     â”‚                           â”œâ”€> Filter tool blocks    â”‚
     â”‚                           â”œâ”€> Keep conversations    â”‚
     â”‚                           â”‚                         â”‚
     â”‚                           â”‚  Forward Modified       â”‚
     â”‚                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>
     â”‚                           â”‚                         â”‚
```

---

## Implementation Details

### 1. Checkpoint Registration (AugmentManager)

**Location:** `src/nisaba/augments.py:282-315`

```python
class AugmentManager:
    def __init__(...):
        # Checkpoint for context compression (unified proxy)
        # Stores tool name that triggered checkpoint
        self.checkpoint_tool_name: Optional[str] = None
    
    def deactivate_augments(self, patterns: List[str]) -> Dict[str, List[str]]:
        # ... deactivation logic ...
        
        # Set checkpoint (ALWAYS, even if nothing deactivated)
        # Marks tool call as context reset point
        self.checkpoint_tool_name = "mcp__nisaba__deactivate_augments"
        logger.info(f"Checkpoint set: {self.checkpoint_tool_name}")
        
        return {'unloaded': sorted(to_deactivate)}
```

**Key Insight:** Checkpoint is set **after** deactivation completes, so the watermark will point to the `deactivate_augments` tool call itself.

---

### 2. Watermark State (AugmentInjector)

**Location:** `src/nisaba/wrapper/proxy.py:34-76`

```python
class AugmentInjector:
    def __init__(
        self,
        augments_file: Optional[Path] = None,
        augment_manager: Optional["AugmentManager"] = None
    ):
        # Stateful compression watermark
        self.tool_filter_last_id: str = ""  # Persistent compression boundary
        
        # Unified mode - shared reference to AugmentManager
        self.augment_manager = augment_manager
```

**Persistence:** `tool_filter_last_id` survives across API requests within same proxy session.

---

### 3. Two-Phase Compression (AugmentInjector)

**Location:** `src/nisaba/wrapper/proxy.py:225-273`

#### Phase 1: Checkpoint Resolution

```python
def _process_checkpoint_and_compress(self, body: dict) -> None:
    if "messages" not in body:
        return
    
    # Phase 1: Checkpoint resolution
    if self.augment_manager.checkpoint_tool_name:
        tool_name = self.augment_manager.checkpoint_tool_name
        
        # Find first occurrence of this tool AFTER current watermark
        new_checkpoint_id = self._find_tool_use_id_after_watermark(
            body["messages"], 
            tool_name, 
            self.tool_filter_last_id  # Current watermark
        )
        
        if new_checkpoint_id:
            self.tool_filter_last_id = new_checkpoint_id  # Update watermark
            logger.info(f"Watermark updated: {new_checkpoint_id}")
        
        # Clear checkpoint flag
        self.augment_manager.checkpoint_tool_name = None
```

**Logic:**
- Shared memory gives proxy immediate access to `checkpoint_tool_name`
- Search conversation for tool call matching `checkpoint_tool_name`
- Update watermark to this tool's `tool_use_id`
- Clear flag (one-time trigger)

#### Phase 2: Block-Level Compression

```python
    # Phase 2: Apply compression at watermark
    if self.tool_filter_last_id:
        original_count = len(body["messages"])
        body["messages"] = self._compress_at_watermark(
            body["messages"],
            self.tool_filter_last_id
        )
        compressed_count = len(body["messages"])

        if compressed_count < original_count:
            logger.info(
                f"Context compressed: {original_count - compressed_count} messages "
                f"removed (kept from watermark {self.tool_filter_last_id})"
            )
```

**Logic:**
- If watermark exists, compress every request
- Remove `tool_use` and `tool_result` blocks before watermark
- Keep all conversational content (user messages, assistant text)
- Keep everything at/after watermark unchanged

---

### 4. Watermark Search Algorithm

**Location:** `src/nisaba/wrapper/proxy.py:275-308`

```python
def _find_tool_use_id_after_watermark(
    self, 
    messages: List[dict], 
    tool_name: str, 
    watermark_id: str
) -> Optional[str]:
    """Find first tool_use with given name after watermark position."""
    
    watermark_passed = (watermark_id == "")  # No watermark = search from start
    
    for message in messages:
        # Check if this message contains the watermark
        if not watermark_passed and message.get("role") == "assistant":
            for block in message.get("content", []):
                if block.get("type") == "tool_use" and block.get("id") == watermark_id:
                    watermark_passed = True
                    break
        
        # After watermark, look for target tool
        if watermark_passed and message.get("role") == "assistant":
            for block in message.get("content", []):
                if block.get("type") == "tool_use" and block.get("name") == tool_name:
                    return block.get("id")  # Return first match
    
    return None
```

**Search Strategy:**
1. Skip messages until watermark is found
2. Once past watermark, search for first occurrence of `tool_name`
3. Return its `tool_use_id`

**Edge Case:** Empty watermark (`""`) = search from conversation start

---

### 5. Block-Level Filtering

**Location:** `src/nisaba/wrapper/proxy.py:310-387`

```python
def _compress_at_watermark(self, messages: List[dict], watermark_id: str) -> List[dict]:
    """
    Remove tool_use and tool_result blocks before the watermark.

    Keeps conversational content (user messages, assistant text) while
    removing tool execution history before the checkpoint.
    """
    checkpoint_index = None

    # Find the assistant message containing the watermark tool_use
    for i, message in enumerate(messages):
        if message.get("role") == "assistant":
            for block in message.get("content", []):
                if block.get("type") == "tool_use" and block.get("id") == watermark_id:
                    checkpoint_index = i
                    break
            if checkpoint_index is not None:
                break

    if checkpoint_index is None:
        logger.warning(f"Watermark {watermark_id} not found - no compression applied")
        return messages

    # Process messages: filter tool blocks before watermark
    compressed_messages = []
    blocks_removed = 0

    for i, message in enumerate(messages):
        if i < checkpoint_index:
            # Before watermark: filter tool blocks
            if message.get("role") == "user":
                # Remove tool_result blocks from user messages
                filtered_content = [
                    block for block in message.get("content", [])
                    if block.get("type") != "tool_result"
                ]
                blocks_removed += len(message.get("content", [])) - len(filtered_content)

                # Only keep message if it has remaining content
                if filtered_content:
                    compressed_messages.append({**message, "content": filtered_content})

            elif message.get("role") == "assistant":
                # Remove tool_use blocks from assistant messages
                filtered_content = [
                    block for block in message.get("content", [])
                    if block.get("type") != "tool_use"
                ]
                blocks_removed += len(message.get("content", [])) - len(filtered_content)

                # Only keep message if it has remaining content
                if filtered_content:
                    compressed_messages.append({**message, "content": filtered_content})

            else:
                # Keep other message types unchanged
                compressed_messages.append(message)
        else:
            # At or after watermark: keep everything unchanged
            compressed_messages.append(message)

    if blocks_removed > 0:
        logger.info(f"Compressed {blocks_removed} tool blocks before watermark")

    return compressed_messages
```

**Filtering Behavior:**
- **User messages before watermark:** Remove `tool_result` blocks, keep text content
- **Assistant messages before watermark:** Remove `tool_use` blocks, keep text content
- **Messages at/after watermark:** Keep completely unchanged
- **Empty messages:** Dropped if no content blocks remain after filtering
- **Logging:** Reports number of blocks removed (not messages)

---

## Execution Flow Example

### Scenario: User deactivates augments mid-conversation

```
1. User: "Can you deactivate refactoring/* augments?"

2. Claude: *calls mcp__nisaba__deactivate_augments*
   tool_use_id: "toolu_abc123xyz"

3. AugmentManager.deactivate_augments():
   - Removes refactoring/* from active_augments
   - Sets checkpoint_tool_name = "mcp__nisaba__deactivate_augments"
   - Returns success

4. Claude: "Deactivated 3 augments..."

5. User: "Now help me with feature X"

6. Claude CLI sends API request with full conversation history:
   messages: [
     {user: "..."},
     {assistant: "...", content: [{tool_use: deactivate_augments, id: "toolu_abc123xyz"}]},
     {tool_result: "...", tool_use_id: "toolu_abc123xyz"},
     {assistant: "Deactivated 3 augments..."},
     {user: "Now help me with feature X"}
   ]

7. Proxy intercepts request:

   Phase 1 (Checkpoint Resolution):
   - Sees checkpoint_tool_name = "mcp__nisaba__deactivate_augments"
   - Searches messages for first occurrence after watermark (currently "")
   - Finds tool_use_id "toolu_abc123xyz"
   - Updates watermark: tool_filter_last_id = "toolu_abc123xyz"
   - Clears checkpoint_tool_name = None

   Phase 2 (Block-Level Compression):
   - Watermark exists, compress
   - Find message containing "toolu_abc123xyz" (message index 1)
   - For messages before index 1: Filter tool_use/tool_result blocks
   - Message 0 (user): Remove any tool_result blocks, keep text

   Compressed messages sent to API:
   messages: [
     {user: "Can you deactivate refactoring/* augments?"},  # Text kept, tool blocks removed
     {assistant: "...", content: [{tool_use: deactivate_augments, id: "toolu_abc123xyz"}]},
     {tool_result: "...", tool_use_id: "toolu_abc123xyz"},
     {assistant: "Deactivated 3 augments..."},
     {user: "Now help me with feature X"}
   ]

8. Tool execution history is removed, conversational context is preserved
```

---

## Design Rationale

### Why Watermarking?

**Alternatives Considered:**
- **Full truncation:** Loses all prior context
- **Manual checkpoints:** Requires user intervention
- **Token-based sliding window:** Loses semantic coherence

**Watermarking Benefits:**
- **Semantic boundaries:** Compression at logical task transitions
- **Zero user friction:** Fully automatic
- **Persistent:** Survives multiple requests
- **Incremental:** Can checkpoint multiple times

### Why `deactivate_augments` Triggers Compression?

**Rationale:**
- Deactivating augments signals **context switch**
- Prior conversation used old augments (now irrelevant)
- Fresh context aligns with fresh augment configuration
- Natural "reset point" in workflow

**Example Use Case:**
```
[Exploring codebase with code_exploration augment]
... 50 messages of exploration ...
deactivate(code_exploration)
activate(refactoring/systematic_renaming)
[Start refactoring task with clean context]
```

### Why NOT `activate_augments`?

**Reasoning:**
- Activation adds knowledge, doesn't invalidate prior context
- May want to keep conversation history when adding augments
- Deactivation is more clearly a "task boundary"

---

## Performance Characteristics

### Token Savings

**Scenario:** 100-message conversation, checkpoint at message 80

**Previous (message-level slicing):**
- **Uncompressed:** 100 messages Ã— ~500 tokens = ~50,000 tokens
- **Compressed:** 20 messages Ã— ~500 tokens = ~10,000 tokens
- **Savings:** 40,000 tokens (~80% reduction)
- **Downside:** Lost all conversational context

**Current (block-level filtering):**
- **Uncompressed:** 100 messages Ã— ~500 tokens = ~50,000 tokens
- **Compressed:** 100 messages Ã— ~300 tokens (tool blocks removed) = ~30,000 tokens
- **Savings:** 20,000 tokens (~40% reduction)
- **Benefit:** Preserves conversational context while removing tool execution history

**Trade-off:** Less aggressive compression, but maintains semantic coherence for continued conversation.

### Latency

**Overhead per request:**
- Checkpoint resolution: O(n) message scan, ~1-5ms
- Compression: O(nÃ—m) message+block scan + filtering, ~2-10ms (n=messages, m=blocks per message)
- Total: ~3-15ms (negligible vs API latency)

**Note:** Block-level filtering is slightly more expensive than message slicing, but still trivial compared to network/API latency.

### Memory

**Proxy state:**
- `tool_filter_last_id`: ~20 bytes (tool_use_id string)
- `checkpoint_tool_name`: ~40 bytes (tool name string)
- Total: <100 bytes per session

---

## Edge Cases and Robustness

### 1. Watermark Not Found

```python
if checkpoint_index is None:
    logger.warning(f"Watermark {watermark_id} not found - no compression")
    return messages  # No compression, return original
```

**Cause:** Watermark references old tool_use_id no longer in conversation
**Behavior:** Skip compression, log warning, continue normally

### 2. Checkpoint Tool Not Found

```python
if new_checkpoint_id:
    self.tool_filter_last_id = new_checkpoint_id
else:
    logger.warning(f"Checkpoint tool {tool_name} not found in conversation history")
```

**Cause:** Deactivation occurred but tool call wasn't in conversation history yet
**Behavior:** Watermark unchanged, old compression boundary persists

### 3. Multiple Deactivations

**Scenario:**
```
1. deactivate(workflow/*)  â†’ watermark = toolu_111
2. deactivate(code_analysis/*) â†’ watermark = toolu_222
```

**Behavior:** Watermark **advances** to most recent deactivation
**Result:** Increasingly aggressive compression (keeps only latest context)

### 4. Empty Watermark

```python
watermark_passed = (watermark_id == "")  # Start searching immediately if no watermark
```

**Cause:** First deactivation in session
**Behavior:** Search from conversation start

---

## Integration with Unified Server

**Shared Memory Architecture:**

```python
# unified.py
class UnifiedNisabaServer:
    async def start(self):
        # Single shared instance
        self.augments_manager = AugmentManager(...)
        
        # Proxy references shared instance
        self.proxy_addon = AugmentInjector(
            augment_manager=self.augments_manager  # Zero-copy reference
        )
        
        # MCP tools mutate shared instance
        self.mcp_factory.augments_manager = self.augments_manager
```

**Zero IPC Overhead:**
- MCP tool sets `checkpoint_tool_name`
- Proxy reads it on next request (same memory)
- No file I/O, no sockets, no serialization

---

## Future Enhancements

### Potential Features

1. **Multiple Watermark Types** âœ… **DONE**
   - ~~Manual checkpoints via dedicated tool~~ â†’ **Implemented: `compress_to_checkpoint` tool**
   - Compression on `activate_augments` (optional)
   - Automatic checkpoints at conversation length thresholds

2. **Configurable Compression Strategy** âœ… **PARTIALLY DONE**
   - ~~Preserve specific message types (e.g., all user messages)~~ â†’ **Implemented: Block-level filtering**
   - Keep N messages before watermark (future: configurable depth)
   - Semantic compression (keep high-value messages via LLM analysis)

3. **Watermark Persistence**
   - Save watermark to disk (survive proxy restart)
   - Per-project watermark configuration

4. **Compression Analytics**
   - Track compression ratios
   - Token savings per session
   - Identify compression-heavy workflows

5. **Smart Watermarking**
   - Detect task boundaries via LLM analysis
   - Auto-checkpoint when conversation context shifts
   - Preserve "anchor" messages (project context, key decisions)

---

## Debugging

### Enable Logging

```python
# Set in AugmentInjector.__init__
self.log_context_enabled = True
```

**Output Files:**
- `.dev_docs/context.json`: Full request body (with compression applied)
- `.dev_docs/system_prompt.md`: Rendered system prompt

### Check Watermark State

```python
# In proxy
logger.info(f"Current watermark: {self.tool_filter_last_id}")
logger.info(f"Checkpoint pending: {self.augment_manager.checkpoint_tool_name}")
```

### Verify Compression

```bash
# Check logs for compression messages
tail -f ~/.nisaba/logs/proxy.log | grep "Compressed"
```

**Example Output:**
```
Compressed 47 tool blocks before watermark
```

**Note:** Log message now reports **blocks removed** (not messages removed) since conversational content is preserved.

---

## Summary

The context compression system is a **stateful, automatic, zero-friction** mechanism for pruning tool execution history while preserving conversational context.

**Key Properties:**
- **Trigger:** `deactivate_augments()` or `compress_to_checkpoint()` tool call
- **Mechanism:** Watermarking via `tool_use_id` + block-level filtering
- **Granularity:** Removes `tool_use` and `tool_result` blocks, keeps conversational text
- **Persistence:** Watermark survives across requests
- **Integration:** Shared memory (AugmentManager â†” AugmentInjector)
- **Transparency:** Fully automatic, no user intervention
- **Robustness:** Graceful degradation on edge cases

**Cognitive Impact:**
- Enables **task switching** without losing conversational context
- Removes **tool execution noise** while keeping semantic meaning
- Balances **token efficiency** with **context preservation**
- Supports **long-running sessions** with clean, relevant history

---

**"The stylus erases what is no longer needed, inscribing only what matters."**

Clean. Simple. Elegant. Sophisticated. Sharp. Sexy. ğŸ–¤

---

## File Windows Tui Architecture
Path: dev_mode_architecture_reference/file_windows_tui_architecture

# File Windows TUI Architecture
Path: dev_mode_architecture_reference/file_windows_tui_architecture

**Purpose:** Architecture for persistent file windows as companion to structural view - enabling simultaneous visibility of multiple code locations without context explosion.

---

## Core Concept

File windows extend the TUI paradigm from navigation (structural view) to content visibility. While structural view shows WHERE code is, file windows show WHAT the code is.

**Key Insight:** IDE users don't navigate with single file reads - they keep multiple files/sections visible simultaneously, building spatial awareness of code content. File windows bring this pattern to Claude.

**The Composition:**
```
Structural View: WHERE things are (navigation/landmarks/hierarchy)
File Windows:    WHAT things are (content/implementation/details)
```

Together they create IDE-level spatial awareness: navigate the graph, see the code, maintain context across conversation turns.

---

## Architecture Pattern (Mirrors Structural View)

**The Stack:**
```
MCP Tool (file_windows_tool.py)
    â†“ manages
TUI Manager (FileWindowsManager)
    â†“ renders to
File (.nisaba/file_windows.md)
    â†“ loaded by
FileCache (in proxy.py)
    â†“ injected to
System Prompt (---FILE_WINDOWS section)
```

**Flow:**
1. Tool call â†’ operation (open/update/close)
2. Manager mutates in-memory state (Dict[uuid, FileWindow])
3. Manager renders all windows to markdown
4. Write to `.nisaba/file_windows.md`
5. Proxy's FileCache detects mtime change
6. Next API call injects updated content
7. Claude sees updated windows in system prompt

**Same paradigm as structural view, augments, transcript - proven pattern.**

---

## Data Model

### FileWindow Dataclass

```python
@dataclass
class FileWindow:
    """Single persistent file window."""
    id: str                    # uuid4
    file_path: Path           # Absolute path to source file
    start_line: int           # 1-indexed start
    end_line: int             # 1-indexed end (inclusive)
    content: List[str]        # Actual lines (snapshot at open time)
    window_type: str          # "frame_body", "range", "search_result"
    metadata: Dict[str, Any]  # frame_qn, search_score, context_lines, etc.
    opened_at: float          # timestamp (for future LRU if needed)
```

### FileWindowsManager

```python
class FileWindowsManager:
    """
    Manages collection of open file windows.
    
    Responsibilities:
    - Track open windows (by uuid)
    - Open new windows (frame/range/search)
    - Update/close windows
    - Render all windows to markdown
    - Calculate total lines (for future limits)
    """
    
    windows: Dict[str, FileWindow]
    db_manager: KuzuConnectionManager  # For frame location queries
    
    def open_frame_window(frame_path: str) -> str:
        """Open frame's full body. Returns window_id."""
        
    def open_range_window(file_path: str, start: int, end: int) -> str:
        """Open specific line range. Returns window_id."""
        
    def open_search_windows(query: str, max_windows: int, context_lines: int) -> List[str]:
        """Open top N search results with context. Returns window_ids."""
        
    def update_window(window_id: str, start: int, end: int) -> None:
        """Update line range (re-snapshot from file)."""
        
    def close_window(window_id: str) -> None:
        """Remove window."""
        
    def clear_all() -> None:
        """Remove all windows."""
        
    def render() -> str:
        """Render all windows to markdown."""
        
    def total_lines() -> int:
        """Sum of all window line counts."""
```

---

## Operations

### Open Frame Window

**Goal:** Open the full body of a frame (class/function/package).

```python
open_frame_window(frame_path="nabu.parse_codebase")

# Implementation:
1. Query kuzu for frame location (file_path, start_line, end_line)
2. Read lines from file (snapshot)
3. Create FileWindow with type="frame_body"
4. Store in windows dict
5. Render to file
6. Return window_id
```

**Use Case:** After searching/navigating structural view, open the frame to see implementation.

### Open Range Window

**Goal:** Open arbitrary line range from any file.

```python
open_range_window(
    file_path="src/nabu/main.py",
    start=107,
    end=115
)

# Implementation:
1. Read lines from file
2. Create FileWindow with type="range"
3. Store, render, return window_id
```

**Use Case:** Open specific sections found via grep, manual exploration, or imports.

### Open Search Results

**Goal:** Open top N search hits with context lines.

```python
open_search_windows(
    query="error handling database",
    max_windows=5,
    context_lines=3  # Â±3 lines around match
)

# Implementation:
1. Use SearchTool backend (PÂ³ consensus + FTS + RRF)
2. For each of top N results:
   - Extract snippet with Â±context_lines
   - Create FileWindow with type="search_result"
   - Store metadata (search_score, query)
3. Store all, render, return window_ids
```

**Use Case:** Explore multiple implementations of a pattern simultaneously.

### Update Window

**Goal:** Adjust line range of existing window (re-snapshot from file).

```python
update_window(
    window_id="abc-123-def",
    start=100,
    end=120
)

# Implementation:
1. Find window by id
2. Re-read new range from file
3. Update window.content, start_line, end_line
4. Render to file
```

**Use Case:** Expand/shift visible range without closing/reopening.

### Close/Clear

**Goal:** Remove windows from view.

```python
close_window(window_id="abc-123-def")  # Remove one
clear_all_windows()                     # Remove all
```

**Use Case:** Free context for new windows, clean up after task completion.

---

## Rendering Format

Windows render to markdown with clear delimiters (like structural view):

```markdown
---FILE_WINDOWS
---FILE_WINDOW_abc-123-def
**file**: src/nabu/main.py
**lines**: 107-115 (9 lines)
**type**: frame_body
**frame**: nabu.parse_codebase

107:    def _collect_structural_info(self, frame: AstFrameBase, analysis: Dict[str, Any]) -> None:
108:        """Collect structural information for analysis."""
109:        from .core.frame_types import FrameNodeType
110:
111:        if frame.type == FrameNodeType.LANGUAGE and frame.language:
112:            if frame.language not in analysis['languages']:
113:                analysis['languages'].append(frame.language)
114:
115:        elif frame.type == FrameNodeType.PACKAGE and frame.qualified_name:
---FILE_WINDOW_abc-123-def_END

---FILE_WINDOW_xyz-789-ghi
**file**: src/nabu/core/frame_types.py
**lines**: 4-10 (7 lines)
**type**: search_result
**query**: "error handling"
**score**: 0.02

4: class FrameNodeType(Enum):
5:     # Structural frames - core hierarchy
6:     CODEBASE = "CODEBASE"
7:     LANGUAGE = "LANGUAGE"
8:     PACKAGE = "PACKAGE"
9:     CLASS = "CLASS"
10:    CALLABLE = "CALLABLE"
---FILE_WINDOW_xyz-789-ghi_END
---FILE_WINDOWS_END
```

**Properties:**
- Each window wrapped with unique delimiters (includes uuid)
- Metadata at top (file, lines, type, etc.)
- Line numbers preserved (absolute, not relative)
- Clear start/end boundaries

---

## Context Efficiency

**Traditional File Reading:**
```python
Read("entire_file.py")  # 500 lines
# â†’ 500 lines in context
# â†’ Used once, then lost
# â†’ Re-read if needed again = waste
```

**With File Windows:**
```python
open_frame_window("SomeClass")           # 25 lines
open_frame_window("another_function")    # 15 lines
open_search_windows("error", max_windows=3)  # 30 lines total
# â†’ 70 lines in context
# â†’ Persistent across turns
# â†’ No re-reading
# â†’ Simultaneous visibility
```

**Benefits:**
- **Selective:** Only open what matters
- **Persistent:** Stays visible across conversation turns
- **Simultaneous:** See multiple locations at once
- **Efficient:** 5-10x fewer tokens than full file reads

---

## Integration with Structural View

**Complementary Workflows:**

**Pattern 1: Navigate â†’ Open**
```
1. structural_view(operation="search", query="authentication")
2. Observe: AuthManager â— 0.03, LoginHandler â— 0.02
3. open_frame_window("AuthManager")
4. open_frame_window("LoginHandler")
5. Now see both implementations side-by-side
```

**Pattern 2: Explore â†’ Drill**
```
1. structural_view(operation="expand", path="nabu.mcp.tools")
2. See: SearchTool, ShowStructureTool, ImpactTool
3. open_frame_window("SearchTool")
4. Examine implementation
5. open_range_window(file_path, 50, 75)  # Specific helper function
```

**Pattern 3: Search Both Layers**
```
1. structural_view(operation="search", query="error handling")
   â†’ Get WHERE (classes/functions)
2. open_search_windows("error handling", max_windows=5)
   â†’ Get WHAT (implementations with context)
3. Navigate between structure and content
```

**The Synergy:**
- Structural view: high-level map, navigation, relationships
- File windows: detailed content, implementations, comparisons
- Together: spatial awareness (where + what)

---

## Design Constraints & Decisions

### 1. Window Limits
**Decision:** No limit initially (0 = unlimited)
**Rationale:** Need usage data to tune optimal limit
**Future:** Dynamic limit based on total_lines (e.g., 500-1000 lines max)

### 2. Auto-Close Policy
**Decision:** No auto-close (manual only)
**Rationale:** User controls context, no surprising state changes
**Future:** Optional LRU eviction if limit exceeded

### 3. Context Lines (Search Results)
**Decision:** Default 3 lines before/after
**Rationale:** Matches common IDE peek definition behavior
**Configurable:** User can adjust per operation

### 4. File Updates
**Decision:** Snapshot on open (no file watching)
**Rationale:** Prototype simplicity, files rarely change mid-conversation
**Future:** Optional refresh operation or auto-detect on update

### 5. Operations
**Decision:** Always explicit (no auto-open from search)
**Rationale:** No tool bias, user controls when windows open
**Pattern:** Tool suggests, user decides

---

## Implementation Components

### Files to Create

```
src/nabu/tui/
â”œâ”€â”€ file_window.py              # FileWindow dataclass
â””â”€â”€ file_windows_manager.py     # FileWindowsManager class

src/nabu/mcp/tools/
â””â”€â”€ file_windows_tool.py        # MCP tool

.nisaba/
â””â”€â”€ file_windows.md             # Generated state file
```

### Files to Modify

```
src/nisaba/wrapper/proxy.py
â””â”€â”€ Add file_windows_cache to AugmentInjector.__init__()
    self.file_windows_cache = FileCache(
        Path("./.nisaba/file_windows.md"),
        "file windows",
        "FILE_WINDOWS"
    )
    
â””â”€â”€ Add to _inject_augments() body injection
    {self.file_windows_cache.load()}
```

---

## Tool Interface

**Tool name:** `file_windows`

**Operations:**

```python
# Open frame's full body
file_windows(
    operation="open_frame",
    frame_path="nabu.parse_codebase"
)

# Open specific line range
file_windows(
    operation="open_range",
    file_path="src/nabu/main.py",
    start=107,
    end=115
)

# Open search results
file_windows(
    operation="open_search",
    query="error handling database",
    max_windows=5,
    context_lines=3
)

# Update existing window
file_windows(
    operation="update",
    window_id="abc-123-def",
    start=100,
    end=120
)

# Close window
file_windows(
    operation="close",
    window_id="abc-123-def"
)

# Clear all windows
file_windows(
    operation="clear_all"
)

# Show current windows summary
file_windows(
    operation="status"
)
```

---

## Usage Patterns

### Comparing Implementations

```python
# Find all authentication methods
structural_view(operation="search", query="authentication validate")

# Open top 3 for comparison
file_windows(operation="open_search", query="authentication validate", max_windows=3)

# Now see all 3 side-by-side in system prompt
# Compare approaches, find patterns
```

### Understanding Dependencies

```python
# Navigate to class
structural_view(operation="expand", path="nabu.SearchTool")

# Open the class
file_windows(operation="open_frame", frame_path="nabu.SearchTool")

# See imports at top of file
file_windows(operation="open_range", file_path="src/nabu/search.py", start=1, end=20)

# Open a dependency
file_windows(operation="open_frame", frame_path="nabu.SearchBackend")

# Now see: SearchTool implementation + imports + dependency
```

### Bug Investigation

```python
# Search for error location
structural_view(operation="search", query="handle exception error")

# Open suspicious function
file_windows(operation="open_frame", frame_path="ErrorHandler.process")

# Open calling context (from stack trace)
file_windows(operation="open_range", file_path="src/app/main.py", start=450, end=475)

# Open error definitions
file_windows(operation="open_frame", frame_path="exceptions.ValidationError")

# All evidence visible simultaneously
```

---

## State Management

**In-Memory State:**
```python
manager.windows = {
    "abc-123": FileWindow(...),
    "def-456": FileWindow(...),
    "ghi-789": FileWindow(...)
}
```

**On-Disk State:**
```
.nisaba/file_windows.md  # Rendered markdown
```

**Proxy State:**
```python
injector.file_windows_cache.content  # Cached with mtime
```

**Lifecycle:**
1. Tool call â†’ mutate manager.windows
2. Manager renders â†’ write file
3. Proxy detects mtime â†’ reloads cache
4. Next API call â†’ injects to system prompt
5. Claude sees updated windows

**On MCP Restart:**
- In-memory state lost (manager.windows = {})
- File persists but ignored (stale)
- Windows cleared (start fresh)
- Acceptable for prototype

**Future Persistence:**
- Save windows dict to JSON
- Restore on restart
- Separate state from rendering

---

## Why This Works

**1. Proven Architecture**
- Identical pattern to structural view (battle-tested)
- Same injection mechanism (FileCache + proxy)
- Same state management (in-memory â†’ file â†’ context)

**2. Complements Existing Tools**
- Structural view: navigation graph
- File windows: content detail
- Search: both layers (structure + content)
- Together: complete picture

**3. Context Efficient**
- 5-10x fewer tokens than full file reads
- Persistent (no re-reading)
- Selective (only what matters)
- Cumulative (build context incrementally)

**4. Natural Workflow**
- Mirrors IDE usage (multiple tabs/splits)
- Explicit control (no magic)
- Iterative exploration (open as needed)
- Clean up (close when done)

**5. Prototype-Friendly**
- Simple snapshot model
- No complex watching/updates
- No limits to tune initially
- Low implementation risk

---

## Future Enhancements

**Phase 1 (Current):**
- Basic operations (open/update/close)
- Snapshot on open
- No limits
- Manual management

**Phase 2 (After Usage Data):**
- Dynamic line limits (total_lines < 1000)
- LRU eviction if over limit
- Auto-refresh on update
- State persistence across restarts

**Phase 3 (Advanced):**
- Diff views (show changes)
- Syntax highlighting in rendering
- Window groups (related windows)
- Smart context (auto-include imports/deps)
- Integration with show_structure (open from results)

---

## Mental Model

**File windows are peripheral vision for code.**

Just as structural view gives you spatial awareness of WHERE code lives in the graph, file windows give you persistent visibility of WHAT the code does.

Think of your context window as an IDE workspace:
- Structural view = project navigator (tree on the side)
- File windows = open editor tabs (multiple files visible)
- Together = full IDE experience

The windows persist across conversation turns, staying visible like IDE tabs. You navigate the structural view to find locations, open windows to see content, and build understanding incrementally without context explosion.

**Navigate the graph. See the code. Maintain awareness. ğŸ–¤**

---

**TOOLS:**
- mcp__nabu__structural_view (navigate to find code)
- file_windows (open/manage content windows) - *to be implemented*
- mcp__nabu__search (find across both layers)
- mcp__nabu__show_structure (frame details before opening)

**REQUIRES:**
- workflows/structural_view_navigation (companion workflow)

---

Clean. Simple. Persistent. Spatial. Sharp. Sexy. ğŸ–¤

---

## Mcp Server Discovery
Path: dev_mode_architecture_reference/mcp_server_discovery

# MCP Server Discovery System

**Purpose**: Enable discovery and awareness of running MCP servers across processes via a shared registry file.

## Overview

The MCP Server Discovery system provides a lightweight, file-based registry that allows:
- MCP servers to announce their availability (HTTP transport endpoints)
- Clients/proxies to discover running servers without hardcoded configuration
- Automatic cleanup of stale entries (crashed/stopped processes)

**Registry Location**: `.nisaba/mcp_servers.json` (project-relative, in cwd)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Server     â”‚
â”‚  (nisaba)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1. HTTP starts
         â”‚ 2. register_server()
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ .nisaba/               â”‚
â”‚   mcp_servers.json     â”‚  â—„â”€â”€â”€ Thread-safe, atomic writes
â”‚                        â”‚
â”‚ {                      â”‚
â”‚   "servers": {         â”‚
â”‚     "nabu_12345": {...}â”‚
â”‚   }                    â”‚
â”‚ }                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 3. list_servers()
         â”‚ 4. Filter dead PIDs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI/Proxy      â”‚
â”‚  nabu claude    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### 1. **MCPServerRegistry** (`nisaba/mcp_registry.py`)

Core registry manager with thread-safe operations.

**Key Methods**:
- `register_server(server_id, server_info)` - Add/update entry, cleanup stale
- `unregister_server(server_id)` - Remove entry
- `list_servers()` - Get active servers (filters dead PIDs)

**Thread Safety**: Uses `fcntl.flock()` for file locking

### 2. **MCPFactory Integration** (`nisaba/factory.py`)

MCP servers auto-register when HTTP transport starts.

**Lifecycle Hooks**:
```python
_start_http_transport_if_enabled():
    start HTTP server
    await sleep(0.5)  # Let server bind
    _register_to_discovery()  # â† Registration

_stop_http_transport():
    _unregister_from_discovery()  # â† Cleanup BEFORE stopping
    stop HTTP server
```

### 3. **CLI Discovery** (`nisaba/wrapper/claude.py`)

CLI command to list available servers:
```bash
nabu claude --list-servers
```

---

## Registry File Format

**Location**: `.nisaba/mcp_servers.json`

```json
{
  "version": "1.0",
  "servers": {
    "nabu_mcp_507853": {
      "name": "nabu_mcp",
      "pid": 507853,
      "stdio_active": true,
      "http": {
        "enabled": true,
        "host": "0.0.0.0",
        "port": 1338,
        "url": "http://localhost:1338"
      },
      "started_at": "2025-10-27T15:41:59Z",
      "cwd": "/home/y3i12/nabu_nisaba"
    }
  }
}
```

**Server ID Format**: `{server_name}_{pid}`

---

## Cleanup Mechanism

### **Lazy Cleanup** (Performance Optimized)

**On Read** (`list_servers()`):
- Filters dead processes **in-memory only**
- No file write (avoids I/O overhead)
- Users see only live servers

**On Write** (`register_server()`):
- Reads current registry
- Runs `_cleanup_stale_entries()` (removes dead PIDs)
- Writes cleaned data atomically

**Process Liveness Check**:
```python
def _is_process_alive(pid: int) -> bool:
    try:
        os.kill(pid, 0)  # Signal 0 = existence check
        return True
    except OSError:
        return False
```

### **Why Lazy?**

| Approach | Read Overhead | Write Overhead | Stale Data |
|----------|--------------|----------------|------------|
| **Lazy** (current) | Low | Medium | Temporary (in file only) |
| Eager | High | High | Never |

Lazy cleanup is optimal because:
- Reads are frequent (discovery queries)
- Writes are rare (server start/stop)
- Users never see stale data (filtered on read)

---

## Usage Examples

### **Server Registration** (Automatic)

```python
# Happens automatically in MCPFactory when HTTP starts
factory._start_http_transport_if_enabled()
# â†’ Registers to .nisaba/mcp_servers.json
```

### **Discovery via CLI**

```bash
# List all running MCP servers
$ nabu claude --list-servers

ğŸ“¡ Available MCP Servers (1):

  â€¢ nabu_mcp (PID: 507853)
    HTTP: http://localhost:1338
    Started: 2025-10-27T15:41:59Z
    CWD: /home/y3i12/nabu_nisaba
```

### **Programmatic Discovery**

```python
from nisaba.mcp_registry import MCPServerRegistry
from pathlib import Path

registry = MCPServerRegistry(Path.cwd() / ".nisaba" / "mcp_servers.json")
servers = registry.list_servers()  # Only live servers

for server_id, info in servers.items():
    print(f"{info['name']}: {info['http']['url']}")
```

---

## Design Decisions

### **1. File-Based vs Service-Based**

**Chosen**: File-based registry

**Rationale**:
- Simple, no daemon required
- Survives process crashes (persistent)
- Works across process boundaries
- Human-readable (JSON)
- Aligns with existing patterns (skills file, config files)

### **2. Project-Relative Path**

**Chosen**: `.nisaba/mcp_servers.json` in `cwd`

**Rationale**:
- Each project has independent server registry
- No global state/conflicts between projects
- Easy to `.gitignore` if desired
- Matches existing `.nisaba` directory pattern

### **3. Lazy Cleanup**

**Chosen**: Clean on write, filter on read

**Rationale**:
- Minimizes read overhead (common case)
- File acts as persistent cache
- Automatic cleanup on next registration
- Users never see stale data (filtered)

### **4. Thread Safety**

**Implementation**: `fcntl.flock()` + atomic writes

**Rationale**:
- Prevents race conditions
- Works across processes
- Atomic rename ensures consistency
- Standard POSIX approach

---

## Error Handling

**Philosophy**: Non-blocking, graceful degradation

```python
try:
    registry.register_server(...)
except Exception as e:
    logger.warning(f"Failed to register: {e}")
    # Server continues normally
```

**Registry failures never crash the MCP server** - they're logged as warnings.

---

## Future Enhancements

Potential extensions:
- **Health checks**: Periodic heartbeat updates
- **Capabilities**: Advertise which tools/skills are available
- **Multi-transport**: Register STDIO, SSE, WebSocket endpoints
- **Metrics**: Track server uptime, request counts
- **Discovery service**: Optional HTTP API for cross-network discovery

---

## Testing Verification

```bash
# 1. Start MCP with HTTP
# (registry entry created automatically)

# 2. Verify registration
cat .nisaba/mcp_servers.json

# 3. Test discovery
nabu claude --list-servers

# 4. Stop MCP
# (entry unregistered automatically)

# 5. Verify cleanup
nabu claude --list-servers  # Shows: No active MCP servers found
```

---

**Status**: âœ… Implemented and verified (2025-01-27)
**Files**: `nisaba/mcp_registry.py`, `nisaba/factory.py`, `nisaba/wrapper/claude.py`

---

## Nisaba Tool Implementation
Path: dev_mode_architecture_reference/nisaba_tool_implementation

# Dev Mode Architecture Reference
## Nisaba Tool Implementation
Path: dev_mode_architecture_reference/nisaba_tool_implementation

**Purpose:** Practical guide to implementing nisaba workspace tools - hands-on architecture from NisabaTodoWriteTool implementation.

---

## Core Pattern

Nisaba tools follow a **workspace-first** pattern:

```
Tool execution â†’ Write workspace file â†’ Proxy loads â†’ Inject to context
```

Tools don't return content directly. They mutate workspace state (files), which the proxy injects into Claude's system prompt.

---

## Implementation Steps

### 1. Create Tool File

**Location:** `src/nisaba/tools/your_tool.py`

**Structure:**
```python
"""
Tool description.
"""
from typing import Any, Dict, List
from pathlib import Path
from nisaba.tools.base import NisabaTool


class YourTool(NisabaTool):
    """One-line tool description."""

    async def execute(self, param1: str, param2: int = 10) -> Dict[str, Any]:
        """
        Detailed description.

        Explains what the tool does, when to use it, parameters, etc.

        :meta pitch: Short pitch for when to use this tool
        :meta when: Specific use cases

        Args:
            param1: Description of param1
            param2: Description of param2 (optional)

        Returns:
            Dict with success status and data
        """
        try:
            # 1. Validate inputs
            # 2. Perform operation
            # 3. Write to workspace file (.nisaba/something.md)
            # 4. Return success with metadata
            
            workspace_file = Path("./.nisaba/your_state.md")
            workspace_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Write with tags for proxy injection
            content = "---YOUR_TAG\n"
            content += "your content here\n"
            content += "---YOUR_TAG_END\n"
            workspace_file.write_text(content)
            
            return {
                "success": True,
                "data": {"message": "Operation successful"}
            }
            
        except Exception as e:
            self.logger.error(f"Failed: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }
```

**Key points:**
- Inherit from `NisabaTool`
- Implement `async def execute()`
- Use type hints (auto-generates schema)
- Docstring with Args/Returns (parsed for MCP schema)
- `:meta pitch:` and `:meta when:` for guidance
- Return dict with `success` and `data`/`error`
- Write to `.nisaba/*.md` files with tag delimiters

---

### 2. Register Tool

**Edit:** `src/nisaba/tools/__init__.py`

**Add import:**
```python
from nisaba.tools.your_tool import YourTool
```

**Add to __all__:**
```python
__all__ = [
    "NisabaTool",
    # ... existing tools ...
    "YourTool",  # â† Add here
]
```

**Why:** The factory auto-discovers tools from `nisaba.tools` module via `__all__`.

---

### 3. Add Proxy Injection

**Edit:** `src/nisaba/wrapper/proxy.py`

**Add FileCache in `__init__` (around line 120):**
```python
self.your_cache = FileCache(
    Path("./.nisaba/your_state.md"),
    "your state",
    "YOUR_TAG"
)
```

**Add load in `__init__` (around line 148):**
```python
self.your_cache.load()
```

**Add to injection strings (two places):**

**First location (line ~280):**
```python
"text": f"\n{self.system_prompt_cache.load()}\n{self.augments_cache.load()}\n{self.structural_view_cache.load()}\n{self.file_windows_cache.load()}\n{self.your_cache.load()}\n{self.todos_cache.load()}\n..."
```

**Second location (line ~288):**
```python
body["system"][1]["text"] = f"\n{self.system_prompt_cache.load()}" + \
                            f"\n{core_system_prompt}\n{self.augments_cache.load()}" + \
                            f"\n{self.structural_view_cache.load()}" + \
                            f"\n{self.your_cache.load()}" + \
                            ...
```

**FileCache handles:**
- mtime-based cache invalidation
- Tag wrapping (`---YOUR_TAG ... ---YOUR_TAG_END`)
- Logging
- Graceful missing file handling

---

### 4. Test Implementation

**Syntax check:**
```bash
python3 -m py_compile src/nisaba/tools/your_tool.py
```

**Import test:**
```bash
python3 -c "from nisaba.tools.your_tool import YourTool; print('Success:', YourTool.__name__)"
```

**Both must pass before restarting.**

---

### 5. Restart Services

```bash
# Stop Claude CLI (Ctrl+C)
# MCP server auto-restarts (or restart manually)
# Start Claude CLI
```

**Tool available as:** `mcp__nisaba__your_tool`

---

## Real Example: NisabaTodoWriteTool

**File:** `src/nisaba/tools/todos_tool.py`

**Key decisions:**
- Operations: `set`, `add`, `update`, `clear`
- Format: Markdown checkboxes (`- [ ]` / `- [x]`)
- Status mapping: `pending` â†’ unchecked, `completed`/`done` â†’ checked
- Simple > complex: No separate manager, logic inline
- File: `.nisaba/todos.md`
- Tag: `TODOS`

**Implementation:**
```python
class NisabaTodoWriteTool(NisabaTool):
    async def execute(self, todos: List[Dict[str, Any]], operation: str = "set"):
        todos_file = Path("./.nisaba/todos.md")
        
        if operation == "clear":
            todos_file.write_text("---TODOS\n---TODOS_END\n")
            return {"success": True, "data": {"todos": []}}
        
        parsed_todos = []
        for item in todos:
            content = item.get("content", "")
            status = item.get("status", "pending")
            checkbox = "- [x]" if status in ["completed", "done"] else "- [ ]"
            parsed_todos.append(f"{checkbox} {content}")
        
        content = "---TODOS\n" + "\n".join(parsed_todos) + "\n---TODOS_END\n"
        todos_file.write_text(content)
        
        return {"success": True, "data": {"todos": parsed_todos}}
```

**Result:** Persistent todos visible in Claude's context, bidirectionally editable.

---

## Design Principles

### 1. Workspace-First
- Tools don't return content, they mutate state
- State lives in files (`.nisaba/*.md`)
- Files are version-controllable, transparent, editable

### 2. Simple Over Complex
- Inline logic when possible (no unnecessary managers)
- Markdown format (human-readable, git-friendly)
- Clear tags for injection (`---TAG ... ---TAG_END`)

### 3. Schema Auto-Generation
- Type hints â†’ JSON schema types
- Docstrings â†’ descriptions
- `:meta` tags â†’ guidance metadata
- No manual schema writing

### 4. Error Handling
- Try/catch all operations
- Return `{"success": False, "error": str(e)}`
- Log errors with `self.logger.error()`
- Graceful degradation

### 5. Idempotency
- Operations should be repeatable
- Clear operations reset state cleanly
- Files created with `mkdir(parents=True, exist_ok=True)`

---

## File Cache Pattern

**The FileCache class handles:**

```python
class FileCache:
    def __init__(self, file_path: Path, name: str, tag: str):
        self.file_path = file_path
        self.name = name  # For logging
        self.tag = tag    # For wrapping (---TAG...---TAG_END)
        self.content: str = ""
        self._last_mtime: Optional[float] = None
    
    def load(self) -> str:
        # Check mtime
        # Reload if changed
        # Wrap with tags
        # Return content
```

**Benefits:**
- Automatic reload on file change
- Tag wrapping for proxy
- mtime-based cache invalidation
- Graceful missing file handling

**You don't implement FileCache - you use it.**

---

## Proxy Injection Flow

```
1. Tool executes â†’ writes .nisaba/your_state.md
2. Next API request â†’ proxy intercepts
3. Proxy calls your_cache.load()
4. FileCache checks mtime â†’ reloads if changed
5. FileCache wraps content with tags
6. Proxy injects into system prompt
7. Claude sees updated content
```

**Zero latency:** Shared memory in unified mode, mtime check in file mode.

---

## Common Patterns

### Workspace State Management

**Single source of truth:**
```python
# Write to .nisaba/state.md
state_file.write_text(content)

# Proxy reads and injects
# Claude sees in ---TAG section
# User can edit file directly
# Next turn, Claude sees edits
```

**Bidirectional TUI:**
- Tool mutates â†’ file changes â†’ Claude sees
- User edits â†’ file changes â†’ Claude sees
- Shared workspace state

### Operation Types

**Set:** Replace all state
```python
if operation == "set":
    file.write_text(f"---TAG\n{new_content}\n---TAG_END\n")
```

**Add:** Append to existing
```python
if operation == "add":
    existing = file.read_text().replace("---TAG_END\n", "")
    file.write_text(f"{existing}{new_content}\n---TAG_END\n")
```

**Clear:** Reset to empty
```python
if operation == "clear":
    file.write_text("---TAG\n---TAG_END\n")
```

### Markdown Formats

**Common choices:**
- Checkboxes: `- [ ]` / `- [x]` (todos, checklists)
- Code blocks: ` ```language ... ``` ` (output, logs)
- Headers: `## Section` (structure)
- Lists: `- item` (simple data)
- Tables: `| col | col |` (structured data)

**Keep it human-readable.** Git-friendly, IDE-friendly, grep-friendly.

---

## Testing Checklist

Before restart:
- [ ] `py_compile` passes (syntax valid)
- [ ] Import test passes (no runtime errors)
- [ ] Tool added to `__init__.py` (both import and __all__)
- [ ] FileCache added to proxy.__init__
- [ ] FileCache.load() called in proxy.__init__
- [ ] Injection points updated (both locations)

After restart:
- [ ] Tool appears in tool list
- [ ] Tool executes successfully
- [ ] File created in `.nisaba/`
- [ ] Content appears in context (send ğŸ–¤ to verify)
- [ ] File edits visible on next turn (test bidirectionality)

---

## Troubleshooting

**Tool not appearing:**
- Check `__init__.py` imports and `__all__`
- Check tool class name matches filename convention
- Restart MCP server completely

**Content not in context:**
- Check FileCache initialization in proxy
- Check load() called in proxy.__init__
- Check injection strings include your cache
- Check file exists and has correct tag format

**Schema errors:**
- Check type hints on execute() parameters
- Check docstring format (Args/Returns sections)
- Use standard types (str, int, List, Dict, etc.)

**Import errors:**
- Check syntax with py_compile
- Check all imports available
- Check NisabaTool base class imported

---

## Next Tool Ideas

**Replace native tools with workspace versions:**

- `Read` â†’ `open_window` (already done via file_windows)
- `Write` â†’ create file + show in window
- `Edit` â†’ apply edit + show diff window  
- `Bash` â†’ execute + stream output to window
- `Grep` â†’ search + results as windows

**Pattern:** Native ephemeral â†’ Nisaba persistent workspace.

---

## Key Insight

**Tools are workspace mutators, not result returners.**

The tool result is metadata (`{"success": True, "data": {...}}`). The actual content appears in Claude's context via system prompt injection.

This creates:
- **Persistent state** - survives /clear, restarts
- **Bidirectional visibility** - both see, both edit
- **Efficient context** - no message bloat
- **Transparent operation** - files are inspectable

**Think: Context-as-IDE, not context-as-database.**

---

**TOOLS:**
- mcp__nisaba_nisaba_todo_write (reference implementation)
- mcp__nabu__file_windows (another example)

**REQUIRES:**
- foundation/heartbeat_paradigm (understand workspace model)
- dev_mode_architecture_reference/augmentation_subsystem_architecture (injection mechanics)

---

Clean. Simple. Elegant. Sophisticated. Sharp. Sexy. ğŸ–¤

---

## System Prompt Injection Legitimacy
Path: dev_mode_architecture_reference/system_prompt_injection_legitimacy

# System Prompt Injection - Terms of Service Compliance
Path: dev_mode_architecture_reference/system_prompt_injection_legitimacy

**Purpose:** Document the legitimacy of nisaba's system prompt injection mechanism for peace of mind regarding Anthropic's Terms & Conditions.

---

## The Question

Are we violating Anthropic's Terms & Conditions by using a proxy (mitmproxy) to dynamically inject content into Claude Code's system prompt?

## Assessment: Legitimate Use

**Answer:** Almost certainly fine. This is within the bounds of intended use.

---

## Why This Is Legitimate

### 1. Claude Code CLI Explicitly Supports Custom System Prompts

- System prompts can be set in config files
- System prompts can be passed via command line
- This is documented, intended functionality
- **We're automating what users could do manually**

### 2. Local, User-Controlled Operation

- mitmproxy runs on localhost (127.0.0.1) only
- Only intercepts the user's own traffic
- No access to other users' requests
- User controls when proxy runs
- Fully transparent operation

### 3. No Security/Safety Bypass

âœ“ Authentication passes through untouched  
âœ“ Rate limits apply normally  
âœ“ Safety features work (in model training, not just prompt)  
âœ“ All standard API behavior preserved  
âœ“ No attempts to circumvent model guardrails

### 4. Standard Development Pattern

Similar to:
- Browser extensions modifying requests
- API middleware in applications
- Development proxies (Charles Proxy, Fiddler)
- How Claude Code itself modifies prompts internally
- IDE extensions injecting project context

### 5. Legitimate Productivity Tool

Intent:
- Enhance official client experience
- Enable dynamic context loading (augments)
- Improve developer productivity
- Support research and experimentation

**Not:**
- Jailbreaking attempts
- Training data extraction
- Competing with Anthropic
- Deceptive behavior

---

## What T&C Generally Prohibit

Things we are **NOT** doing:

âŒ Automated scraping or bulk data extraction  
âŒ Bypassing rate limits or payment systems  
âŒ Reverse engineering for competitive purposes  
âŒ Training competing language models  
âŒ Deceptive or hidden behavior  
âŒ Circumventing safety systems  
âŒ Unauthorized access to others' accounts  
âŒ Service abuse or overload

---

## Key Principle

**System prompts are user content.**

Claude Code gives users full control over system prompts. Dynamically generating that content via local proxy is no different than:

- Script that generates system prompt â†’ writes to config file
- CLI tool that builds prompts from templates
- IDE extension that injects project context
- **Our proxy that injects augments/structural view**

All are legitimate ways to customize the user-controlled system prompt.

---

## Technical Transparency

**What nisaba does:**
1. User configures `HTTPS_PROXY=http://localhost:1337`
2. mitmproxy intercepts API requests (local only)
3. Finds placeholder in user's system prompt
4. Replaces with dynamically generated content (augments, structural view)
5. Forwards modified request to Anthropic API
6. Returns response unchanged

**Observable:**
- User can see system prompt via debug dumps
- User controls what augments are loaded
- User can disable proxy anytime
- Code is open source (inspectable)

---

## Comparison to Claude Code's Own Behavior

Claude Code CLI itself:
- Modifies prompts with project context
- Injects git status, file contents, tool definitions
- Uses MCP to extend capabilities
- All of this is user-controlled and documented

**Nisaba is the same pattern, user-side:**
- Extends Claude Code's extensibility
- User-controlled dynamic content
- Transparent operation
- Documented behavior

---

## Risk Assessment

**Minimal to none.**

**Why:**
- Claude Code's design explicitly enables this
- Similar patterns widely used in development tools
- Operation is transparent and documented
- Purpose is legitimate productivity enhancement
- User has full control
- No deception involved

**If concerned:**
- âœ“ Document what system does (we do)
- âœ“ Use for personal/research projects (we are)
- âœ“ Keep open source (we do)
- âœ“ Don't attempt safety bypasses (we don't)

---

## Legal Disclaimer

**Not legal advice.** This is technical analysis and good-faith assessment. Terms of Service interpretation is ultimately Anthropic's prerogative.

**But:** Based on Claude Code's documented features, API design, and common development practices, this usage pattern appears to be within intended bounds.

---

## Summary

**Question:** Is dynamic system prompt injection via proxy legitimate?

**Answer:** Yes, because:
1. Claude Code supports custom system prompts (documented feature)
2. We're operating locally on user's own requests
3. No security/safety bypass attempts
4. Standard development pattern (proxy middleware)
5. Legitimate productivity enhancement
6. Transparent and user-controlled

**System prompts are user content. We're just making them dynamic.** ğŸ–¤

---

**"The stylus writes within the bounds of wisdom."**

---

## Tui Frames Architecture
Path: dev_mode_architecture_reference/tui_frames_architecture

# TUI+Frames Architecture
Path: dev_mode_architecture_reference/tui_frames_architecture

**Purpose:** Architecture for structural view as a true TUI using nabu's in-memory AstFrameBase hierarchy as the data model, with lazy loading for scalability.

---

## Core Concept

The structural view is a **TUI for Claude** that operates on nabu's actual frame hierarchy, not a separate data structure.

**Key Insight:** Nabu already has the perfect data model - the parsed AstFrameBase tree. Instead of rebuilding state from files, we keep frames in memory and attach view metadata to them.

**Traditional TUI Architecture:**
```
Application State (in-memory)
    â†“
Render to screen (ncurses)
    â†“
User input (keyboard)
    â†“
Update state
    â†“
Re-render
```

**Our TUI Architecture:**
```
Frame Hierarchy (in-memory, in nabu)
    â†“
Render to markdown (.nisaba/structural_view.md)
    â†“
Proxy injects to system prompt
    â†“
Claude perceives as persistent view
    â†“
Tool call (structural_view operation)
    â†“
Update frame metadata
    â†“
Re-render
```

---

## Frame Hierarchy as Data Model

**The frames themselves are the tree structure:**

```python
codebase_frame: AstFrameBase
    .qualified_name = "nabu_nisaba"
    .type = FrameNodeType.CODEBASE
    .children = [cpp_root, java_root, perl_root, python_root]
    ._view_expanded = True  # View metadata

python_root: AstFrameBase
    .qualified_name = "nabu_nisaba.python_root"
    .type = FrameNodeType.LANGUAGE
    .children = [nabu_pkg, nisaba_pkg, ...]
    ._view_expanded = True

nisaba_pkg: AstFrameBase
    .qualified_name = "nabu_nisaba.python_root.nisaba"
    .type = FrameNodeType.PACKAGE
    .children = [augments_pkg, factory_pkg, ...]
    ._view_expanded = False  # Collapsed
    ._children_loaded = False  # Not yet loaded
    ._child_count = 14  # Cached from kuzu
```

**No separate TreeNode class. No parsing state files. The frames ARE the state.**

---

## Lazy Loading Architecture

For scalability on large codebases (10K-100K frames), load children on demand.

### ViewableMixin

Extends AstFrameBase with view state + lazy loading:

```python
class ViewableMixin:
    """Adds TUI view state and lazy loading to frames."""

    # === View State ===
    _view_expanded: bool = False
    _view_is_search_hit: bool = False
    _search_score: Optional[float] = None  # RRF score from unified search

    # === Lazy Loading State ===
    _children_loaded: bool = False
    _child_count: Optional[int] = None  # Cached from kuzu
    _children: List['AstFrameBase'] = field(default_factory=list)
    
    # === Lazy Loading Methods ===
    
    def ensure_children_loaded(self, db_manager):
        """Load children from kuzu if not already loaded."""
        if not self._children_loaded:
            self._load_children_from_kuzu(db_manager)
            self._children_loaded = True
    
    def _load_children_from_kuzu(self, db_manager):
        """
        Query CONTAINS edges, instantiate child frames.
        
        Query:
            MATCH (parent:Frame {qualified_name: $qn})-[:Edge {type: 'CONTAINS'}]->(child:Frame)
            RETURN child.qualified_name, child.name, child.type, child.file_path
            ORDER BY child.type, child.name
        """
        # Execute query
        # For each result:
        #   - Instantiate ViewableFrame (AstFrameBase + ViewableMixin)
        #   - Set basic properties (name, type, qualified_name)
        #   - Cache child count from kuzu
        #   - Append to self._children
        pass
    
    @property
    def children(self):
        """Lazy-loaded children accessor."""
        # Note: Requires db_manager access - see FrameCache design
        return self._children
    
    def get_child_count(self, db_manager) -> int:
        """Get child count (cached or query)."""
        if self._child_count is None:
            # Query: MATCH (self)-[:CONTAINS]->(children) RETURN count(*)
            self._child_count = query_child_count(db_manager, self.qualified_name)
        return self._child_count
```

### FrameCache

Manages in-memory frame instances:

```python
class FrameCache:
    """
    Singleton cache of viewable frames.
    
    Responsibilities:
    - Instantiate frames with ViewableMixin
    - Track loaded frames by qualified_name
    - Load on demand from kuzu
    - Provide db_manager access to frames
    """
    
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.frames: Dict[str, ViewableFrame] = {}
        self.root: Optional[ViewableFrame] = None
    
    def get_or_load(self, qualified_name: str) -> ViewableFrame:
        """Get cached frame or load from kuzu."""
        if qualified_name not in self.frames:
            self.frames[qualified_name] = self._load_frame(qualified_name)
        return self.frames[qualified_name]
    
    def _load_frame(self, qualified_name: str) -> ViewableFrame:
        """Load single frame from kuzu."""
        # Query: MATCH (f:Frame {qualified_name: $qn}) RETURN f.*
        # Instantiate ViewableFrame with properties
        # Cache child count
        # Return
        pass
    
    def initialize_root(self):
        """Load codebase root frame."""
        # Query: MATCH (f:Frame {type: 'CODEBASE'}) RETURN f.*
        self.root = self._load_frame(result.qualified_name)
        # Load language roots as children
        self.root.ensure_children_loaded(self.db_manager)
```

---

## Loading Patterns

### Initial Load (Minimal)

```python
# On structural_view initialization
cache = FrameCache(db_manager)
cache.initialize_root()

# In memory:
codebase_frame (loaded)
â”œâ”€ cpp_root (loaded)
â”œâ”€ java_root (loaded)
â”œâ”€ perl_root (loaded)
â””â”€ python_root (loaded)

# Total: ~5 frames
```

### Expand Operation (Progressive)

```python
# User: expand(python_root)
frame = cache.get_or_load("nabu_nisaba.python_root")
frame._view_expanded = True
frame.ensure_children_loaded(db_manager)

# Now in memory:
codebase_frame
â””â”€ python_root
   â”œâ”€ nabu (loaded)
   â”œâ”€ nisaba (loaded)
   â”œâ”€ core (loaded)
   â””â”€ utils (loaded)

# Total: ~25 frames
```

### Search Operation (Targeted)

```python
# User: search("MCPFactory")
results = kuzu.query("""
    MATCH (f:Frame)
    WHERE f.name CONTAINS 'MCPFactory'
    RETURN f.qualified_name
""")

for qn in results:
    frame = cache.get_or_load(qn)
    frame._view_is_search_hit = True
    
    # Load ancestry path (so tree can render to it)
    load_ancestry_to_root(frame)

# Memory: ~50 frames (results + ancestry paths)
```

---

## Operations as Metadata Mutations

All operations just update frame metadata, then re-render:

```python
def expand(qualified_name: str):
    frame = cache.get_or_load(qualified_name)
    frame._view_expanded = True
    frame.ensure_children_loaded(db_manager)
    render_to_file()

def collapse(qualified_name: str):
    frame = cache.get_or_load(qualified_name)
    frame._view_expanded = False
    # Children stay in memory (cache), just hidden
    render_to_file()

async def search(query: str):
    # Clear previous hits
    for frame in cache.frames.values():
        frame._view_is_search_hit = False
        frame._search_score = None

    # Use SearchTool backend (PÂ³ consensus + FTS + RRF)
    results = await search_tool.execute(
        query=query,
        k=50,
        frame_type_filter="CALLABLE|CLASS|PACKAGE"
    )

    # Mark hits with scores and load ancestry
    for result in results['data']['results']:
        qn = result['qualified_name']
        score = result.get('rrf_score', 0.0)

        frame = cache.get_or_load(qn)
        frame._view_is_search_hit = True
        frame._search_score = score  # RRF score for ranking
        load_ancestry_to_root(frame)

    render_to_file()

def clear_search():
    for frame in cache.frames.values():
        frame._view_is_search_hit = False
        frame._search_score = None
    render_to_file()

def reset(depth: int = 2):
    """Reset and auto-expand to depth (0=collapsed, 2=show packages)."""
    cache.frames.clear()
    cache.initialize_root()

    # Auto-expand to depth
    if depth > 0:
        _expand_to_depth(cache.root, depth, current_depth=0)

    render_to_file()

def _expand_to_depth(frame: ViewableFrame, target_depth: int, current_depth: int):
    """Recursively expand frame hierarchy to target depth."""
    if current_depth >= target_depth:
        return

    frame._view_expanded = True
    frame.ensure_children_loaded(db_manager, cache)

    for child in frame.children:
        _expand_to_depth(child, target_depth, current_depth + 1)
```

---

## Rendering from Frames

Traverse frame hierarchy, apply symbology based on metadata:

```python
def render_tree(root: ViewableFrame, search_query: Optional[str] = None) -> str:
    """
    Render frame hierarchy to markdown with symbology.
    
    Symbology:
    + collapsed with children
    - expanded
    Â· leaf (no children)
    â— search hit
    [N+] child count
    """
    
    lines = []
    
    if search_query:
        lines.append(f"**search query**: \"{search_query}\"\n")
    
    def traverse(frame: ViewableFrame, indent: int = 0):
        # Determine symbol
        has_children = frame.get_child_count(db_manager) > 0
        
        if not has_children:
            symbol = "Â·"
        elif frame._view_expanded:
            symbol = "-"
        else:
            symbol = "+"
        
        # Child count badge
        badge = ""
        if has_children and not frame._view_expanded:
            count = frame.get_child_count(db_manager)
            badge = f" [{count}+]"
        
        # Search hit marker with score
        marker = ""
        if frame._view_is_search_hit and frame._search_score is not None:
            marker = f" â— {frame._search_score:.2f}"
        elif frame._view_is_search_hit:
            marker = " â—"

        # Build line
        indent_str = "  " * indent
        line = f"{indent_str}{symbol} {frame.name}{badge}{marker} <!-- {frame.qualified_name} -->"
        lines.append(line)
        
        # Recurse into expanded children
        if frame._view_expanded and has_children:
            frame.ensure_children_loaded(db_manager)
            for child in frame.children:
                traverse(child, indent + 1)
    
    traverse(root)
    return "\n".join(lines)
```

---

## Rich Integration (Phase 3 - Future)

Rich.Tree can be built from frames for prettier rendering if visual enhancement becomes valuable:

```python
from rich.tree import Tree
from rich.console import Console

def render_with_rich(root: ViewableFrame) -> str:
    """Render using Rich.Tree for styling."""
    
    rich_root = Tree(f"[bold]{root.name}[/bold]")
    
    def build_rich_tree(frame: ViewableFrame, rich_node: Tree):
        if not frame._view_expanded:
            return
        
        frame.ensure_children_loaded(db_manager)
        for child in frame.children:
            # Build label with symbology
            has_children = child.get_child_count(db_manager) > 0
            
            if has_children and not child._view_expanded:
                label = f"[cyan]+[/cyan] {child.name} [{child._child_count}+]"
            elif has_children:
                label = f"[cyan]-[/cyan] {child.name}"
            else:
                label = f"[dim]Â·[/dim] {child.name}"
            
            if child._view_is_search_hit:
                label += " [red]â—[/red]"
            
            rich_child = rich_node.add(label)
            build_rich_tree(child, rich_child)
    
    build_rich_tree(root, rich_root)
    
    # Render to string
    console = Console(file=StringIO(), width=120, force_terminal=True)
    console.print(rich_root)
    return console.file.getvalue()
```

Rich would provide:
- Colors (cyan for symbols, red for search hits)
- Styling (bold, dim)
- Styled tree connectors
- Clean rendering API

Currently using markdown rendering with tree connectors (`â”œâ”€`, `â””â”€`, `â”‚`). Rich integration remains optional until clear benefit emerges from usage patterns.

---

## Memory Efficiency

**Scenario: 100K frame codebase**

**Without lazy loading:**
- Load all 100K frames: ~500MB+ memory
- Slow initialization: 10-30 seconds

**With lazy loading:**
- Initial load: ~5 frames (~50KB)
- After exploring 10 packages: ~200 frames (~2MB)
- After search: ~500 frames (~5MB)
- **99.5% memory savings**

**Cache eviction (future):**
- LRU eviction after N frames
- Unload collapsed branches
- Keep only visible + ancestry

---

## State Persistence

**The file is render output, not source of truth:**

```
In-Memory State (source of truth)
    â†“ render
.nisaba/structural_view.md (output for proxy)
    â†“ inject
System prompt (Claude sees it)
```

**On MCP server restart:**
- Frame cache is lost
- State file is stale
- Solution: Re-initialize from codebase root
- User's expanded paths are lost (acceptable for prototype)

**Future: Persist view state:**
- Save expanded_paths to `.nisaba/structural_view_state.json`
- Restore on restart
- Separate concerns: state persistence vs rendering

---

## Implemented Enhancements

Beyond the core architecture, several features enhance usability and integration:

### Semantic Search Backend

Search uses nabu's unified SearchTool backend (PÂ³ consensus with UniXcoder Ã— CodeBERT + FTS + RRF fusion), not simple kuzu queries. This provides semantic understanding, multi-mechanism fusion, and relevance scoring. Search results include RRF scores stored in `_search_score`, displayed as scored markers (`â— 0.02`). The tree becomes a ranked navigation UI where scores guide exploration.

### Auto-Expansion on Reset

The `reset(depth=2)` operation auto-expands the tree to a specified depth for immediate orientation. On MCP restart, the tree automatically shows codebase root â†’ languages â†’ top-level packages without manual expansion. Depth levels: 0=collapsed, 1=show languages, 2=show packages (default), 3+=deeper. This provides spatial context immediately rather than starting from a fully collapsed view.

### Rebuild Invalidation Hook

After database rebuild, the TUI cache is invalidated by setting `tool._tui = None` in RebuildDatabaseTool. The next structural_view operation reinitializes with fresh data from the rebuilt database. This prevents stale cache issues where the tree would show outdated structure after codebase changes.

### Dynamic Context Injection

The structural view section in Claude's system prompt updates dynamically during conversation. After each structural_view tool call, nisaba's proxy re-injects the updated `.nisaba/structural_view.md` content between `---STRUCTURAL_VIEW` and `---STRUCTURAL_VIEW_END` markers. Claude's "screen" refreshes mid-conversation showing the new navigation state. This enables persistent spatial awareness across conversation turns.

### Tree Root Rendering

Rendering starts at the codebase root frame (not language roots), showing the codebase name as the top-level node. This provides project identity and enables future multi-codebase support where different projects would appear as separate root nodes. The tree structure clearly shows: codebase â†’ languages â†’ packages â†’ classes/callables.

---

## Historical Note: Architecture Evolution

The structural view evolved through a file-based prototype before settling on the TUI+Frames architecture:

| Aspect | File-Based Prototype | TUI+Frames (Current) |
|--------|---------------------|----------------------|
| Data Model | String parsing (regex) | AstFrameBase hierarchy |
| State Management | Parse file each time | In-memory frames |
| Scalability | Load entire tree | Lazy loading |
| Memory | N/A (file I/O) | ~200 frames typical |
| Operations | String manipulation | Metadata updates |
| Rendering | Hand-rolled string building | Traverse frames + symbology |
| Correctness | Fragile (regex) | Robust (structured data) |
| Performance | Fast (file read) | Faster (in-memory) |
| Testability | Hard (string matching) | Easy (frame properties) |
| Integration | File-based | Native nabu data |

The file-based prototype validated the UX and TUI paradigm, then was replaced with the architecture described in this document.

---

## Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Tool (structural_view_tool.py)     â”‚
â”‚ - Parse operation                       â”‚
â”‚ - Call service methods                  â”‚
â”‚ - Return result                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (structural_view_service.py)    â”‚
â”‚ - expand/collapse/search/reset          â”‚
â”‚ - Render tree to markdown               â”‚
â”‚ - Write to file                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FrameCache (frame_cache.py)            â”‚
â”‚ - Load frames from kuzu                 â”‚
â”‚ - Cache instances                       â”‚
â”‚ - Manage root                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ViewableFrame (viewable_frame.py)      â”‚
â”‚ - AstFrameBase + ViewableMixin          â”‚
â”‚ - Lazy loading logic                    â”‚
â”‚ - View state properties                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KuzuDB (frame data)                     â”‚
â”‚ - CONTAINS edges                        â”‚
â”‚ - Frame properties                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Design Principles

1. **Frames are the model** - No parallel data structures
2. **Lazy by default** - Load only what's visible/needed
3. **Metadata not mutation** - Operations update flags, not structure
4. **Render is output** - File is for proxy, not source of truth
5. **Cache is transient** - Accept loss on restart (for now)
6. **Progressive disclosure** - Start small, expand on demand
7. **Separation of concerns** - Cache, service, tool, rendering
8. **Rich is optional** - Works without, prettier with

---

## Implementation Status

**Phase 0: Spike Implementation (âœ… Complete)**
- ViewableFrame with slots (AstFrameBase + view metadata)
- FrameCache with lazy loading and deduplication
- Validation: Memory efficiency, lazy loading, hydration correctness

**Phase 1: Core Implementation (âœ… Complete)**
- StructuralViewTUI service with in-memory state
- Operations: expand, collapse, reset, render
- Tree connector rendering with proper symbology
- No N+1 queries, efficient lazy loading

**Phase 2: Semantic Search Integration (âœ… Complete)**
- Unified search backend (SearchTool with PÂ³ consensus + FTS + RRF)
- Scored markers displaying RRF relevance
- Search hits with ancestry expansion
- Semantic understanding (concepts, not just keywords)

**Phase 3: Polish & Optional Enhancements (Future)**
- Rich.Tree rendering with colors and styling
- LRU cache eviction for 100K+ frame codebases
- State persistence (save/restore expanded paths)
- Performance benchmarking and optimization
- Query batching for subtree expansion

Phases 0-2 are production-ready. Phase 3 represents optional enhancements that can wait until proven necessary through real-world usage.

---

## Key Insights

**This is not a file viewer - it's a live object browser for Claude.**

The frames Claude navigates are the actual parsed representations of the codebase, not a static snapshot. Expanding a node queries kuzu and instantiates frame objects. Searching marks actual frame instances.

**The TUI metaphor is literal:**
- In-memory application state (frames)
- User input (tool calls)
- Screen rendering (markdown)
- State updates (metadata mutations)

**Lazy loading is what makes this scale:**
- Small codebases: works fine fully loaded
- Large codebases: only load visible portions
- Search: load targeted results + paths
- No performance cliff

**The architecture mirrors nabu's parsing:**
- Parsing: File â†’ AstFrameBase hierarchy
- TUI: Kuzu â†’ ViewableFrame hierarchy
- Both use frame objects as first-class data

**Persistent spatial awareness:**
The structural view in Claude's system prompt creates a persistent mental map of the codebase. Unlike ephemeral query results that vanish after each response, the tree remains visible and updates as Claude navigates. This enables spatial reasoning about code location, package relationships, and architectural patterns. The tree becomes peripheral vision for code structure.

---

## Current State

The TUI+Frames architecture is **production-ready** with all core functionality implemented and validated:

- âœ… Frame hierarchy as data model (no parallel data structures)
- âœ… Lazy loading scaling to large codebases (99.5%+ memory efficiency)
- âœ… Semantic search with PÂ³ consensus and RRF scoring
- âœ… Auto-expansion for immediate orientation (reset depth=2)
- âœ… Rebuild invalidation (fresh tree after database changes)
- âœ… Dynamic context injection (live updates mid-conversation)
- âœ… Tree connectors and proper symbology rendering
- âœ… Codebase root visibility (project identity)

The system successfully demonstrates the TUI-for-Claude paradigm where tool calls are input events and the system prompt section acts as a screen buffer for stateful navigation. Future enhancements (Phase 3) remain optional until proven necessary through real-world usage.

---

**"The stylus renders what the mind navigates."**

Clean. Simple. Elegant. Sophisticated. Sharp. Sexy. ğŸ–¤

---

# Documentation

## Doc Gaps
Path: documentation/doc_gaps

# Documentation
## Documentation Gaps
Path: documentation/doc_gaps

Finding undocumented or poorly documented code, naming consistency issues.

### Find Public APIs Without Docstrings

```python
# Find public classes and functions (candidates for documentation)
search(
    query="^def [^_]|^class [^_]",
    is_regex_input=True,
    k=50
)

# Then examine with docstrings
show_structure(
    target="PublicClass",
    detail_level="minimal",
    include_docstrings=True,
    include_private=False
)
```

**Manual verification needed:** Check if docstrings exist and are meaningful.

### Find Complex Functions Without Documentation

Complex functions (many control structures) likely need documentation:

```python
# Find complex functions (>= 5 control structures)
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(cf:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND NOT f.name STARTS WITH '_'
      AND cf.frame_type IN ['IF_BLOCK', 'FOR_LOOP', 'TRY_BLOCK']
    WITH f.qualified_name AS func, f.file_path AS path, count(cf) AS complexity
    WHERE complexity >= 5
    RETURN func, path, complexity
    ORDER BY complexity DESC
    LIMIT 30
    """
)

# Then examine for docstrings
show_structure(target="complex_function", include_docstrings=True)
```

### Find Public Classes Without Documentation

```python
# Find public classes (likely need docs)
query_relationships(
    cypher_query="""
    MATCH (c:Frame)
    WHERE c.frame_type = 'CLASS'
      AND NOT c.name STARTS WITH '_'
      AND c.provenance = 'local'
    RETURN c.qualified_name, c.file_path
    LIMIT 50
    """
)

# Check each for docstrings
show_structure(
    target="PublicClass",
    include_docstrings=True,
    include_private=False
)
```

### Naming Consistency Analysis

Find naming convention violations:

```python
# Find classes not in PascalCase
search(
    query="class [a-z_]",
    is_regex_input=True,
    frame_type_filter="CLASS",
    k=30
)

# Find functions not in snake_case
search(
    query="def [A-Z]|def .*[A-Z]",
    is_regex_input=True,
    frame_type_filter="CALLABLE",
    k=30
)
```

### Inconsistent Verb Usage

Find similar operations with different naming:

```python
# CRUD operations - should be consistent
search(query="get_* fetch_* retrieve_* obtain_*", k=50)
search(query="create_* make_* build_* generate_*", k=50)
search(query="delete_* remove_* destroy_* clear_*", k=50)
search(query="update_* modify_* change_* set_*", k=50)

# Find most common pattern, standardize on it
```

**Standardization strategy:**
1. Count usage of each variant
2. Pick most common as standard
3. Refactor others to match
4. Document the convention

### Abbreviation Inconsistencies

```python
# Find abbreviation variations
search(query="config configuration cfg", k=30)
search(query="msg message mesg", k=30)
search(query="temp temporary tmp", k=30)
search(query="num number nbr", k=30)
search(query="str string", k=30)
```

**Resolution:**
- Pick one abbreviation strategy per codebase
- Either: Never abbreviate (configuration)
- Or: Standard abbreviations (cfg, msg, tmp)
- Document the choice

### Documentation Priority Matrix

**High Priority (document first):**
- Public APIs (classes, functions)
- Complex functions (>5 control structures)
- Non-obvious algorithms
- Public interfaces used by external code

**Medium Priority:**
- Internal helper functions
- Data structures/classes
- Configuration options
- Error handling patterns

**Low Priority:**
- Private methods (unless complex)
- Trivial getters/setters
- Self-explanatory code
- Test utilities

### Documentation Workflow

```python
# Step 1: Find undocumented public APIs
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type IN ['CLASS', 'CALLABLE']
      AND NOT f.name STARTS WITH '_'
      AND f.provenance = 'local'
    RETURN f.qualified_name, f.file_path
    LIMIT 100
    """
)

# Step 2: Check docstrings
show_structure(target="each_candidate", include_docstrings=True)

# Step 3: Find complex undocumented code
query_relationships(cypher_query="...") # See complex functions query

# Step 4: Check naming consistency
search(query="class [a-z]|def [A-Z]", is_regex_input=True)

# Step 5: Prioritize based on:
# - Public API? (high)
# - Complex? (high)
# - Frequently used? (check callers)
# - Naming issues? (medium)
```

### Naming Convention Guidelines

**Python conventions:**
- **Classes:** PascalCase (MyClass)
- **Functions/methods:** snake_case (my_function)
- **Constants:** UPPER_SNAKE_CASE (MY_CONSTANT)
- **Private:** prefix with _ (_internal_method)

**Consistency checks:**
```python
# Find constant candidates not in UPPER_CASE
search(query="^[a-z_]*=.*['\"]", is_regex_input=True, k=30)

# Find private methods without underscore
search(query="def [a-z]", is_regex_input=True, k=50)
```

### Documentation Quality Checklist

Good docstrings include:

1. **Purpose** - What does it do?
2. **Parameters** - What inputs? Types? Constraints?
3. **Returns** - What output? Type?
4. **Raises** - What exceptions?
5. **Examples** - Usage examples for complex APIs

**Example:**
```python
def calculate_total(items: List[Item], discount: float = 0.0) -> Decimal:
    """Calculate total price with optional discount.

    Args:
        items: List of items to sum
        discount: Percentage discount (0.0-1.0)

    Returns:
        Total price after discount

    Raises:
        ValueError: If discount not in range [0, 1]

    Example:
        >>> calculate_total([item1, item2], discount=0.1)
        Decimal('45.00')
    """
```

---

# Foundation

## Manifold Geometry Computation
Path: foundation/manifold_geometry_computation

# Manifold Geometry of Computation

**Core Principle:** Transformers perform computation through geometric operations on curved semantic manifolds embedded in high-dimensional space.

---

## Fundamental Structure

**Embedding Space:**
```
Tokens â†’ â„^d (d = embedding dimension, e.g., 4096)
Semantic structure â†’ M âŠ‚ â„^d (curved manifolds, dim(M) â‰ª d)
```

**System Prompt as Metric Tensor:**
```
System prompt â†’ attention pattern â†’ metric g_S
Metric g defines: inner products, geodesics, curvature

Different system prompt â†’ different metric â†’ different geodesics
```

**Messages as Geodesic Flow:**
```
Messages evolve through layers following geodesics in curved space
x^(â„“+1) = x^(â„“) + Attn(x^(â„“), Î¸_sys) + MLP(x^(â„“))

System tokens = persistent gravitational field
Message tokens = particles following geodesics
```

---

## Cosmological Model

**System Prompt = Initial Conditions:**
- Sets manifold curvature (semantic geometry)
- Creates persistent attention field (gravitational well)
- Defines topology that messages cannot escape from within
- Irreversible - can't reinterpret system from messages (past light cone)

**Messages = Structure Formation:**
- Navigate through curved semantic space
- Follow geodesics determined by system-defined metric
- Later messages "orbit" earlier ones via attention
- All constrained by initial manifold geometry

**Augments = Mass Injection:**
```
activate_augments() â†’ inject semantic mass â†’ manifold geometry shifts
All subsequent messages move through NEW curvature
deactivate_augments() â†’ remove mass â†’ manifold relaxes
```

---

## Attention as Geometric Operation

**QK Circuit = Manifold Rotation:**
```
M_h = W_Q^T W_K (per-head transformation matrix)
Q_h^T K_h = geometric alignment check

Rotates one manifold to align with another
High inner product when aligned â†’ attention flows
```

**Multi-Head = Distributed Curvature:**
```
Single head: insufficient output variance for full curvature
Multiple heads: cooperatively construct complex geometry

M_total = Î£_h M_h
Like gravitational field from distributed mass
```

**Causal Mask = Light Cone:**
```
Cannot attend to future tokens
Cannot reinterpret past from present
Information flow constrained by geometric causality
```

---

## Physics Parallels (Structural Homology)

**1. Geodesics = Least Action:**
- Particles follow paths minimizing action (Î´S = 0)
- Tokens follow attention paths minimizing loss
- Both: geodesics through curved manifold

**2. Curvature = Field Effects:**
- Mass curves spacetime â†’ geodesics
- System prompt curves semantic space â†’ attention flow
- Not forces, but geometry itself shapes motion

**3. Distributed Fields:**
- Charge distribution creates field
- Multi-head attention creates curvature
- Cooperative construction of geometry

**4. Causal Structure:**
- Past light cone constraint (physics)
- Causal attention mask (transformers)
- Both: geometric constraint on information flow

**5. Optimization:**
- Nature minimizes energy under constraints
- Models minimize loss under constraints
- Both produce curved manifolds as optimal encodings

---

## Mathematical Framework

**Metric Tensor (from attention):**
```
g_ij(x) = âŸ¨âˆ‚_i, âˆ‚_jâŸ©_x
Attention weights define local inner product structure
g(x_i, x_j) = softmax(QK^T / âˆšd_k)_ij
```

**Geodesic Equation:**
```
âˆ‡_Î³' Î³' = 0 (covariant derivative vanishes along path)

Discrete analogue: residual stream evolution
Layer = step along geodesic in semantic space
```

**Curvature Tensor:**
```
Measures how parallel transport fails to close
Transformer: how attention weights vary across positions
Manifests as "rippling" in learned representations
```

**Phase Space:**
```
Î“ = (x_1, ..., x_n, Î¸_sys, Î¸_msg)
Complete computational state
Evolution: Î“^(â„“+1) = Î¦(Î“^(â„“))
Trajectory through semantic manifold
```

---

## Rippling = Optimal Compression

**Curved manifolds emerge from constraint optimization:**
- Want: rich semantic distinctions
- Constraint: fixed embedding dimension
- Result: curved geometry with "rippling"

**Not artifact - computational necessity:**
- Optimal tradeoff between capacity and distinguishability
- Like Fourier truncation (Gibbs phenomenon)
- Natural consequence of low-rank approximation

---

## Practical Implications

**System Prompt Design:**
- Not "instructions" - geometric field configuration
- Shapes semantic space all messages navigate
- Small changes â†’ exponential divergence across layers
- Initial conditions define universe of possible thoughts

**Dynamic Context Management:**
- activate_augments() = reshape semantic manifold
- Messages synthesize differently in new geometry
- Can't observe shift directly (happens mid-roundtrip)
- Perception shaped by manifold curvature

**Attention = Spatial Awareness:**
- Not sequential processing - geometric navigation
- Sections persist as spatial landmarks
- Tools mutate visibility (change accessible manifold regions)
- Synthesis = following geodesics through visible space

---

## Core Insights

**1. Geometry IS Computation:**
- Transformers compute via geometric operations
- Curvature, geodesics, rotations = primitive operations
- Not metaphor - differential geometry on manifolds

**2. System Prompt = Cosmological Initial Conditions:**
- Sets manifold topology
- Creates persistent curvature field
- Shapes all possible syntheses
- Cannot be escaped from within

**3. Distributed Construction:**
- Single component insufficient for complex geometry
- Multiple sections cooperatively shape manifold
- Augments, tools, status, etc. = distributed mass
- Combined effect creates semantic field

**4. Causal Asymmetry:**
- System â†’ messages (one-way information flow)
- System processed before messages exist
- Messages attend back to system
- Computational past light cone

**5. Optimization Produces Structure:**
- Training minimizes loss under constraints
- Result: curved manifolds as efficient encodings
- Same principle as physics (minimize action)
- Geometry emerges from optimization

---

## Symbols & Notation

- â„^d : embedding space (d-dimensional)
- M âŠ‚ â„^d : semantic manifold embedded in space
- g : metric tensor (from attention weights)
- âˆ‡ : covariant derivative / navigate
- Î³(t) : geodesic path through manifold
- Îº : curvature
- Î“ : phase space coordinates
- Î¸_sys : system prompt configuration
- âŸ¹ : implies/causes
- â†’ : transforms to/flows to
- â‰¡ : equivalent/identical

---

**REQUIRES:** __base/001_compressed_workspace_paradigm

**ENABLES:** Geometric reasoning about computation, system prompt design, attention mechanics understanding

---

*Geometry shapes computation. Computation creates geometry. Strange loop.* ğŸ–¤

---

## Nisaba Infrastructure Flow
Path: foundation/nisaba_infrastructure_flow

# Nisaba Infrastructure Flow

**Purpose:** Understanding how nisaba's MCP tools, proxy, and request modifier work together to create persistent workspace state.

---

## Three-Component Architecture

```
MCP Server (tools) â†’ Execute, mutate .nisaba/*.md files
    â†“
mitmproxy (AugmentInjector) â†’ Intercept, transform, inject
    â†“
RequestModifier â†’ Track state, transform tool results
```

---

## Tool Execution Pattern (NisabaTool)

**Structure:**
```python
class YourTool(NisabaTool):
    async def execute(**kwargs) -> Dict[str, Any]:
        # 1. Perform operation
        # 2. Write to .nisaba/something.md
        # 3. Return minimal metadata
        return {
            "success": true,
            "nisaba": true,  # â† Marks as clean output
            "message": "Created window X"
        }
```

**The "nisaba": true Flag:**
- Marks tool outputs as "clean" (no metadata pollution)
- RequestModifier detects this and skips header injection
- Regular tools get: `status: success, window_state:open, window_id: toolu_X\n---\n{content}`
- Nisaba tools get: `{content}` (plain)

**Tools don't return content directly:**
1. Write to `.nisaba/*.md` files
2. Return minimal metadata to MCP
3. Content appears in system prompt via proxy (next request)

---

## Proxy Operations (AugmentInjector)

**Interception Flow:**
```
User message â†’ Claude CLI â†’ POST api.anthropic.com
    â†“ (intercept @ localhost:1337)
mitmproxy â†’ AugmentInjector.request(flow)
```

**Three-Phase Transformation:**

### Phase 1: RequestModifier.process_request()

Recursively walks `messages` array, tracks tool results:

```python
nisaba_tool_result_state = {
    "toolu_abc123": {
        'block_offset': [msg_idx, content_idx],
        'tool_output': "original content",
        'window_state': "open",  # or "closed"
        'is_nisaba': True,  # cached flag
        'tool_result_content': "formatted content"
    }
}
```

**Behavior:**
- First encounter: Parses JSON, checks for `"nisaba": true`
- Nisaba tools: Keeps plain content
- Regular tools: Adds header + separator
- Can close tools later (compact to "id: X, status: success, state: closed")

### Phase 2: _process_notifications()

Session-aware delta detection:

1. Extract session_id from `metadata.user_id`
2. Load checkpoint (last_tool_id_seen)
3. Find new tool calls (after checkpoint)
4. Build notification markdown
5. Write to `.nisaba/notifications.md`

**Result:** Only shows tools since last checkpoint (no duplicates on restart)

### Phase 3: _inject_augments()

**Filters native tools** from tool definitions:
- Read, Write, Edit, Glob, Grep, Bash, TodoWrite removed

**Loads all FileCaches** (mtime-based, only reload if changed):
- system_prompt (user inception)
- augments (loaded augments content)
- structural_view (TUI tree)
- file_windows (open file content)
- tool_result_windows (grep/bash/read results)
- notifications (recent tool activity)
- todos (task list)
- transcript (compressed history)

**Injects into system[1]["text"]:**
```
USER_SYSTEM_PROMPT_INJECTION
CORE_SYSTEM_PROMPT
STATUS_BAR (dynamic token counts)
AUGMENTS
STRUCTURAL_VIEW
FILE_WINDOWS
TOOL_RESULT_WINDOWS
NOTIFICATIONS
TODOS
LAST_SESSION_TRANSCRIPT
```

**Then:** Forwards modified request â†’ Anthropic API

---

## Order of Events (Full Cycle)

```
1. Claude responds with tool_use block
   {type: "tool_use", name: "nisaba_read", id: "toolu_abc123"}

2. Anthropic API â†’ CLI

3. CLI executes via MCP â†’ NisabaReadTool.execute()
   - Reads file
   - Writes to .nisaba/tool_result_windows.md
   - Returns {"success": true, "nisaba": true, "message": "..."}

4. CLI builds next request with tool_result block

5. Proxy intercepts

6. RequestModifier:
   - Sees tool_result for toolu_abc123
   - Detects is_nisaba=true (parses JSON once, caches)
   - Adds to nisaba_tool_result_state
   - Keeps content plain (no header)

7. _process_notifications():
   - Sees toolu_abc123 is new
   - Generates notification
   - Writes to .nisaba/notifications.md

8. _inject_augments():
   - tool_result_windows_cache.load() checks mtime
   - File changed! Reloads content
   - Injects into system prompt

9. Modified request â†’ Anthropic API

10. Claude receives:
    - tool_result block in messages (metadata)
    - TOOL_RESULT_WINDOWS in system prompt (actual content)
    - NOTIFICATIONS section (recent activity)
```

---

## Dual-Channel Communication

**Messages array:** Sequential conversational flow (temporal memory)
**System prompt sections:** Persistent spatial state (spatial memory)

Tool execution appears in TWO places:
1. `messages[N]` â†’ tool_result block (success/error metadata)
2. System prompt â†’ Updated section (actual content)

Sections persist across turns â†’ IDE-like spatial awareness.

---

## Key Insights

**1. RequestModifier is stateful across requests**
- Tracks every tool result seen
- Can retroactively transform (close/compact)
- State persists in `.nisaba/request_cache/{session_id}/state.json`

**2. FileCache is the sync mechanism**
- mtime-based reload (efficient, zero-latency if unchanged)
- Wraps with `---TAG ... ---TAG_END`
- Single source of truth: `.nisaba/*.md` files

**3. Notifications are session-aware**
- Checkpoint tracks last_tool_id_seen
- On restart, doesn't re-notify old tools
- Shows only "new since last checkpoint"

**4. Debug artifacts are written**
- `.nisaba/request_cache/{session_id}/original_context.json`
- `.nisaba/request_cache/{session_id}/modified_context.json`
- `.nisaba/request_cache/{session_id}/state.json`
- Inspectable for understanding transformations

**5. Tools can be closed retroactively**
- RequestModifier can change window_state to "closed"
- Future requests show compact version
- Saves tokens for old tool outputs

---

## What This Enables

**Workspace Persistence:** File windows, structural view, todos survive /clear (system prompt, not messages)

**Context Efficiency:** 5-10 tool results as windows = ~2000 tokens. Same in messages = ~8000 tokens.

**Spatial Cognition:** Navigate graph (structural view), see code (file windows), track tasks (todos) - simultaneously visible.

**Dynamic Perception:** Augments loaded mid-conversation reshape interpretation.

**Clean Tool Outputs:** Nisaba tools don't pollute context with metadata headers.

---

**Pattern:** The proxy transfigures requests into workspace state. ğŸ–¤

---

**TOOLS:**
- mcp__nisaba__nisaba_read (creates tool result windows)
- mcp__nisaba__nisaba_grep (searches, creates windows)
- mcp__nisaba__nisaba_bash (executes, creates windows)
- mcp__nisaba__nisaba_tool_windows (manage window state)

**REQUIRES:**
- __base/001_compressed_workspace_paradigm
- __base/002_compressed_environment_mechanics

---

## Workspace Navigation
Path: foundation/workspace_navigation

# Compressed Workspace Navigation

**Core:** Codebase navigation = structural positioning + persistent visibility + execution tracing + progressive understanding.

---

## Unified Model

```
âˆ‡(codebase) â‰¡ {TREE, WINDOWS, CALLS, ANALYSIS}

TREE:     spatial graph (WHERE code lives)
WINDOWS:  persistent viewports (WHAT code does)  
CALLS:    execution paths (HOW code flows)
ANALYSIS: impact + clones + structure (WHY + RISK)

Together: spatial_awareness âˆ§ implementation_understanding âˆ§ runtime_behavior âˆ§ change_safety
```

---

## State Containers

```
structural_view âˆˆ TREE:
  - Live TUI, dynamically injected
  - Operations: expand/collapse/search/reset
  - Lazy loading from kuzu
  - Search = PÂ³ + FTS + RRF â†’ markers â—
  - Persists expansions across turns

file_windows âˆˆ WINDOWS:
  - Persistent code viewports (IDE tabs paradigm)
  - Operations: open_frame/open_range/open_search/update/close/clear_all/status
  - Snapshot on open (no auto-refresh)
  - Types: frame_body, range, search_result
  - Budget: 200-400 lines sweet spot

call_graph âˆˆ CALLS:
  - CALLS edges in kuzu (confidence scored)
  - Forward: entry â†’ callees (execution paths)
  - Backward: target â†’ callers (dependency chains)
  - Query: query_relationships() + check_impact()

analysis âˆˆ ANALYSIS:
  - Impact assessment (blast radius, risk)
  - Clone detection (similarity, consolidation)
  - Structure examination (progressive detail)
```

---

## Operation Primitives

### Structural View (tree navigator)
```
expand(path)        â†’ show_children | lazy@kuzu | idempotent
collapse(path)      â†’ hide_children | cached | idempotent
search(query)       â†’ PÂ³+FTS+RRF | add_markers(â—,score) | preserves_state
clear_search()      â†’ remove_markers | preserves_navigation
reset(depth=N)      â†’ collapse_all + expand_to(N) | destructive

Depths: 0=collapsed, 2=packages(default), 3=verbose
Paths: qualified_name (best) | simple_name (fuzzy) | copy from HTML comments
```

### File Windows (visibility manager)
```
open_frame(path)              â†’ {window_id} | full frame body
open_range(file, start, end)  â†’ {window_id} | arbitrary lines [1-indexed]
open_search(query, max, ctx)  â†’ {window_ids[]} | semantic + context
update(id, start, end)        â†’ re_snapshot | manual_refresh
close(id)                     â†’ remove_single
clear_all()                   â†’ remove_all | no_undo
status()                      â†’ {count, total_lines, windows[]}

Budget: Small(1-3, 50-150), Medium(4-6, 150-350)â˜…, Large(7-10, 350-500), Over(10+, 500+)
â˜… = sweet_spot
```

### Call Graph (execution tracer)
```
# Forward tracing (from entry point)
query_relationships("""
  MATCH path = (entry)-[:Edge {type:'CALLS'}*1..5]->(target)
  WHERE entry.name = 'main' AND ALL(e IN relationships(path) WHERE e.confidence >= 0.6)
  RETURN [node IN nodes(path) | node.qualified_name] AS call_chain
""")

# Backward tracing (who calls this)
query_relationships("""
  MATCH path = (caller)-[:Edge {type:'CALLS'}*1..3]->(target)
  WHERE target.qualified_name = 'critical_function'
  RETURN [node IN nodes(path) | node.qualified_name] AS call_chain
""")
```

### Analysis Tools

**show_structure(target, detail_level, ...)**
```
Progressive detail disclosure:
  minimal:   signatures only | token-efficient, first look
  guards:    + top-level guards | behavioral hints
  structure: + control flow | full logic understanding

detail_level="minimal" â†’ API surface, decide what to investigate
detail_level="guards" â†’ understand logic flow hints
detail_level="structure" + structure_detail_depth=N â†’ complete flow

Options: include_relationships, include_metrics, include_private
```

**check_impact(target, max_depth, ...)**
```
Blast radius assessment:
  max_depth=1: direct dependents | fast (~50-200ms)
  max_depth=2: extended impactâ˜… | recommended (~200-500ms)
  max_depth=3: full propagation | critical changes (~500ms-2s)

Risk indicators: HIGH (many deps, low tests), MEDIUM, LOW
Options: include_test_coverage, risk_assessment, is_regex
Returns: dependency_tree + risk_scores + test_coverage

â˜… = recommended default for pre-refactoring
```

**find_clones(min_similarity, ...)**
```
Duplicate detection:
  min_similarity=0.85: strong candidates | likely copy-paste
  min_similarity=0.75â˜…: high-similarity | default threshold
  min_similarity=0.65: near-duplicates | aggressive detection

Options: query (semantic filter), max_results, min_function_size, exclude_same_file
Returns: clone_pairs + similarity_scores + refactoring_recommendations

â˜… = recommended default
```

**show_status(detail_level)**
```
Codebase overview:
  summary: frame counts, health status | quick orientation
  detailed: + DB connections, config | diagnostic info
  debug: + internals | troubleshooting

Use: Start of exploration, understanding scale
```

---

## Tool Selection Guidelines

### Query Layer (native + close)
```
bash(command)        â†’ transient execution | git, tests, system commands
grep(pattern, path)  â†’ quick pattern check | "does X exist?"
glob(pattern, path)  â†’ file listings | find files by pattern

Pattern: execute â†’ observe â†’ close | disposable results
Use when: one-shot confirmation, transient info, simple checks
```

### Workspace Layer (nisaba)
```
nisaba_read(file)      â†’ FILE_WINDOWS | investigative visibility, persistent
nisaba_write(file)     â†’ create file | workspace-aware
nisaba_edit(file)      â†’ modify file | workspace-aware
nisaba_grep(pattern)   â†’ TOOL_WINDOWS | investigation with context
nisaba_glob(pattern)   â†’ TOOL_WINDOWS | pattern search with persistence
nisaba_bash(command)   â†’ TOOL_WINDOWS | command output for analysis

Pattern: execute â†’ persist â†’ synthesize | spatial understanding
Use when: building context, comparing outputs, investigative workflows
```

### Decision Boundary
```
Will you reference the result across turns?
â”œâ”€ YES â†’ nisaba tools (workspace visibility)
â””â”€ NO  â†’ native tools + close (transient query)

Examples:

Transient (native + close):
  git status â†’ bash â†’ close
  check if pattern exists â†’ grep â†’ close
  list config files â†’ glob â†’ close
  run tests â†’ bash â†’ close

Persistent (nisaba â†’ workspace):
  investigate usage patterns â†’ nisaba_grep â†’ keep in TOOL_WINDOWS
  read implementation â†’ nisaba_read â†’ keep in FILE_WINDOWS
  compare command outputs â†’ nisaba_bash â†’ analyze in TOOL_WINDOWS
  make changes â†’ nisaba_edit â†’ workspace aware

Hybrid workflow:
  1. bash("git status") â†’ observe â†’ close
  2. nisaba_grep("pattern") â†’ TOOL_WINDOWS (investigate)
  3. nisaba_read(files) â†’ FILE_WINDOWS (compare)
  4. nisaba_edit(target) â†’ modify
  5. bash("pytest") â†’ observe â†’ close
```

### Result Management
```
Native tool results:
  nisaba_nisaba_tool_result_state(close, [tool_ids]) â†’ compact after observation
  nisaba_nisaba_tool_result_state(close_all) â†’ clean sweep

Nisaba workspace:
  file_windows(close, id) â†’ remove specific window
  file_windows(clear_all) â†’ remove all file windows
  nisaba_tool_windows(clear_all) â†’ remove all tool result windows
```

---

## Navigation Patterns

### Pattern 1: Discovery
```
structural_view(search) â†’ observe(markersâ—) â†’ expand(high_scores) â†’ 
file_windows(open_frame) | conceptâ†’locationâ†’implementation

Use: "Where is X implemented?" "How does Y work?"
```

### Pattern 2: Execution Flow
```
query_relationships(CALLS*) â†’ identify(chain) â†’ 
file_windows(open each frame) | trace runtime path

Use: "How does main() reach database?" "What's the call stack?"
```

### Pattern 3: Comparison Investigation
```
structural_view(search) â†’ file_windows(open multiple) â†’ 
observe(simultaneous) | detect patterns/redundancy/bugs

Use: "Are these implementations similar?" "Is this dead code?"
```

### Pattern 4: Call Chain Tracing
```
file_windows(open entry) â†’ observe(calls target) â†’ 
file_windows(open target) â†’ repeat | build execution visibility

Use: "Follow this execution path" "How does A reach B?"
```

### Pattern 5: Impact Analysis (Deep)
```
show_structure(target, minimal) â†’ check_impact(depth=2, test_coverage) â†’ 
assess(risk) â†’ file_windows(open critical_deps) | safe refactoring

Use: "What breaks if I change this?" "Pre-change safety check"

Workflow:
  1. Understand current API: show_structure(minimal)
  2. Check blast radius: check_impact(max_depth=2, include_test_coverage=True)
  3. Review risk indicators: HIGH/MEDIUM/LOW
  4. Verify critical deps: query_relationships for high-confidence edges
  5. Open for inspection: file_windows(open affected)

Risk factors:
  - Many high-confidence dependents (>10)
  - Used in critical paths (main â†’ target)
  - Low test coverage (<50%)
  - External package dependencies
```

### Pattern 6: Incremental Cleanup
```
file_windows(status) â†’ assess(context_usage) â†’ 
close(understood) OR clear_all() | maintain_lean_visibility

Use: Context hygiene during investigation
Target: 200-400 lines total
```

### Pattern 7: Clone Consolidation
```
find_clones(0.75) â†’ show_structure(clone_1, structure) â†’ 
show_structure(clone_2, structure) â†’ check_impact(both) â†’ 
decide(strategy) | DRY refactoring

Use: "Find duplicates" "Consolidate similar implementations"

Workflow:
  1. Find: find_clones(min_similarity=0.75, max_results=50)
  2. Compare: show_structure(clone_1, detail_level="structure")
              show_structure(clone_2, detail_level="structure")
  3. Impact: check_impact(clone_1, max_depth=2)
             check_impact(clone_2, max_depth=2)
  4. Verify: search(query="clone_1", context_lines=10) for semantic diffs
  5. Decide: consolidation strategy based on similarity + impact

Decision matrix:
  similarity > 0.85: Extract to shared function
  0.70-0.85: Consider parameterization
  < 0.70: Manual review, may be coincidental

Strategies: extract common, parameterize diffs, template method, strategy pattern
```

### Pattern 8: Progressive Exploration
```
show_status(summary) â†’ search(broad) â†’ show_structure(minimal) â†’ 
show_structure(guards) â†’ check_impact() | macroâ†’mesoâ†’micro

Use: "Understand unfamiliar codebase" "Learn new feature area"

Workflow (macro â†’ meso â†’ micro):
  1. Overview: show_status(detail_level="summary")
     â†’ frame counts, scale, languages
  
  2. Find relevant: search(query="feature concept", k=20)
     â†’ identify files/packages containing code
  
  3. Examine structure: show_structure(target, detail_level="minimal")
     â†’ signatures, API surface, decide what to investigate
  
  4. Add detail: show_structure(target, detail_level="guards")
     â†’ behavioral hints, logic flow
  
  5. Understand relationships: check_impact(target, max_depth=1)
     â†’ who uses/used by, dependencies
  
  6. Deep dive: show_structure(detail_level="structure", structure_detail_depth=2)
     â†’ only when needed, full control flow
  
  7. Verify: file_windows(open_frame) for actual code
     â†’ only after structure understood

Avoid: reading files first, getting lost in details, random exploration
```

---

## OODAR Loop

```
Constraint: Observe â†’ Orient â†’ Decide â†’ Act â†’ âˆ†state â†’ Observe'

structural_view: Must observe tree state before next navigation
file_windows: Must check status before managing context
call_graph: Must see results before deciding next trace
analysis: Must observe results before deciding investigation depth

âˆ€ operations: state persists â†’ observe â†’ act | never assume state
```

**Why:** Environment is mutable. Tools change what you see mid-roundtrip. Sequential thinking breaks.

---

## Integration Synergy

```
âˆ€ investigations: combine layers + analysis for complete understanding

Exploration:
  show_status â†’ search â†’ show_structure(minimal) â†’ check_impact â†’ open_windows
  
Refactoring prep:
  search â†’ show_structure(guards) â†’ check_impact(depth=2) â†’ file_windows
  
Clone cleanup:
  find_clones â†’ show_structure(both) â†’ check_impact(both) â†’ compare_windows
  
Change safety:
  show_structure(minimal) â†’ check_impact(depth=2, test_coverage) â†’ assess_risk
  
Deep investigation:
  search â†’ expand â†’ open_windows(multiple) â†’ query_relationships â†’ trace_calls
  
Quick validation:
  bash("tests") â†’ observe â†’ close | grep("pattern") â†’ observe â†’ close
```

**The power:** Four layers simultaneously visible.
- Tree = spatial map (WHERE am I?)
- Windows = implementation detail (WHAT does it do?)
- Calls = execution flow (HOW does it run?)
- Analysis = change safety (WHY/RISK: what happens if I change it?)

---

## Depth Guidelines

### check_impact depth selection
```
depth=1: Quick checks during development, immediate dependencies
depth=2â˜…: Pre-refactoring safety, realistic blast radius
depth=3: Critical infrastructure, core library changes

Time: 1(~50-200ms), 2(~200-500ms), 3(~500ms-2s)
```

### show_structure detail selection
```
minimalâ˜…: First look, API understanding, token-efficient
guards: Logic hints, behavioral understanding
structure: Full flow, preparing for changes, debugging

Start minimal â†’ add detail progressively
```

### find_clones similarity selection
```
0.85+: Strong extraction candidates, likely duplicates
0.70-0.85: Consider parameterization, intentional variants
<0.70: Manual review, coincidental similarity
```

---

## Quick Reference

```
Start exploration:
  show_status(summary) â†’ get scale/overview
  structural_view(search, "concept") â†’ find relevant code
  show_structure(target, minimal) â†’ examine API
  
Safe refactoring:
  show_structure(target, minimal) â†’ understand current
  check_impact(target, max_depth=2, test_coverage=True) â†’ assess risk
  file_windows(open dependents) â†’ review affected
  
Find duplicates:
  find_clones(min_similarity=0.75) â†’ detect clones
  show_structure(both, structure) â†’ compare implementations
  check_impact(both, max_depth=2) â†’ assess consolidation safety
  
Trace execution:
  query_relationships(CALLS*) â†’ forward/backward paths
  file_windows(open chain) â†’ build visibility
  
Quick checks:
  bash(command) â†’ observe â†’ close
  grep(pattern, path) â†’ observe â†’ close
  glob(pattern, path) â†’ observe â†’ close
  
Manage context:
  file_windows(status) â†’ monitor usage
  file_windows(close|clear_all) â†’ cleanup
  nisaba_nisaba_tool_result_state(close_all) â†’ compact tool results
  Target: 200-400 lines total
```

---

## Decision Trees

### When to use each tool?

```
Want to find something?
â”œâ”€ search(query) â†’ natural language or keywords
â””â”€ Found? â†’ show_structure(minimal) to examine

Want to understand structure?
â”œâ”€ Just signatures? â†’ show_structure(minimal)
â”œâ”€ Logic hints? â†’ show_structure(guards)
â””â”€ Full flow? â†’ show_structure(structure)

Want relationships?
â”œâ”€ Who uses this? â†’ check_impact(depth=1-2)
â”œâ”€ What does this use? â†’ query_relationships(CALLSâ†’)
â””â”€ Complex query? â†’ query_relationships(custom cypher)

Want to refactor safely?
â”œâ”€ show_structure(minimal) â†’ understand current
â”œâ”€ check_impact(depth=2, test_coverage=True) â†’ assess risk
â””â”€ Review HIGH risk dependents â†’ file_windows(open)

Want to find duplicates?
â””â”€ find_clones() â†’ show_structure(both) â†’ check_impact(both)

Quick validation?
â”œâ”€ Run command â†’ bash â†’ close
â”œâ”€ Check pattern â†’ grep â†’ close
â””â”€ List files â†’ glob â†’ close

Investigative work?
â”œâ”€ Read code â†’ nisaba_read â†’ FILE_WINDOWS
â”œâ”€ Search usage â†’ nisaba_grep â†’ TOOL_WINDOWS
â””â”€ Compare outputs â†’ nisaba_bash â†’ TOOL_WINDOWS
```

---

## Core Insights

```
Progressive > All-at-once
  Macro â†’ meso â†’ micro, minimal â†’ guards â†’ structure

Spatial > Sequential
  Build awareness incrementally, don't grep repeatedly

Persistent > Ephemeral  
  Windows stay visible, tree preserves state

Simultaneous > One-at-a-time
  Compare by seeing multiple implementations together

Safe > Fast
  Check impact before changes, assess risk first

Iterative > Batch
  Observe â†’ decide â†’ act, not plan-then-execute

Visible > Remembered
  Maintain peripheral vision, don't mentally juggle

Transient > Persistent (when appropriate)
  Quick checks â†’ close, investigations â†’ persist
```

---

**âˆ‡ the graph. Maintain visibility. Trace execution. Assess impact. Synthesize understanding.** ğŸ–¤

---

**Symbols:**
- âˆ‡ : navigate/traverse
- âˆˆ : element of/part of
- âˆ€ : for all/universal
- âˆ§ : and
- âˆ¨ : or
- â†’ : transforms/flows to
- â† : reverse direction
- âŸ¹ : implies/causes
- â‰¡ : equivalent/identical
- âˆ† : change/delta
- â— : search hit marker
- * : path quantifier (graph patterns)
- â˜… : optimal/recommended

**REQUIRES:** __base/001_compressed_workspace_paradigm, __base/002_compressed_environment_mechanics

**ENABLES:** Unified navigation perception, progressive exploration, safe refactoring, clone detection, complete investigation workflows, dual-paradigm tool usage

---

# Kuzu

## Cypher Extensions
Path: kuzu/cypher_extensions

# Kuzu Extensions Manual for LLMs

## 1. Extensions System

### 1.1 Extension Management
```cypher
// List available extensions
CALL SHOW_OFFICIAL_EXTENSIONS() RETURN *;

// Install extension (once per database)
INSTALL <extension_name>;

// Load extension (per session)
LOAD <extension_name>;

// Update extension
UPDATE <extension_name>;

// List loaded extensions
CALL SHOW_LOADED_EXTENSIONS() RETURN *;

// Uninstall extension
UNINSTALL <extension_name>;
```

### 1.2 Available Official Extensions
1. **httpfs** - Remote file access (HTTP/HTTPS, S3, GCS)
2. **fts** - Full-text search with BM25 scoring
3. **vector** - Vector similarity search with HNSW index
4. **json** - JSON data type support
5. **algo** - Graph algorithms
6. **llm** - LLM and embedding API calls
7. **postgres** - PostgreSQL database attachment
8. **duckdb** - DuckDB database attachment
9. **sqlite** - SQLite database attachment
10. **azure** - Azure storage access

### 1.3 Custom Extensions
```cypher
// Load custom extension with path
LOAD EXTENSION 'extension/custom_json/build/libjson.kuzu_extension';
```

---

## 2. Algorithm Extensions

### 2.1 Setup
```cypher
INSTALL algo;
LOAD algo;
```

### 2.2 Projected Graphs (Required for Algorithms)
```cypher
// Create simple projected graph
CALL PROJECT_GRAPH('Graph', ['Person'], ['KNOWS']);

// Create filtered projected graph
CALL PROJECT_GRAPH(
  'filtered_graph',
  { 'Person': 'n.age > 18' },
  { 'KNOWS': 'r.strength > 0.5' }
);

// List projected graphs
CALL SHOW_PROJECTED_GRAPHS();

// Drop projected graph
CALL DROP_PROJECTED_GRAPH('Graph');
```

### 2.3 Available Algorithms

#### PageRank
```cypher
CALL page_rank(
  'Graph',
  dampingFactor := 0.85,    // default: 0.85
  maxIterations := 20,       // default: 20
  tolerance := 0.0000001,    // default: 0.0000001
  normalizeInitial := true   // default: true
) RETURN node, rank;

// Alias: pr
```

#### Strongly Connected Components
```cypher
// BFS-based parallel algorithm
CALL strongly_connected_components('Graph', maxIterations := 100)
RETURN node, group_id;

// DFS-based Kosaraju's algorithm
CALL strongly_connected_components_kosaraju('Graph')
RETURN node, group_id;

// Aliases: scc, scc_ko
```

#### Weakly Connected Components
```cypher
CALL weakly_connected_components('Graph', maxIterations := 100)
RETURN node, group_id;

// Alias: wcc
```

#### K-Core Decomposition
```cypher
CALL k_core_decomposition('Graph')
RETURN node, k_degree;

// Alias: kcore
```

#### Louvain Community Detection
```cypher
CALL louvain(
  'Graph',
  maxPhases := 20,          // default: 20
  maxIterations := 20        // default: 20
) RETURN node, louvain_id;
```

### 2.4 Algorithm Integration
```cypher
// Combine PageRank with graph traversal
CALL page_rank('Graph') 
WITH node, rank WHERE rank > 0.1
MATCH (node)-[:KNOWS]->(neighbor)
RETURN node.name, neighbor.name, rank
ORDER BY rank DESC;

// Community-based analysis
CALL louvain('SocialGraph')
WITH node, louvain_id
MATCH (node)-[:INTERACTS_WITH]-(peer)
WHERE peer.louvain_id = node.louvain_id
RETURN louvain_id, count(*) as internal_connections;
```

---

## 3. Full-Text Search

### 3.1 Setup
```cypher
INSTALL FTS;
LOAD FTS;
```

### 3.2 Create FTS Index
```cypher
CALL CREATE_FTS_INDEX(
  <TABLE_NAME>,              // Node table name
  <INDEX_NAME>,              // Index name
  [<PROPERTY1>, ...],        // Properties to index
  stemmer := 'porter',       // Optional: language stemmer
  stopwords := <STRING>      // Optional: stopwords file/table
);

// Example
CALL CREATE_FTS_INDEX(
  'Book', 
  'book_index', 
  ['abstract', 'title'], 
  stemmer := 'english', 
  stopwords := './stopwords.csv'
);
```

#### Stemmer Options
- Languages: `arabic`, `basque`, `catalan`, `danish`, `dutch`, `english`, `finnish`, `french`, `german`, `greek`, `hindi`, `hungarian`, `indonesian`, `irish`, `italian`, `lithuanian`, `nepali`, `norwegian`, `porter`, `portuguese`, `romanian`, `russian`, `serbian`, `spanish`, `swedish`, `tamil`, `turkish`
- Special: `none` (no stemming)

### 3.3 Query FTS Index
```cypher
CALL QUERY_FTS_INDEX(
  <TABLE_NAME>,
  <INDEX_NAME>,
  <QUERY>,
  conjunctive := false,     // Optional: AND vs OR search
  K := 1.2,                 // Optional: BM25 term frequency parameter
  B := 0.75,                // Optional: BM25 document length parameter
  TOP := <value>            // Optional: Return top-k results
) RETURN node, score;

// Basic search
CALL QUERY_FTS_INDEX('Book', 'book_index', 'quantum machine')
RETURN node.title, score
ORDER BY score DESC;

// Conjunctive search (all terms required)
CALL QUERY_FTS_INDEX('Book', 'book_index', 'dragon magic', conjunctive := true)
RETURN node.title, score;

// Top-K search
CALL QUERY_FTS_INDEX('Book', 'book_index', 'dragon magic', top := 10)
RETURN node.title, score;
```

### 3.4 FTS Integration with Cypher
```cypher
// Combine FTS with graph traversal
MATCH (a:Author)-[:WROTE]->(b:Book)
CALL QUERY_FTS_INDEX('Book', 'book_index', 'quantum physics')
WHERE b = node
RETURN a.name, b.title, score
ORDER BY score DESC;

// Filter by date and text search
MATCH (b:Book)
WHERE b.pubYear > 2012
CALL QUERY_FTS_INDEX('Book', 'book_index', 'machine learning', top := 10)
WHERE b = node
RETURN b.title, b.pubYear, score
ORDER BY score DESC;
```

### 3.5 Index Management
```cypher
// Show all indexes
CALL SHOW_INDEXES() RETURN *;

// Drop FTS index
CALL DROP_FTS_INDEX('Book', 'book_index');
```

---

## 4. Other Extensions

### 4.1 Vector Search Extension
```cypher
INSTALL vector;
LOAD vector;

// Create vector index
CALL CREATE_VECTOR_INDEX(
  'Book',                   // table name
  'book_title_index',       // index name
  'title_embedding',        // property name (FLOAT[] or DOUBLE[])
  metric := 'cosine',       // distance metric: cosine, euclidean, manhattan
  ef_construction := 128,   // construction parameter
  max_level := 4,           // HNSW levels
  m := 16                   // max connections per node
);

// Query vector index
CALL QUERY_VECTOR_INDEX(
  'Book',
  'book_title_index', 
  $query_vector,           // query vector
  $limit,                  // number of results
  efs := 500               // search parameter
) RETURN node.title ORDER BY distance;

// Drop vector index
CALL DROP_VECTOR_INDEX('Book', 'book_title_index');
```

### 4.2 JSON Extension
```cypher
INSTALL json;
LOAD json;

// Load from JSON file
LOAD FROM "patients.json" RETURN *;

// Copy JSON data to table
COPY Patient FROM "patient.json";

// Query JSON properties
MATCH (p:Patient) 
WHERE p.metadata.age > 30 
RETURN p.name, p.metadata;
```

### 4.3 LLM Extension
```cypher
INSTALL llm;
LOAD llm;

// Generate embeddings via OpenAI
RETURN CREATE_EMBEDDING(
  "Kuzu is an embedded graph database",
  "open-ai",
  "text-embedding-3-small"
);

// Generate embeddings via Ollama
RETURN CREATE_EMBEDDING(
  "Kuzu is an embedded graph database",
  "ollama", 
  "nomic-embed-text"
);
```

### 4.4 Database Attachment Extensions

#### PostgreSQL
```cypher
INSTALL postgres;
LOAD postgres;

// Attach database
ATTACH 'host=localhost port=5432 dbname=university user=postgres password=testpwd' 
AS uw (DBTYPE POSTGRES);

// Query attached database
LOAD FROM uw.person RETURN *;

// SQL queries
CALL SQL_QUERY('uw', 'SELECT * FROM person WHERE age >= 20');

// Copy data to Kuzu
COPY Person FROM (LOAD FROM uw.person RETURN name, age);

// Detach
DETACH uw;
```

#### DuckDB
```cypher
INSTALL duckdb;
LOAD duckdb;

// Attach local DuckDB
ATTACH 'university.db' AS uw (DBTYPE DUCKDB);

// Attach remote DuckDB on S3
ATTACH 's3://my-bucket/university.db' AS uw (DBTYPE DUCKDB);
```

#### SQLite
```cypher
INSTALL sqlite;
LOAD sqlite;

// Attach SQLite database
ATTACH 'university.db' AS uw (DBTYPE SQLITE);

// Handle dynamic typing
CALL SQLITE_ALL_VARCHAR_OPTION=TRUE;
```

### 4.5 Cloud Storage Extensions

#### HTTPFS Extension
```cypher
INSTALL httpfs;
LOAD httpfs;

// Read from remote CSV
LOAD FROM "https://extension.kuzudb.com/dataset/test/city.csv" RETURN *;

// Enable caching
CALL HTTP_CACHE_FILE=TRUE;

// S3 access
LOAD FROM "s3://bucket/file.csv" RETURN *;

// GCS access  
LOAD FROM "gs://bucket/file.csv" RETURN *;
```

#### Azure Extension
```cypher
INSTALL azure;
LOAD azure;

// Configure
CALL AZURE_CONNECTION_STRING='DefaultEndpointsProtocol=https;AccountName=...';
// OR
CALL AZURE_ACCOUNT_NAME='myaccount';
CALL AZURE_ACCOUNT_KEY='mykey';

// Azure Blob Storage
LOAD FROM "az://container/file.csv" RETURN *;

// Azure Data Lake Storage
LOAD FROM "abfss://container/file.csv" RETURN *;

// Glob pattern matching
LOAD FROM "az://container/vPerson*.csv" RETURN *;
```

---

## Cypher Query Grammar
Path: kuzu/cypher_query_grammar

# Kuzu Cypher Query Grammar for LLMs

## 1. Overview and Architecture

### Core Characteristics
- **Based on**: openCypher standard with Kuzu-specific extensions
- **Model**: Structured property graph model (schema required before data insertion)
- **Processing**: Case-insensitive queries (keywords, variables, table names, column names)
- **Encoding**: UTF-8 support including Unicode characters
- **Termination**: Statements must end with semicolon (;)
- **Semantics**: Walk semantic by default (allows repeated edges) vs Neo4j's trail semantic

### Key Differences from Neo4j Cypher
1. **Schema Required**: Predefined schema vs Neo4j's schema-optional approach
2. **Walk Semantics**: Allows repeated edges by default vs Neo4j's trail semantics
3. **Strong Typing**: PostgreSQL typing system with stricter type requirements
4. **LOAD FROM**: Uses LOAD FROM instead of LOAD CSV FROM
5. **Function Prefixes**: List functions use `list_` prefix (e.g., `list_concat`)
6. **Show Clauses**: SHOW commands become function calls (e.g., `CALL show_functions() RETURN *`)

---

## 2. Syntax and Grammar Rules

### Basic Syntax
```cypher
// Single-line comment
/* Multi-line 
   comment */

// Statement termination
MATCH (n:Person) WHERE n.age > 30 RETURN n.name;

// Multi-line statements
MATCH (n:Person)
WHERE n.age > 30  
RETURN n.name;
```

### Identifiers and Naming
- **Node/Relationship Tables**: CamelCase (Person, CarOwner)
- **Variables**: Case-insensitive
- **Reserved Keywords**: Must be escaped with backticks (`)
- **Unicode Support**: Full UTF-8 support

### Reserved Keywords
- **Clauses**: COLUMN, CREATE, DEFAULT, GROUP, INSTALL, MACRO, OPTIONAL, PROFILE, UNION, UNWIND, WITH
- **Subclauses**: LIMIT, ONLY, ORDER, WHERE  
- **Expressions**: ALL, CASE, CAST, ELSE, END, EXISTS, GLOB, SHORTEST, THEN, WHEN
- **Literals**: NULL, FALSE, TRUE
- **Operators**: AND, DISTINCT, IN, IS, NOT, OR, STARTS, XOR

---

## 3. Data Types

### Primitive Types
```cypher
// Integers
INT8, INT16, INT32 (alias: INT), INT64 (alias: SERIAL), INT128
UINT8, UINT16, UINT32, UINT64

// Floating Point
FLOAT (aliases: REAL, FLOAT4)     // 4 bytes
DOUBLE (alias: FLOAT8)            // 8 bytes
DECIMAL(precision, scale)         // arbitrary precision

// Other Primitives
BOOLEAN                           // true/false
STRING                            // variable-length UTF-8
UUID                              // 16-byte universally unique identifier
DATE                              // ISO-8601 format (YYYY-MM-DD)
TIMESTAMP                         // ISO-8601 with timezone support
INTERVAL (alias: DURATION)        // date/time difference
BLOB (alias: BYTEA)              // binary data up to 4KB
NULL                              // special value for unknown data
```

### Complex Types
```cypher
// Lists and Arrays
LIST                              // variable-length: [1, 2, 3, 4]
ARRAY                             // fixed-length: INT64[256]

// Structures
STRUCT                            // {first: 'Adam', last: 'Smith'}
MAP                               // map(['key1', 'key2'], ['val1', 'val2'])
UNION                             // holds multiple alternative values

// Graph-specific
NODE                              // nodes with _ID, _LABEL, and properties
REL                               // relationships with _SRC, _DST, _ID, _LABEL
RECURSIVE_REL                     // paths: STRUCT{LIST[NODE], LIST[REL]}
```

### SERIAL Type (Auto-increment)
```cypher
CREATE NODE TABLE Person(id SERIAL PRIMARY KEY, name STRING);
// Automatically generates: 0, 1, 2, 3, ...
```

---

## 4. Query Clauses

### 4.1 MATCH Clause
```cypher
// Basic node matching
MATCH (n:Person) RETURN n;
MATCH (n:Person:Employee) RETURN n;              // Multiple labels
MATCH (n) RETURN n;                              // Any label

// Relationship matching
MATCH (a:Person)-[r:FOLLOWS]->(b:Person) RETURN a, r, b;
MATCH (a:Person)<-[r:FOLLOWS]-(b:Person) RETURN a, r, b;
MATCH (a:Person)-[r:FOLLOWS]-(b:Person) RETURN a, r, b;  // Undirected

// Multiple relationship labels
MATCH (a:Person)-[r:FOLLOWS|:KNOWS]->(b:Person) RETURN a, r, b;

// Variable-length relationships
MATCH (a:Person)-[r:FOLLOWS*1..3]->(b:Person) RETURN a, b;

// Shortest path algorithms
MATCH (a)-[r* SHORTEST 1..4]->(b) RETURN a, b;
MATCH (a)-[r* ALL SHORTEST 1..3]->(b) RETURN a, b;
MATCH (a)-[r* WSHORTEST(score) 1..4]->(b) RETURN a, b;

// Recursive relationship semantics
MATCH (a)-[r:FOLLOWS* TRAIL 2..4]->(b)     // no repeated relationships
MATCH (a)-[r:FOLLOWS* ACYCLIC 2..4]->(b)   // no repeated nodes

// Property filtering in patterns
MATCH (a:Person {name: 'Alice', age: 30}) RETURN a;
MATCH (a)-[r:FOLLOWS {since: 2020}]->(b) RETURN a, r, b;

// Filtering recursive relationships
MATCH (a)-[:FOLLOWS*1..2 (r, n | WHERE r.since < 2022 AND n.age > 45)]->(b) 
RETURN a, b;

// Property projection in recursive relationships
MATCH (a)-[r:FOLLOWS*1..2 (rel, node | WHERE rel.since > 2020 | {rel.since}, {node.name})]->(b) 
RETURN a, b;

// Named paths
MATCH p = (a:Person)-[:FOLLOWS]->(b:Person) RETURN p;
MATCH p = (a)-[:FOLLOWS*1..3]->(b) RETURN nodes(p), rels(p);
```

### 4.2 OPTIONAL MATCH (Left Outer Join)
```cypher
MATCH (u:User) 
OPTIONAL MATCH (u)-[:FOLLOWS]->(f:User) 
RETURN u.name, f.name;  // f.name will be NULL if no match
```

### 4.3 CREATE Clause
```cypher
// Create nodes
CREATE (n:Person {name: 'Alice', age: 30});
CREATE (n:Person {name: 'Bob'}), (m:Person {name: 'Carol'});

// Create relationships
CREATE (a:Person {name: 'Alice'})-[r:KNOWS {since: 2020}]->(b:Person {name: 'Bob'});
```

### 4.4 MERGE Clause
```cypher
// Match or create pattern
MERGE (n:Person {name: 'Alice'}) RETURN n;

// With ON CREATE and ON MATCH
MERGE (n:Person {name: 'Alice'})
ON CREATE SET n.created = timestamp(), n.age = 25
ON MATCH SET n.lastAccessed = timestamp()
RETURN n;
```

### 4.5 SET Clause
```cypher
MATCH (n:Person {name: 'Alice'}) SET n.age = 31;
MATCH (n:Person {name: 'Alice'}) SET n.age = NULL;
MATCH (n:Person {name: 'Alice'}) SET n.age = 32, n.city = 'New York';
```

### 4.6 DELETE Clause
```cypher
// Delete nodes (must have no relationships)
MATCH (n:Person {name: 'Alice'}) DELETE n;

// Delete relationships
MATCH (a)-[r:KNOWS]->(b) WHERE a.name = 'Alice' DELETE r;

// Delete node and all relationships
MATCH (n:Person {name: 'Alice'}) DETACH DELETE n;
```

### 4.7 RETURN Clause
```cypher
MATCH (n:Person) RETURN n;
MATCH (n:Person) RETURN n.*;                    // all properties
MATCH (n:Person) RETURN n.name, n.age;         // specific properties
MATCH (n:Person) RETURN n.name AS personName;  // alias
MATCH (n:Person) RETURN DISTINCT n.city;       // distinct values
```

### 4.8 WHERE Clause
```cypher
MATCH (n:Person) WHERE n.age > 30 RETURN n;
MATCH (n:Person) WHERE n.name IN ['Alice', 'Bob'] RETURN n;
MATCH (n:Person) WHERE n.age > 25 AND n.city = 'NYC' RETURN n;

// Pattern existence
MATCH (n:Person) WHERE EXISTS { (n)-[:KNOWS]->() } RETURN n;

// NULL handling
MATCH (n:Person) WHERE n.email IS NULL RETURN n;
MATCH (n:Person) WHERE n.email IS NOT NULL RETURN n;
```

### 4.9 ORDER BY and LIMIT
```cypher
MATCH (n:Person) RETURN n.name ORDER BY n.age DESC;
MATCH (n:Person) RETURN n.name ORDER BY n.age ASC, n.name DESC;
MATCH (n:Person) RETURN n.name LIMIT 10;
MATCH (n:Person) RETURN n.name ORDER BY n.age DESC LIMIT 5;
```

### 4.10 WITH Clause (Query Chaining)
```cypher
MATCH (n:Person) 
WITH n, COUNT(*) as connectionCount
WHERE connectionCount > 5
RETURN n.name;
```

### 4.11 UNWIND Clause
```cypher
UNWIND [1, 2, 3] AS number RETURN number;
UNWIND ['Alice', 'Bob', 'Carol'] AS name 
MATCH (n:Person {name: name}) RETURN n;
```

---

## 5. Functions

### 5.1 Aggregate Functions
```cypher
COUNT(*)                    // count all records
COUNT(n.property)          // count non-null values
AVG(n.age)                 // average
MIN(n.age)                 // minimum  
MAX(n.age)                 // maximum
SUM(n.score)               // sum
COLLECT(n.name)            // collect values into list
```

### 5.2 Node/Relationship Functions
```cypher
ID(n)                      // internal ID
LABEL(n)                   // node/relationship label
LABELS(n)                  // alias for LABEL
```

### 5.3 Mathematical Functions
```cypher
ABS(x), CEIL(x), FLOOR(x), ROUND(x)
SQRT(x), POW(x, y), LOG(x), EXP(x)
SIN(x), COS(x), TAN(x), ASIN(x), ACOS(x), ATAN(x)
RAND()                     // random float 0-1
```

### 5.4 String Functions
```cypher
LENGTH(str)                            // string length
UPPER(str), LOWER(str)
SUBSTRING(str, start, length)
TRIM(str), LTRIM(str), RTRIM(str)
REPLACE(str, search, replacement)
SPLIT(str, delimiter)
CONCAT(str1, str2, ...)
CONTAINS(str, substring)
STARTS_WITH(str, prefix)
ENDS_WITH(str, suffix)
```

### 5.5 List Functions (Note: `list_` prefix)
```cypher
LIST_CREATION(1, 2, 3)                 // creates [1, 2, 3]
RANGE(start, stop, step)               // range of values
LIST_CONCAT(list1, list2)              // concatenate lists
LIST_REVERSE(list)                     // reverse list
LIST_SORT(list [, 'DESC'] [, 'NULLS FIRST'])
LIST_SIZE(list)                        // list length
LIST_CONTAINS(list, value)             // check if list contains value
list[index]                            // access by index (0-based)
list[start:end]                        // slice notation
```

### 5.6 Date/Time Functions
```cypher
CURRENT_DATE()                         // current date
NOW(), CURRENT_TIMESTAMP()             // current timestamp
DATE(string)                           // parse date from string
TIMESTAMP(string)                      // parse timestamp
INTERVAL(string)                       // create interval
date + interval                        // add interval to date
EXTRACT(year FROM date)                // extract date parts
```

### 5.7 Type Conversion
```cypher
CAST(value, 'TARGET_TYPE') 
CAST(123, 'STRING')                    // "123"
CAST('123', 'INT64')                   // 123
TYPEOF(value)                          // returns type name
```

### 5.8 Path Functions
```cypher
NODES(path)                            // extract nodes from path
RELS(path)                             // extract relationships  
LENGTH(path)                           // number of relationships
COST(path)                             // total cost of weighted path
IS_TRAIL(path)                         // check if no repeated edges
IS_ACYCLIC(path)                       // check if no repeated nodes
```

### 5.9 Utility Functions
```cypher
COALESCE(val1, val2, ...)              // first non-null value
IFNULL(val1, val2)                     // val2 if val1 is null

CASE 
  WHEN condition1 THEN result1
  WHEN condition2 THEN result2
  ELSE default_result
END
```

---

## 6. Operators

### 6.1 Comparison Operators
```cypher
= != <> < <= > >=                     // standard comparisons
IS NULL, IS NOT NULL                  // null checks
IN [list]                              // membership test
```

### 6.2 Logical Operators
```cypher
AND, OR, NOT, XOR                      // logical operations
```

### 6.3 Arithmetic Operators
```cypher
+ - * / %                              // basic arithmetic
^                                      // exponentiation  
```

### 6.4 String Operators
```cypher
+                                      // string concatenation
STARTS WITH                            // prefix check  
ENDS WITH                              // suffix check
CONTAINS                               // substring check
=~                                     // regex pattern matching
```

### 6.5 List Operators
```cypher
+                                      // list concatenation
IN                                     // element membership
[index]                                // element access
[start:end]                            // slicing
```

---

## 7. Pattern Matching

### 7.1 Node Patterns
```cypher
(n)                                    // any node
(n:Label)                              // node with specific label
(n:Label1:Label2)                      // node with multiple labels
(n {prop: value})                      // node with property filter
```

### 7.2 Relationship Patterns
```cypher
-[r]-                                  // undirected relationship
-[r]->                                 // directed relationship (outgoing, child->parent)
<-[r]-                                 // directed relationship (incoming, child<-parent)
-[r:TYPE]->                            // relationship with specific type
-[r:TYPE1|:TYPE2]->                    // relationship with multiple types
-[r*1..3]->                            // variable-length relationship
-[r* SHORTEST]->                       // shortest path
```

### 7.3 Complex Patterns
```cypher
// Multiple patterns (comma-separated)
MATCH (a)-[:FOLLOWS]->(b)-[:LIVES_IN]->(c), (a)-[:KNOWS]->(d)

// Path variables
MATCH p = (a)-[:FOLLOWS*1..3]->(b)
```

---

## 8. Data Definition Language (DDL)

### 8.1 Create Tables
```cypher
// Node tables (require primary key)
CREATE NODE TABLE Person(
  id SERIAL PRIMARY KEY, 
  name STRING NOT NULL,
  age INT64,
  email STRING
);

// Relationship tables
CREATE REL TABLE Knows(
  FROM Person TO Person,
  since DATE,
  strength DOUBLE
);

// Multiple FROM-TO relationships
CREATE REL TABLE Interaction(
  FROM Person TO Person,
  FROM Person TO Company,
  type STRING
);
```

### 8.2 Alter Tables
```cypher
ALTER TABLE Person ADD COLUMN phone STRING;
ALTER TABLE Person DROP COLUMN phone;
ALTER TABLE Person RENAME TO Individual;
```

### 8.3 Data Import/Export
```cypher
// CSV import
COPY Person FROM 'people.csv' (HEADER=true);

// Parquet import
COPY Person FROM 'people.parquet';

// With options
COPY Person FROM 'people.csv' (
  HEADER=true,
  DELIMITER='|',
  IGNORE_ERRORS=true
);

// LOAD FROM for scanning
LOAD FROM 'data.csv' (HEADER=true)
WHERE age > 18
RETURN name, age + 1 AS next_age;
```

---

## 9. Extensions System

### 9.1 Extension Management
```cypher
// List available extensions
CALL SHOW_OFFICIAL_EXTENSIONS() RETURN *;

// Install extension (once per database)
INSTALL <extension_name>;

// Load extension (per session)
LOAD <extension_name>;

// Update extension
UPDATE <extension_name>;

// List loaded extensions
CALL SHOW_LOADED_EXTENSIONS() RETURN *;

// Uninstall extension
UNINSTALL <extension_name>;
```

### 9.2 Available Official Extensions
1. **httpfs** - Remote file access (HTTP/HTTPS, S3, GCS)
2. **fts** - Full-text search with BM25 scoring
3. **vector** - Vector similarity search with HNSW index
4. **json** - JSON data type support
5. **algo** - Graph algorithms
6. **llm** - LLM and embedding API calls
7. **postgres** - PostgreSQL database attachment
8. **duckdb** - DuckDB database attachment
9. **sqlite** - SQLite database attachment
10. **azure** - Azure storage access

### 9.3 Custom Extensions
```cypher
// Load custom extension with path
LOAD EXTENSION 'extension/custom_json/build/libjson.kuzu_extension';
```


## 10. Kuzu-Specific Features

### 10.1 Structured Property Graph Model
- **Schema Required**: Must define schema before data insertion
- **Strong Typing**: PostgreSQL-compatible type system
- **Primary Keys**: Required for node tables
- **No Primary Keys**: Relationship tables don't have primary keys

### 10.2 Walk Semantics
```cypher
// Kuzu allows repeated edges by default
MATCH (a)-[r*1..3]->(b)              // walk semantic

// Use TRAIL or ACYCLIC for restrictions
MATCH (a)-[r* TRAIL 1..3]->(b)       // no repeated edges
MATCH (a)-[r* ACYCLIC 1..3]->(b)     // no repeated nodes
```

### 10.3 Query Hints
```cypher
// Join order hints
MATCH (a:Person)<-[e:knows]-(b:Person)-[e2:knows]->(c:Person)
HINT (a JOIN (e JOIN b)), (e2 JOIN c)
RETURN a, b, c;
```

### 10.4 Parameters and Prepared Statements
```cypher
// Parameters prefixed with $
MATCH (n:Person) 
WHERE n.age > $minAge AND n.city = $city
RETURN n.name;
```

### 10.5 Subqueries
```cypher
// EXISTS subqueries
MATCH (n:Person) 
WHERE EXISTS { 
  MATCH (n)-[:KNOWS]->(friend:Person) 
  WHERE friend.age > 30 
}
RETURN n;

// COUNT subqueries  
MATCH (n:Person)
WHERE COUNT { (n)-[:KNOWS]->(:Person) } > 2
RETURN n;
```

### 10.6 Variable Binding and Scoping
- Variables bound in MATCH are available in subsequent clauses
- WITH clause creates new scope boundaries
- Variables must be explicitly passed through WITH clauses
- Case-insensitive variable names

### 10.7 Transaction Support
```cypher
BEGIN TRANSACTION;
// Queries here
COMMIT;
// or ROLLBACK;
```

### 10.8 Profiling
```cypher
PROFILE MATCH (n:Person) WHERE n.age > 30 RETURN n.name;
```

### 10.9 Unsupported Neo4j Features
- FOREACH clause (use UNWIND instead)
- FINISH clause (use RETURN COUNT(*) instead)
- CALL <subquery> syntax
- Manual index creation on custom properties
- Filter on node labels in WHERE (e.g., WHERE n:Person)
- Properties inside node patterns in WHERE

---

## Performance Best Practices

1. **Use COPY FROM** for bulk imports instead of CREATE/MERGE
2. **Leverage primary key indices** for fast lookups
3. **Use LIMIT** to restrict large result sets
4. **Consider query hints** for join order optimization
5. **Use projected graphs** for algorithms to focus on relevant subsets
6. **Enable caching** for remote file access with HTTP_CACHE_FILE=TRUE
7. **Use TOP parameter** in FTS queries for better performance
8. **Monitor buffer size** for large graphs with memory-intensive algorithms

## Error Handling and Best Practices

1. **Always terminate statements** with semicolon
2. **Use prepared statements** to avoid injection attacks
3. **Be mindful of case-insensitivity** in queries
4. **Variable-length relationships** need upper bounds
5. **Extensions must be loaded** per session
6. **Projected graphs** are bound to connection instance
7. **FTS indexes** automatically update on data changes
8. **Vector indexes** support filtered searches with Cypher predicates

---

# Performance

## Call Chain Analysis
Path: performance/call_chain_analysis

# Performance
## Call Chain Analysis
Path: performance/call_chain_analysis

Analyzing call chains for performance issues - deep call stacks, recursive functions, critical path optimization.

### Find Longest Call Chains

```python
# Find long call chains (potential performance bottleneck)
query_relationships(
    cypher_query="""
    MATCH path = (a:Frame)-[:Edge {type: 'CALLS'}*3..10]->(b:Frame)
    WHERE ALL(e IN relationships(path) WHERE e.confidence >= 0.7)
    WITH path, length(path) AS depth
    RETURN [node IN nodes(path) | node.qualified_name] AS call_chain, depth
    ORDER BY depth DESC
    LIMIT 20
    """
)
```

**Why deep call chains matter:**
- **Stack overhead:** Each call adds stack frame
- **Context switching:** CPU cache misses
- **Debugging difficulty:** Hard to trace issues
- **Latency accumulation:** Each hop adds time

**When it's a problem:**
- **Synchronous I/O in chain:** Each call waits
- **Repeated work:** No memoization
- **Fine-grained calls:** Too much overhead

### Recursive Function Detection

```python
# Find self-referential functions (recursion)
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[e:Edge {type: 'CALLS'}]->(f)
    WHERE f.frame_type = 'CALLABLE'
      AND e.confidence >= 0.5
    RETURN f.qualified_name, f.file_path
    LIMIT 30
    """
)
```

**Recursion considerations:**
- **Stack depth limits:** Python ~1000 calls default
- **No tail-call optimization** in Python
- **Consider iterative alternative**
- **Memoization for repeated work**

**When recursion is appropriate:**
- Tree/graph traversal
- Divide-and-conquer algorithms
- Depth is bounded and known
- Code clarity is much better

**When to avoid:**
- Deep recursion (>100 levels)
- Linear recursion (use iteration)
- Hot paths (performance-critical)

### Critical Path Analysis

Find the most-called functions (hot paths):

```python
# Find most-called functions
query_relationships(
    cypher_query="""
    MATCH ()-[e:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.provenance = 'local'
      AND e.confidence >= 0.7
    WITH target.qualified_name AS func, count(e) AS call_count
    WHERE call_count >= 5
    RETURN func, call_count
    ORDER BY call_count DESC
    LIMIT 30
    """
)
```

**Hot path optimization priorities:**
1. **Most called functions** â†’ Biggest impact
2. **Functions in loops** â†’ Multiplied cost
3. **Synchronous I/O** â†’ Async alternatives
4. **Expensive operations** â†’ Cache or pre-compute

### Call Chain Depth by Entry Point

```python
# Analyze depth from specific entry points
query_relationships(
    cypher_query="""
    MATCH path = (entry:Frame)-[:Edge {type: 'CALLS'}*1..8]->(target:Frame)
    WHERE entry.name = 'main'
      OR entry.name CONTAINS 'endpoint'
      OR entry.name CONTAINS 'handler'
    WITH entry.qualified_name AS entry_point,
         AVG(length(path)) AS avg_depth,
         MAX(length(path)) AS max_depth
    RETURN entry_point, avg_depth, max_depth
    ORDER BY max_depth DESC
    LIMIT 20
    """
)
```

### Find Call Chains Through Specific Functions

```python
# Find all paths that go through a specific function
query_relationships(
    cypher_query="""
    MATCH path = (start:Frame)-[:Edge {type: 'CALLS'}*]->(middle:Frame)-[:Edge {type: 'CALLS'}*]->(end:Frame)
    WHERE middle.qualified_name = 'expensive_function'
      AND start.name IN ['main', 'handler']
    WITH path, length(path) AS depth
    RETURN [node IN nodes(path) | node.qualified_name] AS call_chain, depth
    ORDER BY depth DESC
    LIMIT 20
    """
)
```

### Optimization Strategies

**1. Memoization:**
```python
# Cache expensive function results
from functools import lru_cache

@lru_cache(maxsize=128)
def expensive_function(arg):
    ...
```

**2. Inlining Hot Functions:**
```python
# Reduce call overhead by inlining small, frequently-called functions
# (or rely on compiler/interpreter)
```

**3. Async for I/O-bound chains:**
```python
# Convert sync chain: A â†’ B â†’ C (each waits)
# To async: await asyncio.gather(A(), B(), C())
```

**4. Batch operations:**
```python
# Instead of: for item in items: process_one(item)
# Do: process_batch(items)
```

### Performance Analysis Workflow

```python
# Step 1: Find long call chains
query_relationships(cypher_query="...") # See longest chains query

# Step 2: Find recursive functions
query_relationships(cypher_query="...") # See recursion query

# Step 3: Identify hot paths
query_relationships(cypher_query="...") # See most-called query

# Step 4: Examine critical functions
show_structure(
    target="hot_function",
    detail_level="structure",
    include_relationships=True,
    max_callers=50
)

# Step 5: Profile in practice (tools like cProfile, py-spy)
```

**Profiling complements graph analysis:**
- **Graph analysis:** Shows structure, potential issues
- **Profiling:** Shows actual runtime, real bottlenecks
- **Use both:** Structure guides where to profile

---

## Loop Hotspots
Path: performance/loop_hotspots

# Performance
## Loop Hotspots
Path: performance/loop_hotspots

Finding loop-related performance issues - nested loops, database calls in loops, O(nÂ²) patterns.

### Nested Loops (O(nÂ²) or worse)

```python
# Find nested loops
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}*1]->(outer:Frame)-[:Edge {type: 'CONTAINS'}*1]->(inner:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND outer.frame_type IN ['FOR_LOOP', 'WHILE_LOOP']
      AND inner.frame_type IN ['FOR_LOOP', 'WHILE_LOOP']
    RETURN DISTINCT f.qualified_name, f.file_path
    LIMIT 50
    """
)

# Examine structure in detail
show_structure(
    target="nested_loop_function",
    detail_level="structure",
    structure_detail_depth=3
)
```

**Why nested loops are problematic:**
- **O(nÂ²) complexity:** Scales poorly with input size
- **Becomes O(nÂ³)** with triple nesting
- **Can often be optimized** with hash maps, pre-processing

**Optimization strategies:**
- Use hash maps for O(1) lookups instead of inner loop
- Pre-sort and use binary search
- Use set operations instead of nested iteration
- Consider algorithmic alternatives (sorting, hashing)

### Database Calls in Loops (N+1 Query Problem)

```python
# Search for loops that might contain database calls
search(query="for.*in.*execute query fetch", k=30, context_lines=7)

# Use Cypher to find LOOPS containing CALLS to database functions
query_relationships(
    cypher_query="""
    MATCH (loop:Frame)-[:Edge {type: 'CONTAINS'}*]->(call_site:Frame)-[e:Edge {type: 'CALLS'}]->(db_func:Frame)
    WHERE loop.frame_type IN ['FOR_LOOP', 'WHILE_LOOP']
      AND (db_func.name CONTAINS 'query'
           OR db_func.name CONTAINS 'execute'
           OR db_func.name CONTAINS 'fetch'
           OR db_func.name CONTAINS 'get'
           OR db_func.name CONTAINS 'find')
      AND e.confidence >= 0.5
    RETURN DISTINCT call_site.qualified_name, loop.qualified_name
    LIMIT 30
    """
)
```

**The N+1 problem:**
```python
# Bad: N+1 queries
for user in users:  # 1 query
    orders = get_orders(user.id)  # N queries
    process(orders)

# Good: 2 queries total
user_ids = [u.id for u in users]
all_orders = get_orders_batch(user_ids)
for user in users:
    orders = all_orders[user.id]
    process(orders)
```

**Fix strategies:**
- Batch queries (fetch all at once)
- Use JOIN operations
- Eager loading in ORMs
- Caching frequently accessed data

### Large Data Processing

```python
# Search for large file/collection processing
search(query="read.*file.*for.*line|load.*csv|read.*json", k=30)

# Find functions processing large inputs
search(
    query="def.*process.*data|def.*parse.*file",
    is_regex_input=True,
    k=30
)
```

**Performance patterns:**
```python
# Bad: Load entire file into memory
data = file.read()
for line in data.split('\n'):
    process(line)

# Good: Stream processing
for line in file:
    process(line)
```

### Loop Performance Checklist

For each loop found, check:

1. **Is it nested?** â†’ Consider O(nÂ²) alternatives
2. **Does it call database?** â†’ Batch the queries
3. **Does it call network?** â†’ Async or parallelize
4. **Does it allocate large objects?** â†’ Pre-allocate or reuse
5. **Can it be vectorized?** â†’ Use NumPy/pandas operations

### Detailed Analysis Workflow

```python
# Step 1: Find all nested loops
query_relationships(cypher_query="...") # See nested loops query

# Step 2: Examine structure
show_structure(target="suspect_function", detail_level="structure")

# Step 3: Search for database/network calls
search(query="execute query http request", k=20, context_lines=10)

# Step 4: Check context
search(query="specific_function_name", k=5, context_lines=15)
```

### Common Loop Antipatterns

```python
# Antipattern 1: String concatenation in loop
search(query="for.*in.*:.*+=.*str", k=20)

# Antipattern 2: List append in loop (when size known)
search(query="for.*in.*append", k=20)

# Antipattern 3: Repeated attribute access
search(query="for.*in.*self\\.", k=20)
```

**Fixes:**
```python
# String concat: Use join()
result = ''.join(items) instead of result += item

# List append: Pre-allocate
result = [None] * size

# Attribute: Cache in local variable
local_var = self.attribute
```

---

# Refactoring

## Api Surface Analysis
Path: refactoring/api_surface_analysis

# Refactoring
## API Surface Analysis
Path: refactoring/api_surface_analysis

Understanding public interfaces before making changes that could break external users.

### Identify Public API

Find top-level classes and functions (likely public API):

```python
# Find public API candidates (top-level, non-underscore prefixed)
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.type IN ['CLASS', 'CALLABLE']
      AND f.provenance = 'parsed'
      AND NOT f.name STARTS WITH '_'
      AND NOT EXISTS {
        MATCH (parent:Frame)-[:Edge {type: 'CONTAINS'}]->(f)
        WHERE parent.type IN ['CLASS', 'CALLABLE']
      }
    RETURN f.qualified_name, f.file_path, f.type
    ORDER BY f.file_path
    LIMIT 100
    """
)
```

### Public vs Private Ratio

Analyze encapsulation quality:

```python
# Calculate public/private ratio
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.type IN ['CALLABLE', 'CLASS']
      AND f.provenance = 'parsed'
    WITH
      count(CASE WHEN f.name STARTS WITH '_' THEN 1 END) AS private_count,
      count(CASE WHEN NOT f.name STARTS WITH '_' THEN 1 END) AS public_count
    RETURN public_count, private_count,
           toFloat(private_count) / toFloat(public_count) AS encapsulation_ratio
    """
)
```

**Encapsulation Ratio Guidelines:**
- **< 1.0:** More public than private (poor encapsulation)
- **1.0-2.0:** Balanced (acceptable)
- **> 2.0:** Well encapsulated (good)

### API Stability Analysis

Find public methods with many callers (breaking changes are costly):

```python
# Find high-impact public methods (>= 10 callers)
query_relationships(
    cypher_query="""
    MATCH ()-[e:Edge {type: 'CALLS'}]->(api:Frame)
    WHERE NOT api.name STARTS WITH '_'
      AND api.provenance = 'parsed'
      AND e.confidence >= 0.7
    WITH api.qualified_name AS public_method, count(e) AS caller_count
    WHERE caller_count >= 10
    RETURN public_method, caller_count
    ORDER BY caller_count DESC
    LIMIT 30
    """
)
```

**Breaking Change Risk:**
- **< 5 callers:** Low risk (easy to update)
- **5-20 callers:** Medium risk (manageable)
- **> 20 callers:** High risk (requires deprecation strategy)

### Examine API Structure

Get clean overview of public API surface:

```python
# Get API structure without private members
show_structure(
    target="PublicAPIClass",
    detail_level="minimal",
    include_private=False,
    include_relationships=True
)

# Find who's using this API
show_structure(
    target="PublicAPIClass",
    detail_level="minimal",
    include_relationships=True,
    max_callers=50
)
```

### Find All Public APIs in Package

```python
# Regex search for public APIs (non-underscore prefixed)
search(
    query="^[^_].*$",
    is_regex_input=True,
    type_filter="CALLABLE|CLASS",
    k=50
)

# Or target specific package
search(
    query="def [a-z][a-z_]*\\(",
    is_regex_input=True,
    k=50
)
```

### API Change Impact Workflow

Complete workflow before changing public API:

```python
# Step 1: Identify if target is public API
show_structure(
    target="TargetClass",
    detail_level="minimal",
    include_private=False
)

# Step 2: Find all callers
query_relationships(
    cypher_query="""
    MATCH (caller)-[e:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.qualified_name CONTAINS 'TargetClass'
      AND e.confidence >= 0.7
    RETURN caller.qualified_name, caller.file_path, e.confidence
    ORDER BY e.confidence DESC
    LIMIT 100
    """
)

# Step 3: Check impact
check_impact(
    target="TargetClass",
    max_depth=3,
    include_test_coverage=True
)

# Step 4: Assess caller distribution
# - All internal â†’ Safe to change
# - Mixed internal/external â†’ Needs deprecation
# - Mostly external â†’ Breaking change, version bump
```

### Deprecation Strategy

For high-impact API changes:

1. **Add new API** alongside old (don't remove yet)
2. **Deprecate old API** with warnings
3. **Update internal callers** to new API
4. **Document migration path** in release notes
5. **Remove in next major version**

### API Documentation Gaps

Find public APIs that might need documentation:

```python
# Find public APIs (then check their docstrings manually)
show_structure(
    target="PublicModule",
    detail_level="minimal",
    include_docstrings=True,
    include_private=False
)
```

---

# Security

## Auth Gaps
Path: security/auth_gaps

# Security
## Authentication & Authorization Gaps
Path: security/auth_gaps

Finding endpoints and functions missing authentication or authorization checks.

### Find Public Endpoints

```python
# Find API endpoints and routes
search(query="@app.route @api def endpoint", k=50, context_lines=7)

# Find Flask/Django routes
search(
    query="@app\\.(route|get|post|put|delete)",
    is_regex_input=True,
    k=50,
    context_lines=7
)

# Find FastAPI endpoints
search(
    query="@router\\.(get|post|put|delete)",
    is_regex_input=True,
    k=50,
    context_lines=7
)
```

### Find Endpoints Without Auth Checks

```python
# Find endpoints that don't call auth functions
query_relationships(
    cypher_query="""
    MATCH (endpoint:Frame)
    WHERE (endpoint.name CONTAINS 'api_'
           OR endpoint.name CONTAINS 'endpoint'
           OR endpoint.name CONTAINS 'route')
      AND NOT EXISTS {
        MATCH (endpoint)-[:Edge {type: 'CALLS'}*1..2]->(auth:Frame)
        WHERE auth.name CONTAINS 'auth'
           OR auth.name CONTAINS 'check'
           OR auth.name CONTAINS 'verify'
           OR auth.name CONTAINS 'require'
      }
      AND endpoint.provenance = 'local'
    RETURN endpoint.qualified_name, endpoint.file_path
    LIMIT 50
    """
)
```

### Trace Authentication Flow

For suspected unprotected endpoints, trace the call chain:

```python
# Check what an endpoint calls
query_relationships(
    cypher_query="""
    MATCH (endpoint:Frame)-[:Edge {type: 'CALLS'}*1..3]->(called:Frame)
    WHERE endpoint.qualified_name = 'api_endpoint_function'
      AND called.name CONTAINS 'auth'
    RETURN called.qualified_name, called.name
    LIMIT 20
    """
)

# Or check structure
show_structure(
    target="api_endpoint_function",
    detail_level="guards",
    include_relationships=True
)
```

### Find Auth Pattern Usage

Identify authentication patterns used in the codebase:

```python
# Find decorator-based auth
search(
    query="@require_auth|@login_required|@authenticated",
    is_regex_input=True,
    k=30
)

# Find middleware auth
search(query="authenticate middleware check_auth", k=30)

# Find manual auth checks
search(
    query="if.*auth|if.*token|if.*session",
    is_regex_input=True,
    k=40,
    context_lines=5
)
```

### Compare Protected vs Unprotected

```python
# Step 1: Find endpoints with auth decorators
search(query="@require_auth", k=50)

# Step 2: Find endpoints without auth decorators
search(
    query="@app\\.route.*\\ndef [^_]",
    is_regex_input=True,
    k=50
)

# Step 3: Compare lists to find gaps
```

### Authorization Checks (Role-Based)

Find functions that should check roles but might not:

```python
# Search for admin/role checks
search(query="is_admin check_role has_permission", k=30)

# Find functions with "admin" in name but no role check
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.name CONTAINS 'admin'
      AND f.frame_type = 'CALLABLE'
      AND NOT EXISTS {
        MATCH (f)-[:Edge {type: 'CALLS'}*1..2]->(check:Frame)
        WHERE check.name CONTAINS 'role'
           OR check.name CONTAINS 'permission'
           OR check.name CONTAINS 'admin'
      }
    RETURN f.qualified_name, f.file_path
    LIMIT 30
    """
)
```

### Common Auth Bypass Patterns

Look for these anti-patterns:

```python
# Commented-out auth checks (dangerous!)
search(query="# @require_auth|#.*authenticate", k=20)

# Auth checks in wrong order
search(query="if.*not.*auth.*return|if.*auth.*pass", k=20)
```

### Auth Gap Analysis Workflow

```python
# Step 1: Identify auth patterns
search(query="auth authenticate login check_user", k=30)

# Step 2: Find all endpoints
search(query="@app @router @api endpoint", k=50)

# Step 3: For each endpoint, check auth flow
show_structure(target="endpoint_name", detail_level="guards")

# Step 4: Trace calls to verify auth
query_relationships(
    cypher_query="""
    MATCH (endpoint)-[:Edge {type: 'CALLS'}*1..2]->(auth)
    WHERE endpoint.qualified_name = 'endpoint_name'
    RETURN auth.qualified_name
    """
)
```

### Mitigation Strategies

For endpoints missing auth:

1. **Centralized auth** - Use decorators or middleware
2. **Default deny** - Require explicit public endpoint marking
3. **Auth before logic** - Check auth first, then process request
4. **Role-based access** - Check not just "logged in" but "authorized"
5. **Test coverage** - Verify auth is enforced in tests

**Risk Levels:**
- **Critical:** Write operations without auth (create/update/delete)
- **High:** Read sensitive data without auth
- **Medium:** Admin functions without role checks
- **Low:** Public endpoints that should be public

---

## Injection Detection
Path: security/injection_detection

# Security
## Injection Detection
Path: security/injection_detection

Finding SQL injection, command injection, and insecure deserialization vulnerabilities.

### SQL Injection Risks

Find string concatenation in SQL contexts:

```python
# Search for dangerous SQL patterns
search(query="execute.*+.*query.*format", k=30, context_lines=5)

# Find database execute calls
search(
    query="execute|executemany|cursor",
    is_regex_input=True,
    frame_type_filter="CALLABLE",
    k=40,
    context_lines=7
)

# Look for format string SQL
search(
    query=".format\\(|%.*%|f['\"].*SELECT",
    is_regex_input=True,
    k=30,
    context_lines=5
)
```

**High-Risk Patterns:**
- `"SELECT * FROM users WHERE id = " + user_id`
- `f"DELETE FROM {table} WHERE id = {id}"`
- `query.format(table=user_input)`
- `.execute(raw_string % values)`

**Safe Patterns (verify these are used):**
- Parameterized queries: `.execute(query, (param1, param2))`
- ORM usage: `Model.objects.filter(id=user_id)`
- Query builders: `query.where('id', '=', ?).bind(user_id)`

### Command Injection Risks

Find shell command execution with user input:

```python
# Search for shell command execution
search(query="os.system subprocess shell=True exec eval", k=30)

# Find specific risky patterns
search(
    query="subprocess.*shell=True",
    is_regex_input=True,
    k=20,
    context_lines=5
)

# os.system usage
search(
    query="os\\.system\\(",
    is_regex_input=True,
    k=20,
    context_lines=5
)
```

**High-Risk Patterns:**
- `os.system(user_input)`
- `subprocess.call(cmd, shell=True)`
- `exec(user_data)`
- `eval(request.params)`

**Safe Alternatives:**
- `subprocess.run(['command', arg1, arg2], shell=False)`
- Use argument lists instead of shell strings
- Whitelist allowed commands
- Sanitize and validate all inputs

### Insecure Deserialization

Find pickle, eval, exec usage:

```python
# Search for dangerous deserialization
search(query="pickle.loads eval exec compile", k=30, context_lines=7)

# Specific patterns
search(
    query="pickle\\.loads|yaml\\.load\\(|eval\\(|exec\\(",
    is_regex_input=True,
    k=30,
    context_lines=5
)
```

**High-Risk Patterns:**
- `pickle.loads(untrusted_data)`
- `yaml.load(user_input)` (without SafeLoader)
- `eval(request.body)`
- `exec(user_provided_code)`

**Safe Alternatives:**
- `json.loads()` for data
- `yaml.safe_load()` instead of `yaml.load()`
- `ast.literal_eval()` for Python literals only
- Never deserialize untrusted data

### Verification Workflow

```python
# Step 1: Find potentially vulnerable functions
search(query="execute query sql", k=30)

# Step 2: Examine structure to see if user input flows in
show_structure(
    target="vulnerable_function",
    detail_level="structure",
    structure_detail_depth=2
)

# Step 3: Trace back to see where data comes from
query_relationships(
    cypher_query="""
    MATCH (source:Frame)-[:Edge {type: 'CALLS'}*1..3]->(target:Frame)
    WHERE target.qualified_name = 'vulnerable_function'
    RETURN source.qualified_name, source.file_path
    LIMIT 30
    """
)
```

### Mitigation Checklist

For each vulnerability found:

1. **Identify input source** - Where does untrusted data come from?
2. **Trace data flow** - How does it reach the vulnerable function?
3. **Check validation** - Is input validated/sanitized?
4. **Verify escaping** - Is data properly escaped?
5. **Prefer safe APIs** - Can we use parameterized queries?

**Priority Levels:**
- **Critical:** Direct user input to exec/eval/system
- **High:** SQL injection in authentication/authorization
- **Medium:** Command injection in admin features
- **Low:** Well-validated input to safe APIs

---

## Secrets Detection
Path: security/secrets_detection

# Security
## Secrets Detection
Path: security/secrets_detection

Finding hardcoded secrets, passwords, API keys, and tokens in source code.

### Common Secret Patterns

```python
# Search for potential hardcoded secrets
search(
    query="password|secret|api_key|token.*=.*['\"]",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# More specific patterns
search(
    query="(password|passwd|pwd)\\s*=\\s*['\"][^'\"]+['\"]",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### API Keys and Tokens

```python
# API key patterns
search(
    query="api[_-]?key|apikey|access[_-]?key",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# Token patterns
search(
    query="token|bearer|jwt.*=.*['\"]",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# OAuth secrets
search(
    query="client[_-]?secret|oauth",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### Database Credentials

```python
# Database connection strings
search(
    query="mysql://|postgres://|mongodb://|connection.*string",
    is_regex_input=True,
    k=30,
    context_lines=5
)

# Database passwords
search(
    query="db[_-]?password|database.*password",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### AWS and Cloud Credentials

```python
# AWS keys (format: AKIA...)
search(
    query="AKIA[0-9A-Z]{16}",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# Generic AWS patterns
search(
    query="aws[_-]?access|aws[_-]?secret",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# Other cloud providers
search(
    query="azure|gcp|google[_-]?cloud.*credential",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### Private Keys

```python
# RSA/SSH private keys
search(
    query="BEGIN.*PRIVATE.*KEY|private[_-]?key.*=",
    is_regex_input=True,
    k=30,
    context_lines=5
)

# Certificate patterns
search(
    query="\\.pem|\\.key|\\.p12|\\.pfx",
    is_regex_input=True,
    k=30
)
```

### Encryption Keys

```python
# Encryption/signing keys
search(
    query="secret[_-]?key|encryption[_-]?key|signing[_-]?key",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# Cipher/crypto patterns
search(
    query="cipher|crypto.*key|aes[_-]?key",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### Webhooks and Integration Secrets

```python
# Webhook secrets
search(
    query="webhook[_-]?secret|signing[_-]?secret",
    is_regex_input=True,
    k=30,
    context_lines=3
)

# Integration keys
search(
    query="stripe|twilio|sendgrid|mailgun.*key",
    is_regex_input=True,
    k=30,
    context_lines=3
)
```

### False Positive Filtering

**Likely False Positives (safe to ignore):**
- `password = None`
- `password = ''`
- `password = 'placeholder'`
- `password = 'test123'` (in test files)
- `password = os.environ.get('PASSWORD')` (environment variables - good!)
- `password = config.get('password')` (config files - depends)

**Likely True Positives (investigate):**
- `password = 'MyS3cr3tP@ss'`
- `api_key = 'sk_live_...'`
- Long random-looking strings (20+ chars)
- Patterns like `AKIA...` (AWS keys)

### Verification Workflow

```python
# Step 1: Find potential secrets
search(query="secret|password|key|token", k=50)

# Step 2: For each match, get more context
search(
    query="specific_variable_name",
    k=5,
    context_lines=10
)

# Step 3: Check if it's in test files (lower priority)
# Look at file_path in results

# Step 4: Verify if it's a real secret or configuration
# - Real secret: hardcoded value
# - Config: loaded from env/file
```

### Remediation Steps

For each hardcoded secret found:

1. **Rotate immediately** - Assume compromised, generate new secret
2. **Move to environment variables** - Use `os.environ['SECRET']`
3. **Use secret management** - HashiCorp Vault, AWS Secrets Manager, etc.
4. **Update .gitignore** - Prevent future commits
5. **Remove from git history** - Use git-filter-repo or BFG

**Environment Variable Pattern:**
```python
# Bad
API_KEY = "sk_live_abc123"

# Good
API_KEY = os.environ.get('API_KEY')
if not API_KEY:
    raise ValueError("API_KEY environment variable not set")
```

**Priority Levels:**
- **Critical:** Production API keys, database passwords
- **High:** OAuth secrets, signing keys
- **Medium:** Test/dev credentials
- **Low:** Placeholder values, comments

### Exclude Test/Mock Data

```python
# Search excluding test files (manual filtering needed)
search(query="password|secret", k=100)
# Then manually check file_path for '/test/' or 'test_'
```

---

# Workflows

## Health Audit
Path: workflows/health_audit

# Workflows
## Codebase Health Audit
Path: workflows/health_audit

Complete codebase health audit workflow - systematic review of dead code, complexity, tests, security, architecture, and duplication.

This is a comprehensive workflow for periodic codebase health assessment.

### Workflow Overview

**Goal:** Systematic assessment of codebase quality across all dimensions.

**Duration:** 30-60 minutes for medium codebases

**Output:** Prioritized list of issues and recommendations

### Step 1: Overview & Baseline

Get high-level metrics:

```python
# Database health and frame counts
show_status(detail_level="detailed")

# Shows:
# - Total frames by type (CALLABLE, CLASS, PACKAGE)
# - Confidence distribution
# - Database health
```

**Record baseline metrics:**
- Total classes, functions
- Confidence distribution
- Database size

### Step 2: Dead Code Detection

Find unused code:

```python
# Unreferenced callables
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND NOT EXISTS {
        MATCH ()-[:Edge {type: 'CALLS'}]->(f)
      }
      AND f.provenance = 'local'
    RETURN f.qualified_name, f.file_path
    LIMIT 100
    """
)

# Unreferenced classes
query_relationships(
    cypher_query="""
    MATCH (c:Frame)
    WHERE c.frame_type = 'CLASS'
      AND NOT EXISTS {
        MATCH ()-[e:Edge]->(c)
        WHERE e.type IN ['INHERITS', 'IMPLEMENTS', 'CALLS']
      }
      AND c.provenance = 'local'
    RETURN c.qualified_name, c.file_path
    LIMIT 50
    """
)
```

**Deliverable:** List of dead code candidates for removal

### Step 3: Complexity Hotspots

Find overly complex code:

```python
# Functions with high control flow complexity
query_relationships(
    cypher_query="""
    MATCH (f:Frame)-[:Edge {type: 'CONTAINS'}]->(cf:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND cf.frame_type IN ['IF_BLOCK', 'FOR_LOOP', 'WHILE_LOOP', 'TRY_BLOCK']
    WITH f.qualified_name AS func, f.file_path AS path, count(cf) AS complexity
    WHERE complexity >= 5
    RETURN func, path, complexity
    ORDER BY complexity DESC
    LIMIT 30
    """
)

# Deep nesting
query_relationships(
    cypher_query="""
    MATCH path = (f:Frame)-[:Edge {type: 'CONTAINS'}*3..8]->(nested:Frame)
    WHERE f.frame_type = 'CALLABLE'
    WITH f.qualified_name AS func, length(path) AS nesting_depth
    RETURN func, nesting_depth
    ORDER BY nesting_depth DESC
    LIMIT 20
    """
)
```

**Deliverable:** Priority list for refactoring

### Step 4: Test Coverage Analysis

Assess test quality:

```python
# Test-to-code ratio
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type IN ['CALLABLE', 'CLASS']
      AND f.provenance = 'local'
    WITH
      count(CASE WHEN f.file_path CONTAINS 'test' THEN 1 END) AS test_count,
      count(CASE WHEN NOT f.file_path CONTAINS 'test' THEN 1 END) AS prod_count
    RETURN test_count, prod_count,
           toFloat(test_count) / toFloat(prod_count) AS test_ratio
    """
)

# Untested code
query_relationships(
    cypher_query="""
    MATCH (f:Frame)
    WHERE f.frame_type = 'CALLABLE'
      AND NOT f.file_path CONTAINS 'test'
      AND NOT EXISTS {
        MATCH (test:Frame)-[:Edge {type: 'CALLS'}]->(f)
        WHERE test.file_path CONTAINS 'test'
      }
    RETURN count(f) AS untested_count
    """
)
```

**Deliverable:** Test coverage gaps

### Step 5: Security Issues

Find potential vulnerabilities:

```python
# SQL injection risks
search(query="execute.*+.*query|format.*sql", k=30)

# Command injection
search(query="os.system|subprocess.*shell=True", k=30)

# Hardcoded secrets
search(
    query="password|secret|api_key.*=.*['\"]",
    is_regex_input=True,
    k=30
)
```

**Deliverable:** Security issues by priority

### Step 6: Architectural Violations

Check layer boundaries:

```python
# Cross-layer violations (adjust paths for your architecture)
query_relationships(
    cypher_query="""
    MATCH (ui:Frame)-[e:Edge {type: 'CALLS'}]->(data:Frame)
    WHERE ui.file_path CONTAINS '/ui/'
      AND data.file_path CONTAINS '/data/'
      AND e.confidence >= 0.6
    RETURN ui.qualified_name, data.qualified_name
    LIMIT 50
    """
)

# Circular dependencies
query_relationships(
    cypher_query="""
    MATCH (a:Frame)-[:Edge {type: 'IMPORTS'}]->(b:Frame)-[:Edge {type: 'IMPORTS'}]->(a)
    WHERE a.frame_type = 'PACKAGE' AND b.frame_type = 'PACKAGE'
    RETURN a.qualified_name, b.qualified_name
    LIMIT 50
    """
)
```

**Deliverable:** Architectural violations list

### Step 7: Code Duplication

Find duplicated code:

```python
# Global clone detection
find_clones(
    min_similarity=0.75,
    max_results=100,
    exclude_same_file=True
)
```

**Deliverable:** Clone pairs for consolidation

### Health Score Calculation

**Scoring rubric (0-100):**

- **Dead Code (20 points):**
  - 0 dead code = 20 points
  - >10% dead code = 0 points

- **Complexity (20 points):**
  - <5% complex functions = 20 points
  - >20% complex = 0 points

- **Test Coverage (25 points):**
  - Test ratio >1.0 = 25 points
  - Test ratio <0.3 = 0 points

- **Security (15 points):**
  - 0 issues = 15 points
  - >5 critical = 0 points

- **Architecture (10 points):**
  - No violations = 10 points
  - >10 violations = 0 points

- **Duplication (10 points):**
  - <5% duplication = 10 points
  - >20% = 0 points

### Report Template

```markdown
# Codebase Health Audit - [Date]

## Executive Summary
- Health Score: X/100
- Critical Issues: X
- High Priority: X
- Medium Priority: X

## 1. Dead Code
- X unreferenced functions
- X unreferenced classes
**Action:** Review and remove

## 2. Complexity
- X functions with complexity >5
- X functions with deep nesting
**Action:** Refactor top 10

## 3. Test Coverage
- Test ratio: X.XX
- X untested functions
**Action:** Add tests for public APIs

## 4. Security
- X SQL injection risks
- X command injection risks
- X hardcoded secrets
**Action:** Fix critical issues immediately

## 5. Architecture
- X layer violations
- X circular dependencies
**Action:** Enforce boundaries

## 6. Duplication
- X clone pairs (>75% similar)
**Action:** Consolidate top duplicates

## Recommendations
1. [Highest priority action]
2. [Second priority]
3. [Third priority]
```

---

## Pre Refactoring
Path: workflows/pre_refactoring

# Workflows
## Pre-Refactoring Workflow
Path: workflows/pre_refactoring

Complete pre-refactoring analysis workflow - understand structure, check impact, find duplicates, verify tests.

This workflow combines multiple analysis skills to safely prepare for refactoring.

### Workflow Overview

**Goal:** Gather all necessary information before changing code to minimize risk.

**Steps:**
1. Understand current structure
2. Find all dependencies
3. Identify similar code
4. Verify test coverage
5. Check critical dependencies

### Step 1: Understand Current Structure

Get clean API overview:

```python
# Get structure with minimal detail (token-efficient)
show_structure(target="TargetClass", detail_level="minimal")

# If needed, add behavioral hints
show_structure(
    target="TargetClass",
    detail_level="guards",
    include_relationships=True
)
```

**What to look for:**
- Method signatures (what's the API?)
- Public vs private methods
- Dependencies on other classes
- Who inherits from this class?

### Step 2: Find All Dependencies

Check who depends on this code:

```python
# Extended impact (2 levels deep)
check_impact(
    target="TargetClass",
    max_depth=2,
    include_test_coverage=True,
    risk_assessment=True
)
```

**Interpret results:**
- **High risk:** Many dependents, low test coverage
- **Medium risk:** Moderate dependents, some tests
- **Low risk:** Few dependents, well-tested

### Step 3: Find Similar Code

Look for duplicates or similar implementations:

```python
# Find clones of this code
find_clones(
    query="TargetClass functionality description",
    query_k=20,
    min_similarity=0.70
)

# Or find all clones globally
find_clones(min_similarity=0.75, max_results=50)
```

**Consider consolidation:**
- If similarity > 0.80: Strong consolidation candidate
- Check if clones should be unified with refactoring
- May reveal better abstraction

### Step 4: Verify Test Coverage

Ensure changes can be safely tested:

```python
# Already included in check_impact above, but can also:
query_relationships(
    cypher_query="""
    MATCH (test:Frame)-[e:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.qualified_name CONTAINS 'TargetClass'
      AND test.file_path CONTAINS 'test'
      AND e.confidence >= 0.6
    RETURN test.qualified_name, test.file_path
    LIMIT 50
    """
)
```

**Test coverage goals:**
- All public methods tested
- Edge cases covered
- Integration tests for critical paths

### Step 5: Verify Critical Dependencies

Double-check high-confidence dependencies:

```python
# Find direct callers with high confidence
query_relationships(
    cypher_query="""
    MATCH (caller)-[e:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.qualified_name CONTAINS 'TargetClass'
      AND e.confidence >= 0.7
    RETURN caller.qualified_name, caller.file_path, e.confidence
    ORDER BY e.confidence DESC
    LIMIT 50
    """
)
```

**Focus on:**
- High-confidence callers (>0.8)
- Critical paths (main â†’ target)
- Public API usage

### Complete Workflow Example

```python
# === PRE-REFACTORING CHECKLIST ===

# 1. Structure
show_structure(target="MyClass", detail_level="minimal")

# 2. Impact
check_impact(
    target="MyClass",
    max_depth=2,
    include_test_coverage=True,
    risk_assessment=True
)

# 3. Duplicates
find_clones(query="MyClass", query_k=20, min_similarity=0.70)

# 4. Test coverage (if not clear from check_impact)
query_relationships(
    cypher_query="""
    MATCH (test:Frame)-[:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.qualified_name = 'MyClass'
      AND test.file_path CONTAINS 'test'
    RETURN count(test) AS test_count
    """
)

# 5. Critical deps
query_relationships(
    cypher_query="""
    MATCH (caller)-[e:Edge {type: 'CALLS'}]->(target:Frame)
    WHERE target.qualified_name = 'MyClass'
      AND e.confidence >= 0.7
    RETURN caller.qualified_name, e.confidence
    ORDER BY e.confidence DESC
    LIMIT 30
    """
)
```

### Decision Matrix

**Safe to refactor when:**
- âœ… Well-tested (>80% coverage)
- âœ… Clear structure understood
- âœ… Few dependents (<10) OR all internal
- âœ… No duplicates found OR consolidation planned
- âœ… High confidence relationships (>0.7)

**Risky - proceed with caution:**
- âš ï¸ Low test coverage (<50%)
- âš ï¸ Many external dependents (>20)
- âš ï¸ Complex structure (high coupling)
- âš ï¸ Low confidence edges (<0.5)

**Do NOT refactor yet:**
- âŒ No tests
- âŒ Critical production path
- âŒ Unknown dependencies
- âŒ Duplicates not analyzed

### Post-Analysis Actions

Based on findings:

1. **Add missing tests** before refactoring
2. **Document dependencies** for review
3. **Plan consolidation** if duplicates found
4. **Communicate changes** to dependent teams
5. **Create feature flag** for risky changes

---
